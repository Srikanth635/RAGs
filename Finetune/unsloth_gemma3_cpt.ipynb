{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-22T13:43:18.806485Z",
     "start_time": "2025-04-22T13:43:04.643602Z"
    }
   },
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\", # \"unsloth/mistral-7b\" for 16bit loading\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malineni/envs/unsloth/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.15: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070. Num GPUs = 1. Max memory: 11.719 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T13:44:20.504529Z",
     "start_time": "2025-04-22T13:44:18.090127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ],
   "id": "1d4172e2d54165ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.vision_tower.vision_model.encoder` require gradients\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T13:44:23.085625Z",
     "start_time": "2025-04-22T13:44:23.081047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "def read_markdown_files(directory):\n",
    "    markdown_contents = []\n",
    "    text_contents = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\", errors='replace') as f:\n",
    "                        content = f.read()\n",
    "                        markdown_contents.append(content)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {e}\")\n",
    "            elif file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\", errors='replace') as f:\n",
    "                        content = f.read()\n",
    "                        text_contents.append(content)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    return markdown_contents, text_contents"
   ],
   "id": "536c76a653eb9a4b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T13:44:23.793129Z",
     "start_time": "2025-04-22T13:44:23.788731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs_content, text_content = read_markdown_files(os.path.join(os.curdir, \"../documentation/marker_pdfs\"))\n",
    "print(docs_content[2])"
   ],
   "id": "ae89c71ceff06ba2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## U[NIVERSITÃ„T](http://www.uni-bremen.de) BREMEN\n",
      "\n",
      "MASTER THESIS\n",
      "\n",
      "## **Tactile-Based and Cognition-Enabled Manipulation for Real World Assembly Tasks**\n",
      "\n",
      "*Author:* Arthur NIEDZWIECKI\n",
      "\n",
      "*Supervisor:* Prof. Dr. h.c. Michael BEETZ, PhD *Second Supervisor:* Dr. rer. nat. Federico RUIZ Ugalde *Advisor:* Gayane KAZHOYAN\n",
      "\n",
      "*A thesis submitted in fulfillment of the requirements for the degree of Master of Science*\n",
      "\n",
      "*in the*\n",
      "\n",
      "[Institute for Artificial Intelligence](http://ai.uni-bremen.de/) [Computer Science](http://www.informatik.uni-bremen.de)\n",
      "\n",
      "November 11, 2020\n",
      "\n",
      "- \n",
      "- \n",
      "- \n",
      "- \n",
      "- \n",
      "- \n",
      "\n",
      "Bremen, den 12.11.2020\n",
      "\n",
      "#### <span id=\"page-4-0\"></span>[UNIVERSITÃ„T BREMEN](HTTP://WWW.UNI-BREMEN.DE)\n",
      "\n",
      "## *Abstract*\n",
      "\n",
      "#### [Faculty 3](http://fb3.uni-bremen.de) [Computer Science](http://www.informatik.uni-bremen.de)\n",
      "\n",
      "#### Master of Science\n",
      "\n",
      "#### **Tactile-Based and Cognition-Enabled Manipulation for Real World Assembly Tasks**\n",
      "\n",
      "#### by Arthur NIEDZWIECKI\n",
      "\n",
      "Robotic arms are an important asset in efficient automation industry. Their inexhaustible capabilities for repetitive tasks outperform those of human beings by far. In the past years also the demand for varying assembly tasks has risen. Adapting an industrial robot to novel environments usually requires to manually adjust the robot's movement, even if the novelty only includes small changes in an object's position. Contemporary solutions involve autonomous robots, which are equipped with Artificial Intelligence to reason upon changes in the environment. Tasks designed for such robots need to be only general descriptions, whereby the robot interprets its procedure by actively perceiving its surroundings, enabling reaction to, and recovery from undesired situations. Using robot perception, however, produces inaccuracy in localization of a robot and objects equally, which makes intricate assembly tasks hard to perform.\n",
      "\n",
      "This thesis aims to increase accuracy of an autonomous mobile robot during assembly tasks in the real word. Two approaches are designed for this, which utilize detection and evaluation of rigid-rigid contact forces between the gripper and an object. One approach updates an object's estimated position with respect to the robot by touching it, which inherently increases the robot's accuracy when manipulating it afterwards. The other one is applicable during immediate assembly of two parts, where the gripper adjusts its position incrementally over several trials by evaluating contact forces. The approaches are evaluated by assembling a Battat toy airplane in the real world. Said airplane is included in the Yale-CMU-Berkeley object dataset for manipulation benchmarks and consists of several irregular-shaped parts. Therefore it provides a contemporary and comparable setup for object manipulation in every-day environments.\n",
      "\n",
      "#### [UNIVERSITÃ„T BREMEN](HTTP://WWW.UNI-BREMEN.DE)\n",
      "\n",
      "## <span id=\"page-6-0\"></span>*Zusammenfassung*\n",
      "\n",
      "[Fachbereich 3](http://fb3.uni-bremen.de) [Informatik](http://www.informatik.uni-bremen.de)\n",
      "\n",
      "Master of Science\n",
      "\n",
      "#### **Tast- und Kognitions-basierte Manipulation fÃ¼r Zusammenbau-Aufgaben in der echten Welt**\n",
      "\n",
      "#### von Arthur NIEDZWIECKI\n",
      "\n",
      "Roboterarme sind eine wichtige Investition in effizienter Automatisierungsindustrie. Ihre unerschÃ¶pflichen MÃ¶glichkeiten in sich wiederholenden Aufgaben Ã¼bertreffen jene von Menschen bei Weitem. In den vergangenen Jahren ist auch der Bedarf nach verÃ¤nderlichen Montage-Aufgaben gestiegen. Um einen industriellen Roboter an eine neue Umgebung anzupassen, mÃ¼ssen seine Bewegungen Ã¼blicherweise manuell eingestellt werden, auch wenn die Neuerung lediglich kleine VerÃ¤nderung in der Position von Objekten beinhaltet. ZeitgenÃ¶ssische LÃ¶sungen involvieren autonome Roboter, welche, mit KÃ¼nstlicher Intelligenz ausgerÃ¼stet, Ã¼ber die VerÃ¤nderung ihrer Umgebung nachdenken kÃ¶nnen. Aufgaben fÃ¼r solche Roboter brauchen nur abstrakte Beschreibungen zu sein, wonach der Roboter, durch Wahr- nehmung seiner Umgebung, seine Prozedur selbst interpretiert, und er auf ungewollte Situationen reagieren, und sich daraus befreien kann. Leider kommen durch robotische Wahrnehmung auch Ungenauigkeit einher, woduch komplizierte Aufgaben schwer zu bewÃ¤ltigen sind.\n",
      "\n",
      "Diese Thesis zielt darauf ab die Genauigkeit von autonomen, mobilen Robotern bei Montage-Aufgaben in der echten Welt zu verbessern. DafÃ¼r wurden zwei AnsÃ¤tze entwickelt, welche den Krafteinfluss zwischen einem Greifer und Objekt wahrnehmen und evaluieren. Einer der AnsÃ¤tze aktualisiert die vermutete Position eines Objektes im VerhÃ¤ltnis zum Roboter durch BerÃ¼hrung, was wiederum die Genauigkeit des Roboter verbessert, wenn das Objekt im nachhinein von ihm manipuliert wird. Der andere Ansatz ist direkt im Zusammenbau zweier Objekte anwendbar, wo der Greifer seine Position inkrementell Ã¼ber mehrere Versuche, durch Evaluation der KontaktkrÃ¤fte, anpasst. Die AnsÃ¤tze werden evaluiert, indem der Roboter ein Battat Spiel-Flugzeug in der echten Welt zusammenbaut. Das verwendete Flugzeug ist Teil des Yale-CMU-Berkeley Object Dataset, welches fÃ¼r Benchmarks in Robotermanipulation verwendet wird, und besteht aus mehreren unregelmÃ¤ÃŸig geformten Bauteilen. Deshalb bietet es ein zeitgenÃ¶ssisches und vergleichbares Beispiel fÃ¼r die Manipulation von Objekten in alltÃ¤glichen Umgebungen.\n",
      "\n",
      "## **Contents**\n",
      "\n",
      "|   |                                                      | Declaration of Authorship                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | iii                                                                                                      |\n",
      "|---|------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\n",
      "|   | Abstract                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | v                                                                                                        |\n",
      "|   |                                                      | Zusammenfassung                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | vii                                                                                                      |\n",
      "| 1 | 1.1<br>1.2<br>1.3<br>1.4                             | Introduction<br>General Approach and Research Questions<br>.<br>Contributions<br>.<br>Related Work<br>.<br>Reader's Guide<br>.                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 1<br>1<br>2<br>3<br>4                                                                                    |\n",
      "| 2 | 2.1<br>2.2<br>2.3<br>2.4<br>2.5<br>2.6<br>2.7<br>2.8 | Foundations<br>ROS<br>Boxy<br>.<br>Naive Kinematics Simulator<br>Giskard<br>Bullet Physics<br>.<br>RViz<br>Gazebo<br>.<br>CRAM<br>.<br>2.8.1<br>Fluents<br>2.8.2<br>Designators<br>.<br>2.8.3<br>Self-Recovering Plans<br>.                                                                                                                                                                                                                                                                                                                                                                          | 5<br>5<br>5<br>7<br>7<br>8<br>10<br>11<br>11<br>11<br>13<br>16                                           |\n",
      "| 3 | 3.1<br>3.2<br>3.3                                    | Methods and Implementation<br>Sensor Communication<br>3.1.1<br>Using the Wrench-Sensor in CRAM<br>.<br>Approach 1: End Effector Pose Adjustment<br>.<br>3.2.1<br>Torques in Simulation<br>.<br>3.2.2<br>Heuristic Classifier<br>.<br>3.2.3<br>Procedure<br>.<br>3.2.4<br>Implementation<br>Contact Detection<br>Reacting to collisions<br>Assembly Plan in CRAM<br>.<br>Approach 2: Object Pose Adjustment<br>3.3.1<br>Pseudo Ray casting<br>Procedure<br>.<br>3.3.2<br>Touching the Target<br>.<br>Procedure<br>.<br>Object pose adjustment during contact<br>.<br>Contact Detection - Generic<br>. | 19<br>22<br>22<br>24<br>24<br>25<br>26<br>27<br>27<br>28<br>29<br>31<br>32<br>33<br>34<br>35<br>36<br>37 |\n",
      "\n",
      "|   |     | Implementation                                             | 37 |\n",
      "|---|-----|------------------------------------------------------------|----|\n",
      "| 4 |     | Experimental Evaluation                                    | 39 |\n",
      "|   | 4.1 | End Effector Pose Adjustment                               | 40 |\n",
      "|   |     | 4.1.1<br>Simulation in Gazebo<br>.                         | 40 |\n",
      "|   |     | 4.1.2<br>Real-World Application<br>.                       | 42 |\n",
      "|   |     | 4.1.3<br>Summary - End Effector Adjustment                 | 44 |\n",
      "|   | 4.2 | Object Pose Adjustment<br>.                                | 45 |\n",
      "|   |     | 4.2.1<br>Simulation in Bullet-Physics - Pseudo Ray Casting | 45 |\n",
      "|   |     | 4.2.2<br>Real-World Application<br>.                       | 45 |\n",
      "|   | 4.3 | Working with Robots in the real world                      | 48 |\n",
      "|   |     | 4.3.1<br>KUKA LWR-4+ Arms<br>.                             | 48 |\n",
      "|   |     | 4.3.2<br>Localization<br>.                                 | 49 |\n",
      "|   |     | 4.3.3<br>Giskard<br>.                                      | 49 |\n",
      "|   |     | 4.3.4<br>KMS40 Force-Torque Sensor                         | 50 |\n",
      "|   | 4.4 | Evaluation Summary                                         | 51 |\n",
      "|   |     |                                                            |    |\n",
      "| 5 |     | Conclusion                                                 | 53 |\n",
      "|   | 5.1 | Summary                                                    | 53 |\n",
      "|   | 5.2 | Discussion<br>.                                            | 53 |\n",
      "|   | 5.3 | Recommendations on Future Work                             | 54 |\n",
      "| A |     | Real-World Force Data                                      | 59 |\n",
      "|   | A.1 | Chassis on Horizontal Holder                               | 60 |\n",
      "|   | A.2 | Bottom-wing on Chassis<br>.                                | 62 |\n",
      "|   | A.3 | Underbody on Bottom-wing                                   | 64 |\n",
      "|   | A.4 | Upper-body on Underbody<br>.                               | 66 |\n",
      "|   | A.5 | Bolt into rear hole                                        | 69 |\n",
      "|   |     |                                                            |    |\n",
      "|   |     | Bibliography                                               | 73 |\n",
      "\n",
      "x\n",
      "\n",
      "## <span id=\"page-10-0\"></span>**Chapter 1**\n",
      "\n",
      "## **Introduction**\n",
      "\n",
      "## <span id=\"page-10-1\"></span>**1.1 General Approach and Research Questions**\n",
      "\n",
      "Industrial assembly nowadays is supported by a variety of machines and robots, performing the same task over and over again to account for tasks usually dangerous or exhausting for a human being. In recent years the demand for industrial robots has risen and their spectrum of applicability became incrementally richer. For each task a new robot is designed and programmed to solve it. Some robots can be re-used from one task to another by changing their movement, adjusting it to novel environments. Contemporary research already tries to design industrial robots that collaborate with humans for a common goal, where the robot reacts to human activities and changes its movement accordingly (Sun et al., [2020\\)](#page-84-0). But on their own, robots rely heavily on detailed knowledge about their immediate environment to autonomously make decisions on what to do next, and how to do it. Depending on the accuracy of such knowledge, intelligently planned movement can be very precise, or completely fail their goal. Autonomous robots have been, and still are heavily researched. Industrial arms achieve their precision by being manually programmed to do the exact same thing. Especially in assembly tasks, where high accuracy is imperative, robots without any knowledge of what they are doing can be extremely precise in one task, but it takes only one misplaced part to render such robots incapable. On the other hand, autonomous cognitive robots are supposed to be general, reacting to novel situations by adjusting themselves. This requires perception of the environment, which is never completely flawless.\n",
      "\n",
      "With the help of a cognitive system a robot is able to keep track of its surroundings and the changes within, including the parts it is tasked to assemble together. This world-state representation helps robots to find their way around, navigating while avoiding collisions, locating objects etc., but since the accuracy of an autonomous robot in a real-world environment depends heavily on its sensors, an object's correct location can only be estimated to a certain degree. Combining the abilities of a cognitive system with sensor measurements in the correct places, the accuracy can be greatly improved. Localizing the robot is typically done with laser sensors and odometry measurements, retrieved from optical encoders in the wheel motors of the mobile base. Object locations are estimated by applying computer vision algorithms to data coming from RGB-D sensors. Accuracy in general relies on the accuracy of the sensors and algorithms to evaluate their emitted data.\n",
      "\n",
      "To improve accuracy, one can use force-torque sensors to gain tactile information. Robot's arms have optical encoders for each joint, enhanced with torque sensors to improve their estimated state, which is used to know where the arm is in space. Force-torque sensors can be equipped to the arm's end effector to receive contact forces. Knowing where the arm is, combined with contact detection between the\n",
      "\n",
      "<span id=\"page-11-1\"></span>![](_page_11_Figure_1.jpeg)\n",
      "\n",
      "FIGURE 1.1: Battat airplane parts for assembly\n",
      "\n",
      "arm and the environment, can be used to improve the estimates of object locations with respect to the robot.\n",
      "\n",
      "In this thesis I want to show how the assembly process, executed by an autonomous robots in a real-world environment, high-level controlled by a cognitive system using a gaming engine as knowledge representation, can benefit from utilizing force-torque sensors to increase accuracy and feasibility of such intricate manipulation tasks. In two different approaches I will discuss how force response can be helpful, and where its limits might be.\n",
      "\n",
      "### <span id=\"page-11-0\"></span>**1.2 Contributions**\n",
      "\n",
      "Assembly tasks are usually designed for robotic arms in precise environments. My work shows, that the combination of a cognitive environment, gaming-engine-based spatial reasoning, reactive planning and simple evaluations of force feedback can improve the accuracy of autonomous, humanoid robots in uncertain environments. Two approaches improve the assembly of a toy airplane (see figure [1.1\\)](#page-11-1), executed by an autonomous robot. This airplane is included in the Yale-CMU-Berkeley dataset, which is used for manipulation benchmarks.\n",
      "\n",
      "In the first approach, contact forces are used to adjust the gripper's pose during assembly. To assemble two parts of the airplane, one part is held by the gripper and placed onto another part. If the actuated part is not correctly aligned, rotational velocity is received in the force-torque sensor. Depending on the direction of misalignment different torques emerge, which are reasoned upon to adjust the gripper until the assembly is successful.\n",
      "\n",
      "A second approach aims to identify the actual pose of an object with respect to the robot before manipulating it. By touching the object in the real world the estimate of its pose can be adjusted. To calculate the adjustment a gaming engine is used to estimate an expected contact point between gripper and object, which is compared to the actual contact obtained by the gripper's position in the real world, depending on the arm's configuration during contact. Improving an object's believed pose inherently improves the robot's accuracy when manipulating it.\n",
      "\n",
      "Both procedures are tested with every-day shaped objects, namely the airplane parts. The first approach is evaluated quantitatively in simulation and both approaches on the real-world robot.\n",
      "\n",
      "### <span id=\"page-12-0\"></span>**1.3 Related Work**\n",
      "\n",
      "The following section puts this thesis into perspective with related and similar contemporary work. Two different approaches are formulated to assemble a toy airplane in an uncertain environment. The Yale-CMU-Berkeley Object and Model Set (Calli et al., [2015\\)](#page-82-1) where this airplane is included, has been recently considered in (Yang et al., [2020\\)](#page-84-1) for benchmarking their gripping capability. Therefore, assembling this airplane designs a suitable and contemporary task.\n",
      "\n",
      "Each airplane part is of different shape. Evaluating contact forces between nonprimitive shapes is a complex problem, because contact forces between these shapes are highly irregular. Research on standard Peg-In-Hole tasks (Wong, [1975\\)](#page-84-2), classifying contact forces to adjust gripper positioning for successful execution through deep learning is not applicable, because the observed experiments usually include primitive shapes of cylinders and regular cubes. A much simpler, more generic heuristic classifier, based on the observations in (Bouchard et al., [2015\\)](#page-82-2), is designed to work in collaboration with the underlying cognitive framework CRAM (MÃ¶senlechner, [2016\\)](#page-83-0). Similar to the the work in (Muxfeldt and Kubus, [2016\\)](#page-83-1), classification of the current state of assembly is done by transferring sub-symbolic into symbolic representation, in order to reason upon and react to collisions in a comprehensible way.\n",
      "\n",
      "Assembling the airplane parts can also be described as insertion tasks, since each part needs to fit properly on another. Recent work on industrial insertion tasks makes use of various sensors. In (Schoettler et al., [2019\\)](#page-84-3) for example, a reinforcementlearning based approach is designed, which makes use of image processing to specify rewards. Later, an other approach was release by the same author (Schoettler et al., [2020\\)](#page-84-4). Compared to his previous work, the presented reinforcement-learning technique was adapted to learn insertion in simulation, to decrease the amount of trials in real-world. Similar to his work, this thesis tries to successfully execute insertion tasks under uncertainty, however, the correction of error here is obtained through contact forces, and the underlying mechanisms are embedded within a cognitive framework (CRAM) using symbolic and sub-symbolic reasoning.\n",
      "\n",
      "Tactile perception has recently been used to detect contact events through wiping actions (Stelter, Bartels, and Beetz, [2018\\)](#page-84-5). Stelter's work was considered in this thesis for classification of the gripper's positioning error during assembly, but the diversity of shapes would imply major changes to Stelter's existing system and requires reliable data for training, which is difficult to acquire in my experiments. Classification of shapes and material through tactile arrays, as performed by (Taddeucci et al., [1997\\)](#page-84-6) and (Luo et al., [2017\\)](#page-83-2), is not necessary, since all shapes are known a prior. Instead, the general idea of (Ruiz-Ugalde, Cheng, and Beetz, [2011\\)](#page-84-7) is reflected in this thesis, where the original position of an object is known through image processing, but updating an object's position is done with contact forces. A CRAM specific concept of continuous reaction to conditions is combined with the capabilities of gaming engines to find the position of objects.\n",
      "\n",
      "Incorporating gaming engines into reasoning frameworks is followed in contemporary research, as shown in (Haidu et al., [2018\\)](#page-82-3), where the Unreal Engine is used for bleeding edge reasoning tasks in everyday environment. In this thesis the Bullet Physics Library (Coumans, [2015\\)](#page-82-4) is used to simulate the robot's surroundings, which enables spacial reasoning on the airplane's parts. It provides collision detection and contact point estimation between simulated objects.\n",
      "\n",
      "## <span id=\"page-13-0\"></span>**1.4 Reader's Guide**\n",
      "\n",
      "Over four chapters the contribution of this thesis is presented.\n",
      "\n",
      "**Chapter [2](#page-14-0) Foundations** explains the existing components on which this research is based, including the cognitive machine CRAM, the physics environments Gazebo and Bullet, the RViz visualization tool, constraint- and optimization-based controller, called Giskard and a description of Boxy, the robot used for real-world application.\n",
      "\n",
      "**Chapter [3](#page-28-0) Methods and Implementation** describes the two different approaches that I developed to integrate force response in cognitive procedures for improving robot performance in assembly tasks.\n",
      "\n",
      "**Chapter [4](#page-48-0) Experimental Evaluation** puts both approaches to the test, in simulation as well as in the real world on Boxy, where a toy airplane is to be assembled. Also several difficulties in working with real robots are elaborated.\n",
      "\n",
      "**Chapter [5](#page-62-0) Conclusion** summarizes the presented approaches and their results, discussing the problems dealt with from a wider angle to bring it into perspective for future work.\n",
      "\n",
      "## <span id=\"page-14-0\"></span>**Chapter 2**\n",
      "\n",
      "## **Foundations**\n",
      "\n",
      "All the software stacks and prior implementations used throughout the thesis are explained in this chapter. It contains a general overview of the Robot Operating System (ROS) and a description of Boxy, an autonomous robot which is used throughout all experiments. Controlling Boxy is administered by Giskard, whose involvement is briefly addressed and oriented on the principle of the Naive Kinematics Simulator for robots in simulation, explanation included. Afterwards the main belief state, Bullet Physics Library is shown and how its engine is utilized. Then a visualization through RViz is given, both helpful in simulation and real-world. Also used in this thesis is the Gazebo physics simulator, whose involvement is explained thereafter. Then a thorough explanation is given on the Cognitive Robot Abstract Machine (CRAM), used for designing the procedures described in chapter [3.](#page-28-0)\n",
      "\n",
      "## <span id=\"page-14-1\"></span>**2.1 ROS**\n",
      "\n",
      "The Robot Operating System (Quigley et al., [2009\\)](#page-84-8) is a standard middleware and dynamic framework for communication between components of a larger control system to implement robust robot programs. It is mostly used within research projects in robotics and pitching demonstrations in industrial use. Multiple exchangeable nodes can be designed separately to solve one specific task, be it image processing, localization, joint control, high-level planning, etc., which in collaboration can achieve a greater goal. In this thesis it is used as middleware for communicating the robot state, occupancy map of the environment, object positions, Giskard's administration of all joint controllers, transmission of laser sensor data for localization and force-torque feedback, as well as high-level control and monitoring of all collaborating nodes, while everything above is visualized in RViz.\n",
      "\n",
      "### <span id=\"page-14-2\"></span>**2.2 Boxy**\n",
      "\n",
      "Boxy is a mobile manipulation robot platform. In contrast to the PR2 from Willow Garage (Cousins, [2010;](#page-82-5) Bohren et al., [2011\\)](#page-82-6) or Toyota's HSR (Yamamoto et al., [2018\\)](#page-84-9), Boxy (see figure [2.1\\)](#page-15-0) is assembled by the staff from the Institute for Artificial Intelligence (IAI). Boxy evolved from TUM-Rosie, first described in (Maldonado, Klank, and Beetz, [2010\\)](#page-83-3), assembled in Munich and later built from scratch in Bremen. There are 4 omnidirectional Mecanum wheels driving its base, allowing for holonomic movement. With two LIDAR sensors (Light Detection and Ranging) attached at a front-left and back-right, which provides 360 degree data laser scanner data for localization with the base wheel's odometry, which is estimated by an Adaptive Monte Carlo Localization (AMCL) algorithm. A map of the environment was already present, so no mapping (e.g. through SLAM) was necessary.\n",
      "\n",
      "<span id=\"page-15-0\"></span>![](_page_15_Picture_1.jpeg)\n",
      "\n",
      "FIGURE 2.1: Humanoid robot called Boxy, used for manipulation tasks.\n",
      "\n",
      "On top of Boxy's corpus a UR3 arm from Universal Robots is mounted with an X-Box One Kinect camera for processing 2D images and RGB-D data. During the implementation of this thesis' approach no image processing is performed, though. A torso can be moved prismatically, allowing up and down movement. Two arms are attached to Boxy's torso, both LWR-4+ 7-DOF lightweight arms from KUKA (Hirzinger et al., [2002;](#page-82-7) Burger et al., [2010\\)](#page-82-8). Mounted to the end effector is a sixdimensional Weiss KMS40 force-torque sensor (further called wrench sensor) and a Weiss WSG50 gripper with rubber fingertips to reduce slip. Both, the wrench sensor and gripper are connected via USB 3.0 to the main computer. Base motors communicate over Ethercat, and the LIDAR are connected via Fast Ethernet, while the arms are controlled over an independent Ethernet connection on a SERCOS bus to a designated computer. In combination with KUKAs LWR-4+ fiber-optic data transfer between the joints it allows communication of joint-velocities at up to 500Hz.\n",
      "\n",
      "There are two on-board computers. One controls the LWR-4+ arms on a Debian Machine with real-time kernel. It provides a joint-impedance controller to the ROS network which makes it possible to move the arms from a separate machine, and provides the Links-And-Nodes middleware to manually control an arm's joints. The other computer serves as Ethercat Master to all sensors and base motors. It runs an Ubuntu 18.04 system with a low-latency kernel, using 32 CPU threads and 64 GB of RAM and is connected via fast Ethernet to its slaves in the base. On this machine, called Leela, runs the ROS core and all robot-dependent nodes like robot state broadcast, Emergency-Stop interruption, localization, teleoperation interface, torso, base and neck control, and navigation.\n",
      "\n",
      "In the left image of figure [2.2](#page-16-2) the wrench sensor's six axes are illustrated. On the right its orientation as end effector is shown. The gripper's and sensor's frame have the same orientation in the world. Later in this thesis the gripper's orientation changes throughout different assembly tasks, and with it the expected contact forces when the gripper is in collision with something in the real world.\n",
      "\n",
      "<span id=\"page-16-2\"></span>![](_page_16_Figure_1.jpeg)\n",
      "\n",
      "FIGURE 2.2: Left: Axes of a force-torque sensor (image taken from (Lee et al., [2018\\)](#page-83-4)), with axes adjusted to the particular sensor on Boxy. Right: Orientation of the Weiss KMS40 wrench sensor and WSG50 gripper on the LWR-4+ wrist.\n",
      "\n",
      "## <span id=\"page-16-0\"></span>**2.3 Naive Kinematics Simulator**\n",
      "\n",
      "For comfortable simulation of a robot, the Naive Kinematics Simulator was developed in the IAI. This software is solely used during simulation of robot movement, not in real-world experiments. Its purpose is to have a simple kinematic representation of the robot, operating in ROS. It provides an interface to move not only the prismatic torso, UR3 neck and LWR-4+ arms, but also the robot's base as joint-velocity controllers. By this principle every aspect of a robot can be moved in the same way, and determining the robot's current status is done only through joint states, while every separate controller accounts for a subset of these joints.\n",
      "\n",
      "## <span id=\"page-16-1\"></span>**2.4 Giskard**\n",
      "\n",
      "The constraint and optimization-based motion planner Giskard[1](#page-16-3) can be used for moving a robot. Giskard can consider all joints, from base, torso and arms, into its trajectory calculation to achieve a certain goal represented as cartesian pose or joint state, e.g. when the gripper is to be put at a distant pose, all the usually disjoint controllers can collaborate to move the gripper into the desired position. Figure [2.3](#page-17-1) illustrates this process for the PR2 robot, but is applicable for Boxy as well. On a given goal, Giskard calculates a sequence of joint-velocities for each involved joint, then splits the whole trajectory among the designated controllers, which execute their own joint-trajectory in parallel. For such a system to work, each controller must be designed to expect a trajectory of joint-velocities as input argument, which is an array of velocities per joint over a series of time. Such a controller moves its designated joints per time-step at the given velocity. Before this thesis, Boxy's controllers were addressed separately, without the described administration and motion control by Giskard. For the already available joint controllers on Boxy, new nodes were implemented as intermediate translators between Giskard and these low-level controllers.\n",
      "\n",
      "To calculate trajectories for moving the gripper without causing collision, Giskard has an internal representation, called collision scene, of the robot's physique. Through\n",
      "\n",
      "<span id=\"page-16-3\"></span><sup>1</sup><http://giskard.de/>\n",
      "\n",
      "<span id=\"page-17-1\"></span>![](_page_17_Figure_1.jpeg)\n",
      "\n",
      "FIGURE 2.3: Image courtesy by Georg Bartels et. al. Giskard's procedure for administration of several joint-velocity controllers for the PR2 robot. Upon an incoming request from 'Your application' (bottom-left to top-left) a suitable trajectory for all joints is calculated (top-left) to achieve the requested goal. This trajectory is split per controller (middle) and send as commands (right) to each joint-controller. Interfaces (A) describe ROS actions, (S) services.\n",
      "\n",
      "its URDF description (Unified Robotic Description Format) and all joint states, provided by each optical encoder of a joint, the current configuration of a robot is continuously up-to-date. Furthermore, the collision scene knows of the dimensions of all objects in the environment (see figure [2.4\\)](#page-18-0). An interface allows to update environmental changes as well, e.g. when the position of an object changes. Moving an object through manipulation with the gripper is automatically accounted for as well: by attaching a grasped objects to the gripper's link in the collision scene, the object moves together with the specified link. This allows for collision avoidance even between a held object and another. Giskard's internal collision scene is synchronized with the belief state, a simulated representation of the robot's surroundings in the gaming engine Bullet Physics, which is elaborated in section [2.5.](#page-17-0)\n",
      "\n",
      "During certain trajectories, some joints can be restricted from Giskard's calculations. This functionality is used during precise movements throughout this thesis, especially the base is kept still in some sequences. For that, the interface between our high level control CRAM and Giskard was extended to allow such constraints.\n",
      "\n",
      "### <span id=\"page-17-0\"></span>**2.5 Bullet Physics**\n",
      "\n",
      "The Bullet Physics Library (further abbreviated as Bullet) is a physics engine, designed for the simulation of collision detection, soft and rigid body dynamics (Coumans, [2015\\)](#page-82-4). Bullet provides tools for spatial reasoning and simulation of rigid bodies. In the top-middle image of figure [2.4](#page-18-0) the Bullet world is shown with Boxy in front of the assembly board with all airplane parts. The shapes look relatively rough, because it only uses collision shapes instead of visuals from any CAD model.\n",
      "\n",
      "Each significant object in the environment has its representation in Bullet and collision between then can be properly calculated. Bullet's collision detection alone has a huge benefit when simulating robot movement in cluttered environments, to find suitable positions and trajectories without damaging anything in the real world. In the current setup, however, Giskard takes care of collision avoidance, while Bullet\n",
      "\n",
      "<span id=\"page-18-0\"></span>![](_page_18_Picture_1.jpeg)\n",
      "\n",
      "FIGURE 2.4: Assembly scene with Boxy and all airplane parts on a board. (top-left) RViz with Giskard collision shapes, (top-middle) Bullet Physics. The black boxes serve as spatial constraints for collision avoidance when moving the head. (top-right) Moving *bottomwing* object part with the gripper. (bottom) Battat airplane parts, included in the YCB dataset.\n",
      "\n",
      "is mainly used to update Giskard's collision scene, for object-related reasoning and determining contact points between rigid bodies.\n",
      "\n",
      "Since the robot state is published over the whole ROS network, Bullet can adjusts its internal knowledge on Boxy's configuration as well. Instead of a continuous synchronization, as in Giskard, Bullet updates its robot state only after every terminated movement. This is due to Bullet's original purpose for our high-level control CRAM, where the simulated robot can rapidly teleport between states to immediately check if certain configuration are causing collisions. Also Bullet's robot state can be updated imperatively, e.g. after the robot was moved through teleoperation. Because Giskard takes care of motion planning, this restriction doesn't influence performance or accuracy, as long as both Bullet and Giskard are kept in sync.\n",
      "\n",
      "Simulation of object manipulation is done in Bullet as in Giskard, where a simulated shape is attached to the gripper's as part of the robot's URDF, moving both simultaneously as can be seen in the top-right image in figure [2.4.](#page-18-0) With CRAM in Bullet, moving an object in the gripper is not continuous like in Giskard, but discretely done by teleporting an object along the same transformation, as its origin. But not only the gripper can do this; two objects in the environment can be attached to each other in various ways. Latest developments in the CRAM-Bullet interface allow loosely attached objects, simulating, for example, the transport of multiple items on a tray, or moving a surface with several objects on top. It is possible to both pick an item from the tray and transport the tray with all objects on top of it. This functionality is used when simulating holders for the airplane parts. Every holder is rigidly attached to the assembly board and each airplane part loosely put on their\n",
      "\n",
      "<span id=\"page-19-2\"></span>![](_page_19_Picture_1.jpeg)\n",
      "\n",
      "FIGURE 2.5: Visual prediction of Giskard's motion plan in RViz.\n",
      "\n",
      "respective holder. Moving the assembly board also moves the holders, as well as their parts, in the same direction, as a chain of attachments. Picking a part from its holder attaches the part to the gripper but does not result in moving the holder or any other part.\n",
      "\n",
      "When the collision shape of two objects are in contact, their point of collision can be retrieved from Bullets simulation easily. When the gripper touches an object in the real world, an estimated contact point is determined in Bullet and compared to the current position of the gripper during contact. The difference between actual and estimated contact point can be used to adjust the touched object.\n",
      "\n",
      "Adjusting an object's position is one of the main topics in this thesis. Since Bullet is used as belief state for the robot's environment and every movement of the gripper relies on the object's positions within Bullet, these position are essential for a successful assembly.\n",
      "\n",
      "### <span id=\"page-19-0\"></span>**2.6 RViz**\n",
      "\n",
      "RViz[2](#page-19-1) is an interactive visualization tool for ROS software. It is capable of showing complex meshes of objects, environments and the robot itself, but can be easily extended to render custom markers. It also allows several convenient features, e.g. specify the initial estimate of a robot's pose in the environment for the localization algorithm, command navigation goals, measure distances and provides interfaces to implement custom functions. An example of RViz' visualization is given in the topleft image of figure [2.4,](#page-18-0) where Boxy stands before the assembly, representing its currently simulated configuration. The green boxes on the table are a part of Giskard's collision scene. The airplane parts in the bottom picture of the same figure show each assembly part without Giskard's bounding box. These parts are publishes as CAD-model markers into RViz' scene to visualize Bullet's current belief state.\n",
      "\n",
      "In this thesis RViz is used for visual monitoring of the current belief state and Giskard's trajectory planner, which is visualized in real time before the robot actually moves, allowing to cancel the process if Giskard's trajectory doesn't seem promising (see figure [2.5\\)](#page-19-2). Custom markers published from high-level control in CRAM augment goal poses and trajectories, which encourages rapid prototyping in simulation and in real world.\n",
      "\n",
      "<span id=\"page-19-1\"></span><sup>2</sup><http://wiki.ros.org/rviz>\n",
      "\n",
      "## <span id=\"page-20-0\"></span>**2.7 Gazebo**\n",
      "\n",
      "For the capability to simulate wrench sensors, I additionally used the Gazebo simulator[3](#page-20-3) , as Bullet does not have this capability. In general, Gazebo has been a central component in physics simulation for robots in ROS environments, since it offers a pretty accurate joint and collision model through implementation of the Open Dynamics Engine[4](#page-20-4) (ODE). In this thesis Gazebo is only used for its capability to simulate wrench sensors, which makes it possible to test the first approach in section [3.2](#page-33-0) within an artificial environment.\n",
      "\n",
      "## <span id=\"page-20-1\"></span>**2.8 CRAM**\n",
      "\n",
      "Coming to the last and most important component, the Cognitive Robot Abstract Machine (MÃ¶senlechner, [2016\\)](#page-83-0) or CRAM in short. CRAM is a computational, cognitive framework for developing high-level robot control programs in Lisp. It is used for monitoring robot behavior and rapid prototyping of robot's actions, and combines a high-level descriptive language for action planning of autonomous robots with sophisticated reasoning mechanisms. An integrated Prolog interpreter enriches symbolic reasoning even further, providing the necessary capability to translate descriptive code into sub-symbolic representations without compromising dynamic resolution.\n",
      "\n",
      "Bullet was included into CRAM by interfacing its C++ library for reasoning about action effects through accurate simulation (MÃ¶senlechner and Beetz, [2013\\)](#page-83-5). CRAM's descriptive plans can be easily tested for robustness through collision detection and physics simulation, and new procedures immediately visualized. The interface between CRAM and Bullet provides a broad tool-set to spawn and move objects, but also complete robots, as Bullet can connect several rigid bodies with different types of joints.\n",
      "\n",
      "In the first place, CRAM provides useful control mechanism to design reactive procedures. Since this framework is to be applied on robots in the real world, failure handling and recovery from faulty states is of grave importance. Also important for robot control is handling processes in different ways. In the following, all mechanisms used in this thesis are explained.\n",
      "\n",
      "#### <span id=\"page-20-2\"></span>**2.8.1 Fluents**\n",
      "\n",
      "Fluents are an elegant way to react upon changes in external input, such as sensor data. Usually, reactive mechanism are achieved through synchronizing multithreaded processes, while certain threads constantly evaluate callbacks from foreign processes. With fluents in CRAM reactive procedures can be designed in convenient ways. A fluent is a data-structure that provides notification services while holding a value which can change over time. They can be instantiated like any other Lisp object. In the code snippet below a fluent is created and stored in a variable.\n",
      "\n",
      "```\n",
      "1 (defparameter *number-fluent* (make-fluent))\n",
      "2 (value *number-fluent*) ;; result: NIL\n",
      "```\n",
      "To get and assign a value to this \\*number-fluent\\* the value function is used. Initially, the fluent's value is NIL, because no other value has been given. The following function sets the \\*number-fluent\\* to a given number from the input argument.\n",
      "\n",
      "<span id=\"page-20-3\"></span><sup>3</sup><http://gazebosim.org/>\n",
      "\n",
      "<span id=\"page-20-4\"></span><sup>4</sup><https://www.ode.org/>\n",
      "\n",
      "```\n",
      "3 (defun countdown-callback (number)\n",
      "4 (setf (value *number-fluent*) number))\n",
      "```\n",
      "As the function's name already implies, some kind of callback is to be set up. Imagine there is a ROS topic called /countdown\\_number that counts down from 10 to 0 every second, then starts again at 10. When subscribing to this topic, each new number can be immediately stored as the fluent's new value.\n",
      "\n",
      "```\n",
      "5 (subscribe \"/countdown_number\"\n",
      "6 \"std_msgs/Int8\"\n",
      "7 #'countdown-callback)\n",
      "```\n",
      "This subscriber's sole work here is to call countdown-callback every time a new number is published. Since it runs in a background process, the current thread is not blocked. Checking the fluent's value occasionally will now yield the current countdown.\n",
      "\n",
      "```\n",
      "8 (value *number-fluent*) ;; result: 2 (assumed)\n",
      "9 (sleep 1.0)\n",
      "10 (value *number-fluent*) ;; result: 1\n",
      "11 (sleep 1.0)\n",
      "12 (value *number-fluent*) ;; result: 0\n",
      "13 (sleep 1.0)\n",
      "14 (value *number-fluent*) ;; result: 10\n",
      "```\n",
      "As long as new message are published on /countdown\\_number the \\*number-fluent\\* keeps updating its value. While this process goes on, other tasks can be accounted for until the fluent is needed. Because the fluent contains a value, it is called a valuefluent. Instead of waiting a second for each new value, the pulsed function can be used, which creates a condition-fluent that fires an event every time a value-fluent changes its value. This function immediately fires an event if the fluent is at least initialized, afterwards it only does when the value changes. In combination with the whenever control macro, the above code can be meld into something more elegant. Line 16 is executed every time the \\*number-fluent\\* is pulsed.\n",
      "\n",
      "```\n",
      "15 (whenever ((pulsed *number-fluent*)) ;; blocks thread infinitely\n",
      "16 (print (value *number-fluent*))) ;; prints ..., 3, 2, 1, 0, 10, 9, ...\n",
      "```\n",
      "When a procedure should wait for a certain condition to arise, the wait-for macro can be useful. Comparing a value-fluent to another number creates a conditionfluent, which fires an event each time it is true. Such a junction is called a fluentnetwork. Reacting to condition-fluents is the main goal when working with valuefluents to create reactive procedures.\n",
      "\n",
      "```\n",
      "17 (wait-for (= *number-fluent* 0)) ;; blocks thread until fluent is zero\n",
      "18 (print \"Liftoff.\")\n",
      "```\n",
      "For this demonstration the two previous mechanisms are combined with the pursue macro, which allows execution of multiple threads in parallel. When one of the threads terminates, the others are interrupted and terminate as well.\n",
      "\n",
      "```\n",
      "19 (wait-for (< *number-fluent* 5)) ;; blocks thread until fluent is below 5\n",
      "20 (pursue ;; executes two threads simultaneously\n",
      "21 (whenever ((pulsed *number-fluent*)) ;; blocks infinitely\n",
      "22 (print (value *number-fluent*))) ;; prints 4, 3, 2, 1 and maybe 0\n",
      "23 (and (wait-for (= *number-fluent* 0)) ;; blocks until fluent is zero\n",
      "24 (print \"Liftoff\"))) ;; prints, then terminates both threads\n",
      "```\n",
      "New value-fluents can also be created from an already existing value-fluent. These new value-fluents are updated with every pulse of the original fluent. To create such a fluent-network, the original value-fluent is used as argument in a function, supported by the fl-funcall macro.\n",
      "\n",
      "```\n",
      "25 (defparameter *negative-number-fluent* (fl-funcall #'- *number-fluent*))\n",
      "26 (whenever ((pulsed *number-fluent*)) ;; blocks thread infinitely\n",
      "27 (print (value *number-fluent*)) ;; prints ..., 2, 1, 0, 10, 9,...\n",
      "28 (print (value *negative-number-fluent*))) ;; prints ...,-2,-1, 0,-10,-9,...\n",
      "```\n",
      "Such fluent-networks can also be created on-the-fly, since a fluent is always administered by CRAM in the background. Binding fluents to parameters is usually done to maintain persistence, e.g. when initially binding a fluent to a topic's value. In the following is a last example with a different data-type. Since most sensor data doesn't consist of a single value and this thesis works mostly with wrench sensors, the following fluent contains the geometry\\_msgs/WrenchStamped message type. To compare such structures, they must be decomposed first. The following function returns the sum of the absolute value of all directional forces from such a message.\n",
      "\n",
      "```\n",
      "29 (defun aggregate-force (wrench-message)\n",
      "30 (with-fields ((f-x (x force wrench))\n",
      "31 (f-y (y force wrench))\n",
      "32 (f-z (z force wrench))) wrench-message\n",
      "33 (+ (abs f-x) (abs f-y) (abs f-z))))\n",
      "```\n",
      "As previously shown with the fl-funcall macro in line 25, a fluent containing wrench data can be called equivalently to create a new value-fluent. In this example the fluent is named \\*wrench-fluent\\* and the thread is blocked by wait-for until the sum of all forces is above 1Nm.\n",
      "\n",
      "```\n",
      "34 (wait-for (> (fl-funcall #'aggregate-force *wrench-fluent*) 1))\n",
      "35 (print \"Contact detected.\")\n",
      "```\n",
      "Yes, this is the mechanism to detect contact. Later, in section [3.2,](#page-33-0) this procedure is further elaborated.\n",
      "\n",
      "#### <span id=\"page-22-0\"></span>**2.8.2 Designators**\n",
      "\n",
      "Designators are essential constructs for designing abstract plans. Instead of discrete values they describe actions, motions, objects and locations as symbols, containing general information about their purpose.\n",
      "\n",
      "A designator is interpreted by the Prolog engine, gathering all missing information from Bullet, the robot-description and its current state, environmental constraints and many more. The interpretation of a description continues until every part, and its inferred designators within, are resolved into discrete arguments. As typical for Prolog, multiple solutions for one description are possible, e.g. when finding a suitable position nearby the assembly board, where the gripper can reach a certain object. In such cases, if the first solution doesn't suffice, the next one is probed until it does, which is simulated in Bullet before real-world execution. This procedure is called 'fast projection' which rapidly teleports the robot into several configuration by a simple inverse-kinematic solver, to check Bullet for feasibility. In this thesis, however, Giskard is used for motion planning, therefore no projection is done.\n",
      "\n",
      "Three major types of designators are distinguished: Location, Object and Action. There are also designators called Motions, which are the termination point of Prolog's interpretation for Action-Designators, leading directly to robot-specific functions. Because each designator is interpreted by, Prolog need to know how to distinguish between descriptions (keywords) and values. Those parts of a designator with discrete information have a question-mark as first character, to signalize Prolog that these values can interpreted immediately.\n",
      "\n",
      "**Motion Designators** are named after their most frequent use, but can refer to any kind of low-level platform-dependent function. These lowest-level designators are the result of higher-level designators being completely resolved by the Prolog interpreter, therefore each Motion Designator only contains discrete information. Each such designator is mapped by so-called process modules to platform-dependent implementations. In this thesis, all process modules for controlling robot movement are directed to the CRAM-Giskard interface, while information about objects and locations is provided by Bullet, flavored with predefined parameters in CRAM. Suitable grasping position, for example, are described w.r.t. each object-type. When transitioning from one platform to another, only the process modules' endpoints need to be implemented, which are the interface between CRAM and platform-specific implementations.\n",
      "\n",
      "**Object Designators** cover the description of things in the world. Every object can be described by their name or certain features. The following will yield any object of the type bolt, of which at least five exist in the assembly scenario.\n",
      "\n",
      "```\n",
      "1 (an object\n",
      "2 (type :bolt))\n",
      "```\n",
      "To get one specific bolt, its name can be used. Here the third bolt is searched for.\n",
      "\n",
      "```\n",
      "1 (an object\n",
      "```\n",
      "\n",
      "```\n",
      "2 (name :bolt-3))\n",
      "```\n",
      "The two designators above are only descriptions, which need a Motion Designator to resolve them into discrete values. Instead of actually perceiving each object by visual classification, their representation is fetched from Bullet's belief state with the following Motion Designator.\n",
      "\n",
      "```\n",
      "1 (a motion\n",
      "2 (type world-state-detecting)\n",
      "3 (object (an object\n",
      "4 (type :bolt))))\n",
      "```\n",
      "Furthermore, objects from certain locations can be acquired, e.g. on the assembly board. For this, several designators can be nested.\n",
      "\n",
      "**Location Designators** are required for such spacial reasoning. They are capable of describing poses, which are interpreted immediately, and regions, which depend on the task at hand.\n",
      "\n",
      "```\n",
      "1 (an object\n",
      "2 (location (a location\n",
      "3 (on (an object\n",
      "4 (name :assembly-board))))))\n",
      "```\n",
      "When placing an object onto the assembly board, only the inner location designator is needed to generate a region of possible poses. CRAM then tries to place the object on any possible position within the specified region through fast projection. Since in this thesis every part is meant to be assembled on one specific object, no such descriptions are used. Instead, a location can be determined by a certain pose. In this example a static pose is declared in a local variable a prior.\n",
      "\n",
      "```\n",
      "1 (let ((?goal-pose (make-pose-stamped \"map\" (ros-time)\n",
      "2 (make-3d-vector 1 2 3)\n",
      "3 (make-identity-rotation))))\n",
      "4 (a location\n",
      "5 (pose ?goal-pose)))\n",
      "```\n",
      "**Action Designators** are required from here on, be it to move certain robot parts into position, call image processing, opening/closing the gripper or similar processes. The latest design of action designators was published in (Kazhoyan and Beetz, [2017\\)](#page-83-6) and further developed in (Kazhoyan and Beetz, [2019\\)](#page-83-7). Moving the robot's base in front of the assembly-board is designed like this.\n",
      "\n",
      "```\n",
      "1 (an action\n",
      "2 (type going)\n",
      "3 (target (a location\n",
      "4 (in-front-of (an object\n",
      "5 (name :assembly-board))))))\n",
      "```\n",
      "For this thesis three new actions are developed, which all move the gripper under different conditions: reaching, pushing, retracting. The circumstances define how Giskard calculates its trajectory towards the given goal. The reaching action is used to bring the gripper in a general position nearby its actual target. During this movement to one specific pose it depends on the task at hand, whether certain joints need to be restricted from moving. In the example below, the base and torso joints are restricted, such that only the arm is considered by Giskard. Such restrictions are required in the second approach in section [3.3.](#page-39-0) Also, any potential collision should be avoided between the gripper's current position and the goal (line 8).\n",
      "\n",
      "```\n",
      "1 (an action\n",
      "2 (type reaching)\n",
      "3 (pose ?start-pose)\n",
      "4 (constraints (\"odom_x_joint\"\n",
      "5 \"odom_y_joint\"\n",
      "6 \"odom_z_joint\"\n",
      "7 \"triangle_base_joint\"))\n",
      "8 (collision-mode :avoid-all))\n",
      "```\n",
      "When pushing, however, the gripper is expected to come in collision with other objects, therefore all collisions are allowed, but the base is still restricted. It would also be possible to only allow collision between specific objects, but since multiple parts of the airplane can overlap, determining which collision to allow is hard. This action receives a trajectory of poses, which the gripper approaches sequentially.\n",
      "\n",
      "```\n",
      "1 (an action\n",
      "2 (type pushing)\n",
      "3 (left-poses ?trajectory)\n",
      "4 (constraints (\"odom_x_joint\"\n",
      "5 \"odom_y_joint\"\n",
      "6 \"odom_z_joint\"))\n",
      "7 (collision-mode :allow-all))\n",
      "```\n",
      "Lastly, retracting from a task is done with full freedom of base movement. Even though the gripper's bounding box is probably still in collision with some other shape in Giskard's collision scene, full avoidance wanted. This causes Giskard to first remove the gripper out of the collision safely, before approaching the given retracted pose.\n",
      "\n",
      "```\n",
      "1 (an action\n",
      "2 (type retracting)\n",
      "3 (pose ?retracted-pose)\n",
      "4 (constraints nil)\n",
      "5 (collision-mode :avoid-all))\n",
      "```\n",
      "Per action, the constraints and collision preference is implied through the Prolog interpreter. Therefore they won't be mentioned in the implementations within section [3.](#page-28-0) In the next section fluents and designators are brought together in CRAM's failure handling mechanism.\n",
      "\n",
      "### <span id=\"page-25-0\"></span>**2.8.3 Self-Recovering Plans**\n",
      "\n",
      "Some CRAM macros like pursue enable multi-threading, others cause interruptions to the main thread. The common-lisp error handler can't proficiently account for such conditions, therefore CRAM offers its own failure-handling strategies. An example of CRAM's failure handling mechanism is shown in listing [1.](#page-25-1)\n",
      "\n",
      "```\n",
      "1 (with-retry-counters ((retries 100))\n",
      "2 (with-failure-handling\n",
      "3 ((force-detected (e)\n",
      "4 (print e)\n",
      "5 (do-retry retries\n",
      "6 (setf ?trajectory (calculate-new-trajectory-from ?trajectory))\n",
      "7 (an action\n",
      "8 (type reaching)\n",
      "9 (pose ?start-pose)\n",
      "10 (retry)))))\n",
      "11 (pursue\n",
      "12 (perform\n",
      "13 (an action\n",
      "14 (type pushing)\n",
      "15 (left-poses ?trajectory)))\n",
      "16 (and (wait-for (fl-funcall #'sum-forces *wrench-fluent*))\n",
      "17 (fail 'force-detected)))))\n",
      "```\n",
      "LISTING 1: Self-recovering Plan example code: Main section to achieve (11-17), including gripper movement (12-15) with potentially raising a condition (16-17). Condition-handler (3-10), including changing the trajectory (6), retracting the gripper (7-9) and retrying the main section (10).\n",
      "\n",
      "**with-failure-handling** works mostly like any try-catch mechanism in languages like Java or C++, except it can handle multi-threaded processes and complex macro dynamics when operating with fluents and actions. Its implementation is based on the common-lisp case-handler. What makes it shine is the ability to invoke restarts on failed sequences. This enables a procedure to recover from the failed state before trying again. In this thesis the retrying behavior is exploited by reacting to contact events and changing the sequence's parameters before invoking it again. Listing [1](#page-25-1) shows a rudimentary example for the with-failure-handling mechanism.\n",
      "\n",
      "In contrast to failure-handling in other languages, the catch-block is defined before the critical sequence, so invoking the procedure in said listing jumps to line 11 first. It tries to move the gripper along a trajectory of poses. If a contact event is detected the force-detected condition is handled, then the main sequence is invoked again. The whole procedure only terminates when the gripper has passed all poses of the trajectory without causing collision.\n",
      "\n",
      "**with-retry-counters** offers to terminate procedures after a number of retries. If a procedure takes too many attempts without success it can be terminated. After a maximum of 100 attempts (line 1) the procedure in listing [1](#page-25-1) doesn't handle the force-detected condition anymore. Usually such procedures are nested within more general plans, which can react upon the propagated error themselves on a higher level, e.g. by moving the base to a more promising position before invoking listing [1](#page-25-1) again.\n",
      "\n",
      "## <span id=\"page-28-0\"></span>**Chapter 3**\n",
      "\n",
      "## **Methods and Implementation**\n",
      "\n",
      "Assembly tasks are heavily dependent on precise knowledge of the robot's surroundings, especially in an uncertain environment. Robots are limited in precision, because they can only be as accurate as digital encoding simulates the actual robot. Performing assembly tasks has been present for decades, concentrating on the transition from human demonstration to programmable procedures (Takahashi and Ogata, [1992\\)](#page-84-10) and is still a contemporary topic, now including humans in collaborative scenarios (Hietanen et al., [2020;](#page-82-9) Sun et al., [2020\\)](#page-84-0). It is the procedure of putting two objects together, usually done by manipulating one object, held in a gripper, and moving it on or in an other object. This is easy for a human, because we feel each change in movement of the objects we hold in our hands and see its orientation, can blindly find keyholes with our fingertips and feel when the key fits. We can even predict if two object could fit together just by looking at them. For robots however, it is an immensely difficult task.\n",
      "\n",
      "Usually a robotic arm like the KUKA LWR series or Universal Robots is attached rigidly to a table when executing precise object manipulation (Khansari-Zadeh, Klingbeil, and Khatib, [2016;](#page-83-8) Inoue et al., [2017;](#page-82-10) Stelter, Bartels, and Beetz, [2018\\)](#page-84-5). In this thesis the humanoid robot Boxy is used in a real-world environment (see figure [2.1\\)](#page-15-0). The robot can move holonomically (Kyung-Lyong Han et al., [2009\\)](#page-83-9), while estimating its position with one LIDAR sensor in the base by using Augmented Monte Carlo Localization (Hiemstra and Nederveen, [2007\\)](#page-82-11). It has a 7 degree of freedom KUKA LWR-4 arm, attached to a prismatic torso joint. Especially the moving base makes maintaining precision difficult. Since the robot's position can only be estimated, be it by LIDAR, acoustic, 2D or 3D image processing, the data and its interpretation provides accuracy to a certain degree, depending on their respective quality.\n",
      "\n",
      "In this thesis Boxy is tasked to assemble a toy airplane from Battat[1](#page-28-1) which is included in the YCB dataset (Calli et al., [2015\\)](#page-82-1). The airplane parts can be seen in figure [3.1](#page-29-0)[2](#page-28-2) . Using the YCB dataset and assembling this type of toy airplane is viewed as a benchmark test for contemporary assembly tasks, because is provides objects of different shapes, color, weight and rigidity, making the task more or less difficult, depending on the subset of parts considered in experiments, and the gripper used for manipulating them. Each plane part is held by a custom 3D-printed holder. Compared to peg-in-hole tasks this scenario is directed at diverse approaches on every-day shaped objects. The airplane's parts are placed on an MDF board, where the 3D-printed holders are screwed upon, holding the parts at positions where Boxy, our robot, can reach them.\n",
      "\n",
      "<span id=\"page-28-2\"></span><span id=\"page-28-1\"></span><sup>1</sup><https://battattoys.com/product/battat-take-apart-airplane/>\n",
      "\n",
      "<sup>2</sup>Downloaded on 03.11. from [https://battattoys.com/wp-content/uploads/2018/12/BT2517\\\\_](https://battattoys.com/wp-content/uploads/2018/12/BT2517_ToyPlanePieces_02.jpg) [ToyPlanePieces\\\\_02.jpg](https://battattoys.com/wp-content/uploads/2018/12/BT2517_ToyPlanePieces_02.jpg)\n",
      "\n",
      "<span id=\"page-29-0\"></span>![](_page_29_Picture_1.jpeg)\n",
      "\n",
      "FIGURE 3.1: From left to right: Screwdrivers, vertical plane holder, bolts, upper-body, top-wing, windshield, nuts, front wheels, underbody with grill and propeller, rear-wing on horizontal plane holder, bottom-wing, chassis.\n",
      "\n",
      "To achieve this task the robot must know of its surroundings and positions of the airplane's parts through a belief state, which is a simulated representation of the real world. Unfortunately, even with an immensely precise believe state with respect to the real world, intricate tasks are still hard to accomplish if the robot's estimated state lacks precision.\n",
      "\n",
      "The first idea to overcome inaccuracy in uncertain environments for intricate tasks is similar to a reverse peg-in-hole task, since the airplane's assembly mostly consists of hollow object being put on holders and pegs. If an object is already held in the gripper and then assembled onto its target, contact forces occur which can be reasoned upon. Many before had this idea, basically since peg-in-hole tasks came to life several decades ago [3](#page-29-1) . Nevertheless, an approach was designed to adjust the gripper depending on what forces occur on collisions during assembly, to incrementally adjust the gripper into correct position. The procedure works on paper and in simulation, but is not reliably applicable in real-world. Only during real-world experiments several issues manifested themselves: the influence of friction between the objects in contact, as well as their slipping in the gripper and the spring-like behavior of the LWR-4+ arm joint-impedance controller (see section [4.3.1\\)](#page-57-1).\n",
      "\n",
      "Therefore a second approach is designed. Instead of adjusting the gripper's position, the idea is to find the airplane parts by touching them. Even if a belief state is pretty accurate it only simulates relations between objects in the environment, but the main problem in this thesis is actually the robot's imprecision in the environment. By touching the airplane parts before manipulating them, their position is adjusted in the belief state with respect to the gripper, hence the robot. If an object is touched from several sides its position is estimated even better. Instead of simulating an environment with respect to a fixed point in the real world, the environment is adjusted such that the robot is the point of reference. Manipulating objects precisely can only be done when their position is precise with respect to the robot.\n",
      "\n",
      "The belief state of choice in this thesis is a physics engine called Bullet Physics Library (Coumans, [2015\\)](#page-82-4) coupled with the Cognitive Robot Abstract Machine (CRAM)\n",
      "\n",
      "<span id=\"page-29-1\"></span><sup>3</sup> (Bruyninckx, Dutre, and De Schutter, [1995;](#page-82-12) Newman, Zhao, and Pao, [2001;](#page-83-10) Sharma, Shirwalkar, and Pal, [2013;](#page-84-11) Inoue et al., [2017;](#page-82-10) Park et al., [2020;](#page-84-12) Jiang et al., [2020;](#page-83-11) Liu et al., [2020\\)](#page-83-12)\n",
      "\n",
      "<span id=\"page-30-0\"></span>![](_page_30_Picture_1.jpeg)\n",
      "\n",
      "FIGURE 3.2: Left: Boxy in front of the assembly board. Right: Gripper and force-torque sensor.\n",
      "\n",
      "in which all procedures presented in this thesis are developed in. The robot manipulates all airplane parts with a gripper attached to the LWR-4+ arm's end effector. Moving the gripper to cartesian poses is planned by Giskard, who calculates joint velocities over time-sequences for each joint involved in the gripper's movement, to move joints from multiple controllers in parallel. With Giskard a cartesian goal for the gripper is rarely out of reach, because Giskard involves the base, torso and arms when finding joint trajectories for all controllers involved, to move each component at once. To move the gripper without collision Giskard has a collision scene, which is an exact copy of the objects in Bullet and continuously synchronized with Bullet's belief state. This collision scene contains bounding boxes for each object to calculate the gripper's movement around them.\n",
      "\n",
      "Bullet receives the robot state including localization through ROS, enabling fast prototyping when a robot is in simulation, while in real-world it updates its internal robot state with an estimate of the robot state from each joints controller. RViz is used to visualize Bullet's belief state, Giskard's collision scene, and the current robot state first-hand from the joint controllers.\n",
      "\n",
      "Since Bullet is the standard physics engine for CRAM its functionality has grown with the development on CRAM and now provides all necessary capability as belief state regarding spacial reasoning, collision detection and attaching two objects together. Grasping an object attaches it to the gripper, moving both simultaneously, which inherently extends collision-avoidance in Giskard by including the held object into its collision scene. The holders are attached to the board, such that the simulated holders move, when the board moves, therefore the objects attached to the holders as well. It is possible to define attachments loosely, such that objects can be moved from holders without moving the holder itself. Collision detection in Bullet is done by checking for overlapping bounding boxes. Determining the contact point between two objects is done with bounding boxes as well. To increase accuracy in detection of contact points between two objects, their mesh representations are decomposed into a collection of single collision meshes. What influence this has is explained in section [3.3.1.](#page-41-0)\n",
      "\n",
      "Using the existing implementations from the previous chapter, the following part focuses on the core idea of solving this thesis' proposed problem, utilizing force data to improve accuracy in assembly tasks. Two approaches are taken to achieve this: the first one incrementally adjusts the gripper's position while pushing a held object onto the target, the second one adjusts the target's position in the internal world state by touching it from different angles. They are tested and evaluated through a series of assembly tasks, which are explained in the Experimental Evaluation of chapter [4.](#page-48-0)\n",
      "\n",
      "Both approaches react to force data while moving the gripper to the target. Before any force-torque data can be reasoned upon, there needs to be some data to begin with, which is explained in the first part of this chapter. Afterwards the first approach is presented, involving a classification heuristic that determines, how to adjust the gripper's position in order to successfully reach its goal position. This is achieved by evaluating the force data perceived while pressing against the target object with another object held in the gripper. In the second approach force data is used to detect a touching event between the gripper and the object, after which the object's pose is adjusted based on the gripper's position while in contact. The position of all objects interacted with is uncertain to a degree, which is further elaborated in chapters [4](#page-48-0) and [5.](#page-62-0)\n",
      "\n",
      "## <span id=\"page-31-0\"></span>**3.1 Sensor Communication**\n",
      "\n",
      "A Weiss KMS40 force-torque sensor is used. In physics a force and torque vector can be assembled into a screw, a six-dimensional vector constructed from a pair of two tree-dimensional vectors representing linear and angular velocity, commonly known and further called **wrench**. The wrench sensor is attached as a fixed wrist joint between the last link of the left LWR3 arm and the end effector, a Weiss WSG50 gripper (see figure [3.2\\)](#page-30-0). All object manipulation is done solely by the WSG50 gripper, while the KMS40 wrench sensor constantly sends data. A ROS node publishes the sensor data onto a ROS topic. The communicated message geometry\\_msgs/WrenchStamped is shown in listing [2.](#page-31-2)\n",
      "\n",
      "```\n",
      "1 std_msgs/Header header\n",
      "2 uin t 3 2 seq\n",
      "3 time stamp\n",
      "4 s t r i n g frame_id\n",
      "5 geometry_msgs/Wrench wrench\n",
      "6 geometry_msgs/Vec tor3 f o r c e\n",
      "7 f l o a t 6 4 x\n",
      "8 f l o a t 6 4 y\n",
      "9 f l o a t 6 4 z\n",
      "10 geometry_msgs/Vec tor3 torque\n",
      "11 f l o a t 6 4 x\n",
      "12 f l o a t 6 4 y\n",
      "13 f l o a t 6 4 z\n",
      "```\n",
      "LISTING 2: Wrench message: Standard ROS header (1-4). Wrench data (5-13) with directional (6-9) and rotational (10-13) velocities.\n",
      "\n",
      "The wrench data can be zeroed by calling the ft\\_cleaner/update\\_offset service which is wrapped in the Lisp function (zero-wrench-sensor). Zeroing the data is important, because the real wrench sensor accumulates a drift over time, especially when the gripper's orientation changes, since its inertial calibration has limited precision. Also, if the gripper holds an object, the object's weight influences the sensor, and releasing the object changes wrench again.\n",
      "\n",
      "#### <span id=\"page-31-1\"></span>**3.1.1 Using the Wrench-Sensor in CRAM**\n",
      "\n",
      "With the sensor data published into the ROS network, any node can subscribe to it. Within CRAM, there is a specific data structure for representing continuously changing data called fluents. When subscribing to a ROS topic a subscriber updates the fluent with each new message, the synchronization is taken care of by the data structure. Detailed explanation of fluents in CRAM is given in section [2.8.1](#page-20-2) and in the original work on CRAM (MÃ¶senlechner, [2016\\)](#page-83-0).\n",
      "\n",
      "Some functions in this thesis rely heavily on the wrench sensor. Listing [3](#page-32-0) describes a mechanism to verify that the sensor is transmitting data.\n",
      "\n",
      "```\n",
      "1 (defun fl-active (&optional (fluent *wrench-state-fluent*)\n",
      "2 (sample-rate *fl-default-sample-rate*))\n",
      "3 (declare (type cpl:value-fluent fluent))\n",
      "4 (let ((init-pulse-passed nil)) ;; for better readability\n",
      "5 (pursue\n",
      "6 (whenever ((pulsed fluent))\n",
      "7 (if init-pulse-passed\n",
      "8 (return T)\n",
      "9 (setf init-pulse-passed T)))\n",
      "10 (progn (sleep sample-rate)\n",
      "11 nil))))\n",
      "12\n",
      "13 (defun fl-gate (&optional (fluent *wrench-state-fluent*)\n",
      "14 (sample-rate *fl-default-sample-rate*))\n",
      "15 (declare (type cpl:value-fluent fluent))\n",
      "16 (unless (fl-active fluent sample-rate)\n",
      "17 (error \"The fluent ~a is inactive.\" fluent)))\n",
      "```\n",
      "LISTING 3: Wrench-fluent pulse monitor to determine the wrenchsensor's activity. Function fl-active (1-11) returns T if the fluent receives data. This is done by checking the fluent's pulse. fl-gate (13-17) raises an error if the fluent is dead.\n",
      "\n",
      "If the sensor dies, the robot might cause some serious damage, because a procedure might wait for a collision event to occur, but would never get the contact event to react upon. Therefore a safety precaution is implemented, the fl-gate. Every fluent emits a pulse when its value is updated, like a heartbeat. The implementation in fl-active (lines 1-11) listens to a fluent's pulse to determine if it is still alive, fl-gate uses this mechanism to throw an error if the fluent is dead (lines 13-17). Since these mechanisms are mostly used to check if the wrench sensor is sending new data, their default input argument is the \\*wrench-state-fluent\\*, although it can be used for any kind of value-fluent.\n",
      "\n",
      "<span id=\"page-33-2\"></span>![](_page_33_Figure_1.jpeg)\n",
      "\n",
      "FIGURE 3.3: Strongest torques based on gripper offset. Top-left: +X offset: +Y torque. Top-right: -X offset: -Y torque. Bottom-left: +Y offset: -X torque. Bottom-right: -Y offset: +X torque.\n",
      "\n",
      "### <span id=\"page-33-0\"></span>**3.2 Approach 1: End Effector Pose Adjustment**\n",
      "\n",
      "This is the first approach taken to correct inaccuracy when assembling two parts together. One object is held in the robot's gripper, while the robot is tasked to put the object onto another one.\n",
      "\n",
      "Through this approach five different pairs of objects are assembled, all by putting a held object downwards onto a target. First the held object is positioned above the target, then it moves down to the assembly's goal position. During downward movement contact between the held and target object is expected if the gripper's position is not accurate along the X-Y plane. As explained in (Bouchard et al., [2015\\)](#page-82-2) each two colliding objects yield torques with relation to their point of contact and applied linear force. In Bouchard's work the held object changes orientation due to frictional forces while applying linear velocity upon them, thus torques are applied to resist the objects urge to rotate. Compared to this approach, pushing two misaligned objects together results in rotational movement of the object held, which is transferred to the gripper and thus the wrench sensor. Depending on the direction of misalignment the two object's contact point differs, therefore different torque emerges.\n",
      "\n",
      "During contact the torques are evaluated, then the gripper retracts from contact by moving upwards. It repeats moving into contact and back again, until enough contact forces are gathered, before adjusting the gripper's pose against the most frequenting classified offset. Then this process starts again, classifying new contact forces with the adjusted gripper. After several repetitions the gripper incrementally adjusts to a position, which eventually allows successful assembly.\n",
      "\n",
      "#### <span id=\"page-33-1\"></span>**3.2.1 Torques in Simulation**\n",
      "\n",
      "Before heading into the real world, this scenario is simulated in Gazebo (described in section [2.4\\)](#page-16-1) as visualized in figure [3.3.](#page-33-2)\n",
      "\n",
      "The Gazebo simulation models the robot's arm wrist (cylinder) as kinematic body, unaffected by forces, an object held in the gripper (upper cube) which **is** affected by force as dynamic body, and a static collision object (lower cube) is unaffected again. The wrist and gripper including the object are connected via a fixed joint containing a simulated wrench sensor. When the wrist is moved, the upper cube (gripper & object) moves as well, but when the upper cube touches the static bottom cube, only the upper cube moves and presses against the wrench sensor.\n",
      "\n",
      "In each of the four images the object in the gripper is pressed against the lower cube with offsets in four directions. When comparing the corresponding torques around X and Y with the offset, each case yields easily distinguishable results. In each graph, only torque around X and Y is shown, because linear forces and torque around Z are not as strongly affected as X and Y torques.\n",
      "\n",
      "The bottom two images in figure [3.3](#page-33-2) depict offsets of the gripper along positive and negative Y with the corresponding changes in torque around X and Y. In both scenarios the X torque (*TX*) is stronger than around Y (*TY*), with a negative *T<sup>X</sup>* in the left, and positive *T<sup>X</sup>* in the right image. Opposed to that are the top two scenarios, where *T<sup>Y</sup>* is stronger when the gripper has an offset along the X axis. This gives the idea, that a heuristic can be modeled to classify the general direction of an offset, based on torques.\n",
      "\n",
      "#### <span id=\"page-34-0\"></span>**3.2.2 Heuristic Classifier**\n",
      "\n",
      "With the observation made in the previous section [3.2.1](#page-33-1) a simple heuristic is developed. Its goal is to determine in which direction the end effector needs to be adjusted on the X-Y plane when approaching the goal position downwards, in order to successfully put one object onto another. The heuristic is shown in listing [4](#page-34-1) and takes the \\*wrench-state-fluent\\* value as input argument, which contains the current data from the wrench-sensor.\n",
      "\n",
      "```\n",
      "1 (defun heuristic (wrench-msg)\n",
      "2 (with-fields ((fz (z force wrench))\n",
      "3 (tx (x torque wrench))\n",
      "4 (ty (y torque wrench))) wrench-msg\n",
      "5 ;; Gripper must push with reasonable force\n",
      "6 (when (> fz 1.0)\n",
      "7 (if (> (abs ty) (abs tx))\n",
      "8 ;; If Y-torque is dominant, offset in X\n",
      "9 (if (< 0 ty)\n",
      "10 :+x-off\n",
      "11 :-x-off)\n",
      "12 ;; If X-torque is dominant, offset in Y\n",
      "13 (if (< 0 tx)\n",
      "14 :-y-off\n",
      "15 :+y-off)))))\n",
      "```\n",
      "LISTING 4: Code of the heuristic classifier. Extracts linear force for contact detection, and torques around X and Y (2-4) from the wrench message. Given enough contact force (6) high torque around Y (7) means offset in X (9-11), while torque around X suggests Y offset (13- 15). Arrangement of contact force axis and expected torques for the\n",
      "\n",
      "sensor's orientation in the Gazebo demo of section [3.2.1.](#page-33-1)\n",
      "\n",
      "Each value returned (e.g. :+x-off) is the identifier for the detected offset, based on the angular velocities (torques) around X and Y. Depending on the end effector's orientation during assembly, the order of compared torques must be adjusted. This heuristic only determines an offset in one of four directions, +/-X and +/-Y. If the gripper is misaligned diagonally, only one of the axes is declared for adjustment. Since the gripper adjusts its pose incrementally over several trials, a diagonal misalignment will result in alternating offsets between two axes. The procedure in section [3.2.3](#page-35-0) accounts for this behavior.\n",
      "\n",
      "#### <span id=\"page-35-0\"></span>**3.2.3 Procedure**\n",
      "\n",
      "This is the core implementation of approaching the goal pose while constantly adjusting the gripper's trajectory towards the goal. A diagram in figure [3.4](#page-35-1) shows the process while the pseudo-code is given in listing [5.](#page-36-2)\n",
      "\n",
      "<span id=\"page-35-1\"></span>![](_page_35_Figure_4.jpeg)\n",
      "\n",
      "FIGURE 3.4: Process diagram for end effector pose adjustment.\n",
      "\n",
      "The procedure changes the trajectory towards the goal, based on the heuristic's results during contact events. It takes a trajectory of poses as input and approaches each one sequentially, while cautiously checking for contact events. Any time a collision is detected, the whole trajectory is pushed in the direction recommended by the heuristic classifier. Finally the function ends when the last pose is successfully reached.\n",
      "\n",
      "The gripper must hold an object in its hand already to assemble it onto the target. This is done by moving the gripper to a pose above to the target object, before moving down towards the target by following a trajectory of poses sequentially, without causing contact forces. Three input parameters are used for this recursive pseudo-implementation: (1) a list of poses (trajectory) to be followed, (2) movement constraints, which are generally used to keep the robot's base still during all gripper movements, (3) and a list of classified results. The results are empty in the beginning and to be filled during each recursive call, by classifying offsets with the heuristic explained in [3.2.2.](#page-34-0)\n",
      "\n",
      "The procedure begins by bringing the gripper to the trajectory's first pose. Then it verifies the wrench sensor's correctness (lines 7-10) as described in section [3.1.1.](#page-31-1) After zeroing wrench data, the gripper tries to follow all poses of the trajectory (lines 11-12) while simultaneously checking for contact (line 13). If a contact occurs, a condition is raised and caught below (line 18). The offset is classified using the current wrench data (line 19). If this classified offset is dominant (over 50% occurrence) among other recorded offsets (line 23), the trajectory's poses are adjusted towards correcting the offset (line 25). After too many results the record is cleaned to start anew (lines 24-25), because finding a dominant result among a lot of data by counting occurrence ratio gets incrementally harder. Then the whole trajectory is recalculated (line 26), whether or not a dominant offset was found. This recalculated\n",
      "\n",
      "```\n",
      "1 retries = 100\n",
      "2 def follow_trajectory(trajectory, constraints, results):\n",
      "3 first_pose = trajectory[0]\n",
      "4 move_gripper(first_pose, constraints)\n",
      "5 if retries-- > 0:\n",
      "6 try:\n",
      "7 if not fl_active(wrench_fluent):\n",
      "8 raise WrenchFluentDead\n",
      "9 zero_wrench(wrench_fluent)\n",
      "10 in_parallel:\n",
      "11 for pose in trajectory:\n",
      "12 move_gripper(pose, constraints)\n",
      "13 if force_on_axis(wrench_fluent, 'f_z') > 1.0:\n",
      "14 raise ForceDetected('Contact detected.')\n",
      "15 open_gripper()\n",
      "16 move_gripper(first_pose, [])\n",
      "17 return True\n",
      "18 except ForceDetected:\n",
      "19 offset = classify_offset(wrench_fluent)\n",
      "20 push(offset, results)\n",
      "21 if len(results) > 3 and is_dominant(offset, results):\n",
      "22 results = []\n",
      "23 trajectory = adjust_trajectory_against_offset(trajectory, offset)\n",
      "24 if len(results) > 10:\n",
      "25 results = []\n",
      "26 trajectory = recalculate_trajectory(trajectory)\n",
      "27 follow_trajectory(trajectory, constraints, results)\n",
      "28 except WrenchFluentDead:\n",
      "29 print('Wrench sensor is dead.')\n",
      "30 return False\n",
      "31 else:\n",
      "32 return False\n",
      "```\n",
      "LISTING 5: Pseudo code for end effector adjustment.\n",
      "\n",
      "trajectory starts just above the gripper's current position and goes until the trajectory's last pose, to shrink the whole trajectory closer to contact. Recalculation also includes randomly changing the distance between the current trajectory's poses, in order to apply different strengths when pushing against the target object on each recursive iteration. This brings variation into the caused wrenches, otherwise there would be a high potential for accumulating miss-classified results. With the new trajectory, calculated through offset-adjustment and rearranging the density of poses, this function calls itself (line 27). The recursive procedure terminates, when the last pose is reached within 100 attempts.\n",
      "\n",
      "#### <span id=\"page-36-0\"></span>**3.2.4 Implementation**\n",
      "\n",
      "To implement the procedure in CRAM some additional mechanisms must be established first: Contact detection and reacting to collisions.\n",
      "\n",
      "#### <span id=\"page-36-1\"></span>**Contact Detection**\n",
      "\n",
      "Fluent variables can design conditions, as described in section [2.8.1.](#page-20-2) The function in listing [6](#page-37-1) decomposes wrench data and returns the value of one specified axis. When an object is held vertically from the top, pushing down results in negative force along the sensor's Z axis. Therefore the linear force along Z is used as indicator for\n",
      "\n",
      "```\n",
      "1 (defun force-on-axis (wrench-msg axis)\n",
      "2 (with-fields ((fx (x force wrench))\n",
      "3 (fy (y force wrench))\n",
      "4 (fz (z force wrench))\n",
      "5 (tx (x torque wrench))\n",
      "6 (ty (y torque wrench))\n",
      "7 (tz (z torque wrench))) wrench-msg\n",
      "8 (nth (position axis '(:fx :fy :fz :tx :ty :tz))\n",
      "9 (list fx fy fz tx ty tz))))\n",
      "```\n",
      "\n",
      "```\n",
      "LISTING 6: Returns the specified axis' value. When called with the\n",
      "     *wrench-state-fluent* this value changes continuously.\n",
      "```\n",
      "contact in assembly tasks with a gripper pointing downwards. To create a condition for contact detection the following fluent fires an event when the linear force along Z is below -1.0.\n",
      "\n",
      "<sup>1</sup> (fl> -1.0 (fl-funcall #'force-on-axis \\*wrench-state-fluent\\* :fz))\n",
      "\n",
      "Zeroing the wrench data before contact detection is imperative to mitigate potential drifts in the wrench data through previous movement of the gripper.\n",
      "\n",
      "#### <span id=\"page-37-0\"></span>**Reacting to collisions**\n",
      "\n",
      "The mechanism in listing [7](#page-37-2) resembles the reaction to a touch-event while the gripper follows a trajectory of ?poses until the last one is reached. As failure handling has been explained in section [2.8.1](#page-20-2) it consists of the handling part (lines 1-10) and the sensitive code raising the condition (lines 11-19).\n",
      "\n",
      "```\n",
      "1 (cpl:with-retry-counters ((retries 10))\n",
      "2 (cpl:with-failure-handling\n",
      "3 ((force-detected (e)\n",
      "4 (print e)\n",
      "5 (cpl:do-retry retries\n",
      "6 (setf classified-offset\n",
      "7 (heuristic (cpl:value *wrench-state-fluent*)))\n",
      "8 (recover-from-collision)\n",
      "9 (adjust-position-with classified-offset)\n",
      "10 (cpl:retry))))\n",
      "11 (pursue\n",
      "12 (and (wait-for (fl> -1.0 (fl-funcall #'force-on-axis *wrench-state-fluent*\n",
      "       ,â†’ :fz)))\n",
      "13 (fail 'force-detected))\n",
      "14 (perform\n",
      "15 (an action\n",
      "16 (type pushing)\n",
      "17 (left-poses ?poses))))))\n",
      "```\n",
      "LISTING 7: Contact event detection (12-15) and handling (3-10) for downwards gripper orientation and movement (17-20).\n",
      "\n",
      "The pursue macro embeds (1) listening to the wrench-fluent (lines 12-15) and (2) moving the gripper (lines 16-19), and executes both processes in parallel. Whichever thread terminates first preempts the other thread, meaning, if (2) all goal ?poses are reached before (1) any force is detected, the force-detected condition is never thrown (line 15). Also, if (1) force is detected before the movement is done, the (2) action is interrupted and the condition is handled.\n",
      "\n",
      "When the force-detected condition is raised, it triggers CRAM's failure handling mechanism to react and recover from the situation (lines 5-10), e.g. by evaluating the contact wrench, retracting from the collision, repositioning the end effector and retrying this segment again.\n",
      "\n",
      "#### <span id=\"page-38-0\"></span>**Assembly Plan in CRAM**\n",
      "\n",
      "The CRAM plan on the next page in listing [8](#page-39-1) implements the pseudo-code from section [3.2.3.](#page-35-0) This code combines the mechanism of contact event detection from section [3.2.4](#page-36-1) with reactive failure handling from section [3.2.4.](#page-37-0) Instead of recursive behavior, like in the pseudo-code example, CRAM failure-handling can be utilized (lines 8-23) to retry the main section from lines 39 to 51.\n",
      "\n",
      "First the main section is explained. The goal of this function is to move the gripper along a trajectory without collision, open the gripper and retract to the start (lines 35-51). To verify the wrench sensor's data, fl-gate and zero-wrench-sensor are used in lines 33 and 34. Moving the gripper and waiting for contact is done in parallel through the pursue macro in line 35. Waiting for a contact event is done with the \\*wrench-state-fluent\\* in lines 36 to 38, where an event is fired when force on Z is below -1. Moving along the trajectory happens in a pushing action-designator, allowing all collision in Giskard's collision scene (line 44).\n",
      "\n",
      "When a contact event is fired, CRAM's failure handling mechanism is invoked (line 8). During contact the classifier obtains the offset and saves the result in a hash-map called \\*results\\*, by incrementing the counter for the corresponding offset (lines 12-14). If the offset is dominant, each pose of the current trajectory is transformed in the opposite direction of the offset (lines 18-21), overwriting the current trajectory with these corrected poses. Then the main section is retried (line 32) with the adjusted trajectory.\n",
      "\n",
      "Eventually the gripper reaches its goal without causing collision, to finally release the object and retract to the beginning (lines 45-51). These movements will not raise an error, regardless of detected forces. A maximum of 100 attempts are allowed for adjusting the trajectory (line 6) after which this procedure fails.\n",
      "\n",
      "```\n",
      "1 (defun follow-trajectory (?trajectory ?constraints)\n",
      "2 (declare (type list ?trajectory)\n",
      "3 (type list ?constraints))\n",
      "4 \"`?trajectory' poses along the path to the goal\n",
      "5 `?constraints' for restricting certain joints during movement\"\n",
      "6 (cpl:with-retry-counters ((attempts 100))\n",
      "7 (cpl:with-failure-handling\n",
      "8 ((force-detected (e)\n",
      "9 (roslisp:ros-warn (assembly follow-trajectory) \"Condition: ~a\" e)\n",
      "10 (sleep 0.5)\n",
      "11 (cpl:do-retry attempts\n",
      "12 (let ((heuristic-result\n",
      "13 (value (fl-funcall #'heuristic *wrench-state-fluent*))))\n",
      "14 (incf (gethash heuristic-result *results*))\n",
      "15 (when (and (>= (total-results 3))\n",
      "16 (is-dominant result heuristic-result))\n",
      "17 (reset-results)\n",
      "18 (setf ?trajectory\n",
      "19 (mapcar (rcurry #'adjust-pose-with-heuristic\n",
      "20 heuristic-result)\n",
      "21 ?trajectory)))\n",
      "22 (when (> (total-results) 10)\n",
      "23 (reset-results))\n",
      "24 (setf ?trajectory (rearrange-trajectory ?trajectory)\n",
      "25 (let ((?starting-pose (first ?trajectory)))\n",
      "26 (perform\n",
      "27 (an action\n",
      "28 (type retracting)\n",
      "29 (pose ?starting-pose)\n",
      "30 (constraints ?constraints)\n",
      "31 (collision-mode :allow-all))))\n",
      "32 (cpl:retry)))))\n",
      "33 (fl-gate *wrench-state-fluent*)\n",
      "34 (zero-wrench-sensor)\n",
      "35 (pursue\n",
      "36 (and (wait-for\n",
      "37 (> -1.0 (fl-funcall #'force-on-axis *wrench-state-fluent* :fz)))\n",
      "38 (cpl:fail 'force-detected :description \"Contact detected.\"))\n",
      "39 (perform\n",
      "40 (an action\n",
      "41 (type pushing)\n",
      "42 (left-poses ?trajectory)\n",
      "43 (constraints ?constraints)\n",
      "44 (collision-mode :allow-all))))\n",
      "45 (open-gripper)\n",
      "46 (let ((?start-pose (first ?trajectory)))\n",
      "47 (perform\n",
      "48 (an action\n",
      "49 (type retracting)\n",
      "50 (pose ?start-pose)\n",
      "51 (collision-mode :avoid-all))))))))\n",
      "```\n",
      "<span id=\"page-39-0\"></span>LISTING 8: CRAM plan for end effector pose adjustment including fluent controlled failure handling. A trajectory of poses is followed by the gripper to deliver an object, open the gripper and retract. If collision is detected, the trajectory is adjusted.\n",
      "\n",
      "<span id=\"page-40-0\"></span>![](_page_40_Picture_1.jpeg)\n",
      "\n",
      "FIGURE 3.5: Adjust holder's position by touching it with a chassis in the gripper. (Top left) Contact in real-world, (top-middle) under-estimating yields no contact point in Bullet, (top-right) over-estimating overlaps the objects, giving wrong contact points, (bottom-left) visualization of pseudo ray casting to estimate the contact point correctly, (bottom-right) chassis in Rviz with directional axis towards the actual contact point on the bounding box' surface.\n",
      "\n",
      "## **3.3 Approach 2: Object Pose Adjustment**\n",
      "\n",
      "As explained in the beginning of this chapter, Bullet Physics is used as belief state for the robot's surroundings. Especially important are the airplane parts' positions, which serve as cartesian references for gripper movement during assembly. When picking up an object, Bullet is asked for the object's position and orientation (together called 'pose') to move the gripper w.r.t. the requested pose. The aim of this approach is to identify the actual pose of an object before manipulating it, by touching it in the real world and adjust its pose in Bullet's believe state according to the gripper's position while in contact.\n",
      "\n",
      "In (Tsujimura and Yabuta, [1989\\)](#page-84-13) an object's shape is determined by touching multiple points on their surface. Contemporary work (Kumar et al., [2020\\)](#page-83-13) finds the complete shape of an object by continuously moving a tactile sensor array along an object's edges. In this approach, each object's shape is known and their position hard-coded with respect to their holder, which is rigidly attached to the assembly board. But even with positions given, the robot's precision decides about success or failure of assembly tasks, therefore these positions are only as accurate as the robot is. To decrease the object pose's uncertainty w.r.t. the robot, the gripper is moved to an over-estimated pose beyond the object's surface until in contact with the desired object (see [3.2.4\\)](#page-37-0). While the gripper is in contact, the real-world contact point is compared to the estimated one from Bullet, in order to adjust the believed pose of the object w.r.t. the gripper.\n",
      "\n",
      "Only offsets on the X-Y plane are of importance for this assembly task, which is elaborated in section [3.2,](#page-33-0) therefore the objects are approached only from the sides, not from the top or below. Since each object's shape is known, this information is used to hard-code a promising axis to approach it on, with relation to the object's orientation. These axes are sought to hit a target at its biggest surface, because smaller surfaces are prone to be missed. An object is approached on either the X or Y axis, with predefined offsets depending on the object-type. These offsets are important when manipulating shapes like the airplane parts, because when touching an object, only one contact point is retrieved, and due to inaccuracy it is unclear where this point actually lies on the target's surface. To adjust the object in both X and Y coordinates, offsets for both axes are preset, such that the gripper can touch an object in two separate procedures from perpendicular sides on a promising surface. To eliminate the necessity of estimating an object's orientation, all holders are oriented in 90 degree steps. This arrangement makes it possible to retrieve promising results after a minimum of two contact events.\n",
      "\n",
      "The actual point of contact is only dependent on the gripper's position while in collision with the object, given by the estimated robot state. On the other hand, the estimated point of contact is retrieved from Bullet, since the robot as well as each object is represented within its belief state. Calculating the difference between realworld and estimated contact yields an offset, with which the object's believed pose can be adjusted. During Experimental Evaluation [4](#page-48-0) the difference between actual and believed poses was averaged to around 2cm, but reached up to 3cm in some cases.\n",
      "\n",
      "#### <span id=\"page-41-0\"></span>**3.3.1 Pseudo Ray casting**\n",
      "\n",
      "Bullet's contact point detection between two objects is usually done by checking if their bounding boxes overlap. This is very helpful to rapidly calculate **if** objects are in collision, but is useless when detecting an actual contact point. Most meshes have a concave collision model, representing a bounding box collision shape in Bullet. Two concave collision shapes are not allowed to overlap their bounding boxes, such that a stack of bowls for example would behave like a pile of boxes, instead of neatly fitting into each other. Asking Bullet for contact points between such shapes always yields a point on the bounding box' surface.\n",
      "\n",
      "To mitigate this problem each objects' mesh is decomposed into a collection of concave collision shapes, called compound-shape. Creating compound-shapes from usual convex-hull shapes is done by Volumetric-Hierarchical Approximate Convex Decomposition (V-HACD), which is implemented for Blender[4](#page-41-1) . Depending on the parameters set for this process, the resulting shapes are more or less rougher that its original (see figure [3.6](#page-42-1) for an example), but now collision points lie on the object's actual surface instead of the bounding box.\n",
      "\n",
      "To raise the complexity a bit, not only bare grippers can adjust object poses like this. Moving a held object into collision with another one is needed to find the correct position for an assembly's target in between picking and placing it. In figure [3.5](#page-40-0) the airplane's chassis is pushed against the holder to adjust the holder's position. Since differences between real-world and believed contact point are explicitly expected, retrieving the contact points from Bullet between the chassis and holder is\n",
      "\n",
      "<span id=\"page-41-1\"></span><sup>4</sup><https://github.com/kmammou/v-hacd>\n",
      "\n",
      "<span id=\"page-42-1\"></span>![](_page_42_Picture_1.jpeg)\n",
      "\n",
      "FIGURE 3.6: Contact points (red dots) by pseudo ray casting the chassis against several surfaces. Top f.l.t.r: Contact point on the pedestal, chassis' axis, holder's rear. Bottom f.l.t.r: Contact point on the bottom-wing from the side close to the front, same side closer to the back, from front.\n",
      "\n",
      "not feasible without further preparation, as can be see in the top three images. Since the believed object's position may not touch or overlap with the target, Bullet's contact point detection yields either none, or unreliable data.\n",
      "\n",
      "Inspired by the concept of ray casting, a solution for non-colliding and overlapping objects in the belief state was developed, to find a well-estimated contact point. Ray casting in general is used in visualization engines, in which 'To visualize and analyze the composite solids modeled, virtual light rays are cast as probes.'(Roth, [1982\\)](#page-84-14). In some way this procedure uses a similar idea to reliably find the estimated collision point between two meshes.\n",
      "\n",
      "In Bullet an object's mesh attached to the gripper is retracted from the collision along the axis which the gripper took to get into collision, then the mesh is moved towards the target object bit by bit until they are in contact. Without holding an object the gripper's fingertips can be represented in Bullet instead, to use their mesh like an object held. This procedure is visualized in the bottom images of figure [3.6.](#page-42-1) Instead of casting rays the whole mesh is cast towards the target of interest.\n",
      "\n",
      "#### <span id=\"page-42-0\"></span>**Procedure**\n",
      "\n",
      "In listing [9](#page-43-1) is an example of the procedure in pseudo-code. It is called from a procedure explained in section [3.3.2](#page-43-0) when the gripper is detected to be in contact with a target object. The input arguments are the held object's name, target name, and the direction on which the gripper is approaching the target. To eliminate changes to the actual belief state, the current Bullet world is copied (line 2). A new vector (direction\\_step) points towards the direction of approach but is set to a length of 0.1cm (line 3). To pull the held object out of a potential overlapping state, it is retracted by 3cm in the opposite direction (lines 5). Over a range of 6cm (line 6) the held object is projected towards the target in 0.1cm steps (line 11). This movement continues until\n",
      "\n",
      "<span id=\"page-43-2\"></span>![](_page_43_Figure_1.jpeg)\n",
      "\n",
      "FIGURE 3.7: Process diagram for touching and adjusting the position of an object.\n",
      "\n",
      "both objects are in contact (line 7-8) which yields a contact manifold, a structure containing both object's bodies as well as their respective collision points. The target's contact point is returned, which terminates the process successfully. Otherwise, an error is thrown if the whole 6cm gone by without any contact, signalizing that the offset must be too high for detecting any contact point.\n",
      "\n",
      "```\n",
      "1 def raycast_contact_point(held_obj, target_obj, direction):\n",
      "2 world_copy = current_bullet_world.copy()\n",
      "3 direction_step = 0.001 * normalize_vector(direction)\n",
      "4 max_steps = 60\n",
      "5 move_object(world_copy, held_obj, direction_step * (max_steps / -2))\n",
      "6 for _ in range(max_steps):\n",
      "7 contact_manifold = get_contact_between(world_copy, held_obj, target_obj)\n",
      "8 if contact_manifold:\n",
      "9 return get_contact_point(contact_manifold, target_obj)\n",
      "10 else:\n",
      "11 move_object(world_copy, held_obj, direction_step)\n",
      "12 raise OffsetTooBigError\n",
      "```\n",
      "LISTING 9: Raycast pseudo-code procedure\n",
      "\n",
      "#### <span id=\"page-43-0\"></span>**3.3.2 Touching the Target**\n",
      "\n",
      "During this procedure (see the process diagram in figure [3.7\\)](#page-43-2) the gripper follows a trajectory of poses towards an object on one specific axis normal (along X or Y) until force is detected. While the gripper, with or without holding an object, is pressing against the target, the target's position is updated along the axis on which it is approached. The procedure terminates, when the target has been touched twice on the same axis, to mitigate noisy forces or occasional spikes in the first contact. To adjust and object's pose in X and Y, this procedure has to be called for each axis independently.\n",
      "\n",
      "Since an object is approached on one axis only, the problem of calculating the difference between actual and estimated position can be broken down to the difference in one value, meaning, depending on the direction of approach only the value of one coordinate is required from the actual and estimated contact point. When an object is approached from the front, the directional vector is [1,0,0], right [0,1,0] and left [0,-1,0]. Directions along Z are not important, because every assembly in this thesis is finished by moving the object downwards onto the target anyway, and touching from behind [-1,0,0] is not possible, due to the arm's limited reach. This is really beneficial for determining the offset.\n",
      "\n",
      "**Actual contact point** Each object has a bounding box limited by their furthest expansion from its origin. To determine the actual point of contact, the held object's cartesian position is added to its bounding box expansion towards the direction of approach (see bottom right image in figure [3.5\\)](#page-40-0). The resulting point lies on the bounding box' surface of the held object, which is most likely to be in collision. Therefore the offsets of movement per object-type and axis need to be tweaked correctly, otherwise any bigger difference between real and believed position could result in missing the target. From this point on the bounding box' surface, only the direction-dependent value is used. The same procedure goes for using a bare gripper, where the fingertip's dimension is used instead of an object's bounding box.\n",
      "\n",
      "**Estimated contact point** With pseudo ray casting in Bullet (section [3.3.1\\)](#page-41-0) the held object (or fingertip) is retracted in the opposite direction, then incrementally moved towards the target until in collision (see bottom left image in figure [3.5](#page-40-0) and whole figure [3.6\\)](#page-42-1). This yields a contact point on the target's surface, which is reduced to the coordinate specified by the axis of approach.\n",
      "\n",
      "When an object is touched from front, the resulting X coordinates from actual and estimated contact point are subtracted to get the offset along the approaching axis. This difference is applied to the target's position, to transform it towards the actual contact point, resulting in an adjustment in its X-coordinate. Doing the same from left or right will adjust the targets position on the Y axis.\n",
      "\n",
      "#### <span id=\"page-44-0\"></span>**Procedure**\n",
      "\n",
      "The whole procedure is split in two functions, one that moves the gripper into contact, and another that calculates and adjusts the object's pose. Both are explained in pseudo code.\n",
      "\n",
      "First the touch plan is described in listing [10.](#page-45-1) It terminates successfully, when the target has been touched twice. As input arguments serve the target object, a trajectory of poses w.r.t. the target object's predefined axis offset, the directional vector, and constraints for the joint controller. The trajectory intentionally over-estimates the goal position beyond the target's surface, to reach it even if the object is further away than expected.\n",
      "\n",
      "The plan starts off by closing the gripper, whether or not an object is held (line 2). To verify that the wrench fluent is alive, fl\\_active is probed (line 5-6). Then the gripper is moved to the trajectory's first position (line 7). When the movement is done, the wrench sensor data is zeroed (line 8). Now the trajectory is followed while checking for contact forces in parallel, such that the movement terminates as soon as the gripper and target touch (lines 9-14), which then raises a condition. Since the intention of this procedure is **not** to reach the goal pose, it raises an error to signalize that the target was missed after the whole trajectory is achieved (line 15).\n",
      "\n",
      "When contact is detected, the current gripper position is used to adjust the target along the given axis (line 17), which will be explained in a separate listing below. Since the target is to be touched twice (line 18), on the first run it recalculates the trajectory (line 21). Similar to the mechanism from the first approach of section [3.2.3,](#page-35-0) the trajectory is changed such that is starts close to the gripper and ends at the same pose, except its poses are slightly denser or wider, which brings variation to the contact forces. Afterwards the new trajectory is followed by recursively calling the touch plan itself (line 23).\n",
      "\n",
      "```\n",
      "1 touched_once = False\n",
      "2 close_gripper()\n",
      "3 def touch(target_obj, trajectory, direction, constraints):\n",
      "4 try:\n",
      "5 if not fl_active(wrench_fluent):\n",
      "6 raise WrenchFluentDead\n",
      "7 move_gripper(trajectory[0], [])\n",
      "8 zero_wrench(wrench_fluent)\n",
      "9 in_parallel:\n",
      "10 for pose in trajectory:\n",
      "11 move_gripper(pose, constraints)\n",
      "12 if aggregate_force(wrench_fluent) > 4:\n",
      "13 raise ForceDetected('Contact detected.')\n",
      "14 break\n",
      "15 raise ObjectMissed('Pushed to the end but no contact.')\n",
      "16 except ForceDetected:\n",
      "17 update_object_position(target_obj, direction)\n",
      "18 if touched_once:\n",
      "19 return True\n",
      "20 else:\n",
      "21 trajectory = recalculate_trajectory(trajectory, direction)\n",
      "22 touched_once = True\n",
      "23 touch(target_obj, trajectory, direction, constraints)\n",
      "24 except ObjectMissed:\n",
      "25 return False\n",
      "```\n",
      "LISTING 10: Procedure to touch an object in pseudo-code\n",
      "\n",
      "#### <span id=\"page-45-0\"></span>**Object pose adjustment during contact**\n",
      "\n",
      "During the touch plan, when the gripper and target are in contact, the target's position is adjusted. The function in listing [11](#page-46-2) elaborates calculating this adjustment. As input arguments the target object and direction of approach as 3D-Vector is used, which is assumed to a normal of any axis.\n",
      "\n",
      "First an axis indicator is set, which contains the directional vector with absolute coordinates (line 2). Depending on whether the gripper holds anything or not, a reference object is set, preferring the held object over the fingertip (lines 3-4). Then the action contact point is calculated, using half the object's bounding box dimensions as 3D vector (line 6) because its expansion starts at the gripper's fingertip position and only reaches half the way towards contact. Also, a little margin is subtracted from each bounding box, because Bullet makes them a little bigger (approx. 2mm) than the meshes actually are. This bounding box vector is rotated into correct orientation (line 7) to then eliminate all its coordinates, except for the axis of interest, leaving a normal vector with the bounding box' expansion in the direction of contact w.r.t. the gripper's fingertip tool-frame (line 8).\n",
      "\n",
      "The gripper tool-frame position is retrieved by calling TF (line 10), which is a ROS service who maintains poses of all robot's parts. Adding the preciously calculated offset between gripper an contact surface to the gripper's current position, yields a cartesian point on the bounding box's surface w.r.t. the world's origin (line 11). In this point, again, each coordinate is set to zero except for the axis of interest (line 12).\n",
      "\n",
      "For the estimated contact pseudo ray casting is used, either with the held object or the fingertip (line 14). The resulting point is, again, reduced to a 3D vector with one non-zero entry, on the desired axis (line 15).\n",
      "\n",
      "The two resulting vectors, actual and estimated, are of the same shape: both with three entries and only one is non-zero. When subtracting the estimated from\n",
      "\n",
      "```\n",
      "1 def update_object_position(target_obj, direction):\n",
      "2 axis_indicator = abs(direction)\n",
      "3 held_obj = bullet.get_object_held()\n",
      "4 ref_obj = held_obj if held_obj else robot.fingertip\n",
      "5\n",
      "6 ref_obj_bb = (ref_obj.bounding_box_dimensions / 2) - bb_oversize\n",
      "7 ref_obj_bb_oriented = ref_obj_bb.rotate(ref_obj.orientation)\n",
      "8 gripper_to_contact = multiply_pairwise(ref_obj_bb_oriented, direction)\n",
      "9\n",
      "10 gripper_pose = tf.lookup_transform('map', 'gripper_tool_frame')\n",
      "11 actual_cp_on_surface = gripper_pose.origin + gripper_to_contact\n",
      "12 actual_cp_on_axis = multiply_pairwise(actual_cp_on_surface, axis_indicator)\n",
      "13\n",
      "14 raycast_cp = raycast_contact_point(ref_obj, target_obj, direction)\n",
      "15 estimated_cp_on_axis = multiply_pairwise(raycast_cp, axis_indicator)\n",
      "16\n",
      "17 move_object(target_obj, actual_cp_on_axis - estimated_cp_on_axis)\n",
      "```\n",
      "LISTING 11: Updating an object's pose during contact, in pseudocode\n",
      "\n",
      "the actual vector, the result can be applied as transformation to the target's position (line 17). This pulls its surface to the actual collision point in Bullet's belief state.\n",
      "\n",
      "#### <span id=\"page-46-0\"></span>**Contact Detection - Generic**\n",
      "\n",
      "To implement this procedure in CRAM, a different, axis-agnostic contact detection metric is designed. It simply sums all absolutes of linear velocities from the wrench sensor into one value. Compared to force-detection mechanism in section [3.2.4](#page-36-1) it doesn't require a specific axis, which makes it easier to handle, since no conversion between 3D directional vector and observed axis is necessary. As the other, axis oriented detection, it is called with the wrench-fluent as input argument.\n",
      "\n",
      "```\n",
      "1 (defun aggregate-force (wrench-msg)\n",
      "2 (with-fields ((fx (x force wrench))\n",
      "3 (fy (y force wrench))\n",
      "4 (fz (z force wrench))) wrench-msg\n",
      "5 (apply #'+ (mapcar #'abs (list fx fy fz)))))\n",
      "```\n",
      "#### <span id=\"page-46-1\"></span>**Implementation**\n",
      "\n",
      "Following is the implementation of object pose adjustment in CRAM. Only the touch plan is shown in listing [12,](#page-47-0) since the object pose calculation involves many ROSspecific mechanisms to achieve, and is quite straight-forward anyway.\n",
      "\n",
      "The main section (lines 23-48) first uses fl-gate to verify that the wrench sensor is alive (line 23). After moving the gripper to the trajectory's first position (lines 24- 39) the wrench sensor is zeroed (line 30). With the pursue macro two threads are executed in parallel: moving the gripper along the given trajectory (lines 35-40), while watching out for contact events (line 31-34). Gripper movement is executed with the pushing action designator. Detecting contacts uses the \\*wrench-state-fluent\\* as conditional-fluent with the wait-for mechanic, as explained in section [2.8.1.](#page-20-2) Both threads raise a condition, so neither can terminate properly.\n",
      "\n",
      "As explained for the pseudo-code in section [3.3.2](#page-43-0) it is intended to terminates the touch-plan from within failure handling. Both conditions are caught independently (lines 14 and 21): while the contact force event force-detected invokes the intended sequence, missing an object on the other hand must be handled by a higher-level plan in a different way, e.g. through repositioning the base, or trying a new trajectory. When in contact, the update-object-pose procedure from section [3.3.2](#page-45-0) is used (line 16) to adjust the target object's position w.r.t. the gripper touching it.\n",
      "\n",
      "```\n",
      "1 (defun touch (?target-obj ?trajectory ?direction ?constraints)\n",
      "2 (declare (type keyword ?target-obj)\n",
      "3 (type list ?trajectory)\n",
      "4 (type 3d-vector ?direction)\n",
      "5 (type list ?constraints))\n",
      "6 \"`?target-obj' name of the object to touch\n",
      "7 `?trajectory' list of poses to follow\n",
      "8 `?direction' the direction of approach\n",
      "9 `?constraints' for restricted joints during movement\"\n",
      "10 (close-gripper)\n",
      "11 (let ((?start-pose (pop ?trajectory))\n",
      "12 (touched-once nil))\n",
      "13 (cpl:with-failure-handling\n",
      "14 ((force-detected (e)\n",
      "15 (roslisp:ros-warn (assembly touch) \"~a\" e)\n",
      "16 (update-object-position ?target-obj ?direction)\n",
      "17 (unless touched-once\n",
      "18 (setf ?trajectory (recalculate-touch-trajectory ?direction))\n",
      "19 (setf touched-once T)\n",
      "20 (cpl:retry)))\n",
      "21 (object-missed (e)\n",
      "22 (roslisp:ros-error (assembly touch) \"Object missed, propagating up.\")))\n",
      "23 (fl-gate *wrench-state-fluent*)\n",
      "24 (perform\n",
      "25 (an action\n",
      "26 (type reaching)\n",
      "27 (pose ?start-pose)\n",
      "28 (constraints ?constraints)\n",
      "29 (collision-move :avoid-all)))\n",
      "30 (zero-wrench-sensor)\n",
      "31 (pursue\n",
      "32 (and (wait-for\n",
      "33 (fl< 1.0 (fl-funcall #'force-aggregated *wrench-state-fluent*)))\n",
      "34 (fail 'force-detected :description \"Object touched.\"))\n",
      "35 (and (perform\n",
      "36 (an action\n",
      "37 (type pushing)\n",
      "38 (left-poses ?trajectory)\n",
      "39 (constraints ?constraints)\n",
      "40 (collision-mode :allow-all)))\n",
      "41 (fail 'object-missed\n",
      "42 :description \"Pushed to the end but no collision.\"))))\n",
      "43 (perform\n",
      "44 (an action\n",
      "45 (type retracting)\n",
      "46 (pose ?start-pose)\n",
      "47 (constraints nil)\n",
      "48 (collision-mode :avoid-all)))))\n",
      "```\n",
      "![](_page_47_Figure_3.jpeg)\n",
      "\n",
      "## <span id=\"page-48-0\"></span>**Chapter 4**\n",
      "\n",
      "## **Experimental Evaluation**\n",
      "\n",
      "<span id=\"page-48-2\"></span>![](_page_48_Picture_2.jpeg)\n",
      "\n",
      "FIGURE 4.1: Left: Setup for the assembly of a Battat toy airplane with parts on holders, screwed on a plate. Back from left to right: Bolts, upper-body, top-wing, underbody, horizontal holder with rear-wing, bottom-wing, chassis. Front from left to right: screwdrivers, vertical holder, windshield, front wheels and nuts. Right: Assembled toy airplane\n",
      "\n",
      "This chapter puts the previously explained implementations to the test. First, a general overview of the real-world task is given. Later on, each approach is tested and evaluated in its own way.\n",
      "\n",
      "Parts of a Battat toy airplane[1](#page-48-1) are assembled in the real world to evaluate the two approaches in chapter [3.](#page-28-0) 3D-printed models of holders are screwed onto an MDF board, which hold the plane's parts in place and makes them accessible for Boxy, the robot. A simulation of this scenario in Bullet already existed, while tests and adaptations for the real world have been developed throughout this thesis. About the difficulties in transitioning from simulation to real world, see section [4.3.](#page-57-0)\n",
      "\n",
      "The setup for the plane's assembly is shown in figure [4.1,](#page-48-2) while the first five steps are depicted in figure [4.2.](#page-49-2) It starts with putting the chassis onto the horizontal holder, then the bottom-wing is put onto the chassis while the chassis's vertical pin goes through a hole in the bottom-wing. Afterwards the under-body is placed onto the bottom-wing and rear-wing, again guided by vertical pins. The upper-body closes the plane's corpus and a bolt through both body parts and the rear-wing screw them in place. Then the top-wing is placed onto the upper-body and is also screwed tight through both body parts, bottom-wing and the chassis. Now the windshield is attached and screwed to the upper-body's middle region. Lastly the whole plane needs to be placed onto the vertical holder to assemble the front wheels to the chassis and finish the airplane.\n",
      "\n",
      "<span id=\"page-48-1\"></span><sup>1</sup><https://battattoys.com/product/battat-take-apart-airplane/>\n",
      "\n",
      "<span id=\"page-49-2\"></span>![](_page_49_Picture_1.jpeg)\n",
      "\n",
      "FIGURE 4.2: Wrench sensor orientation during assembly. (Top-left) (1) chassis on holder, (middle-left) (2) bottom-wing on chassis, (topright) (3) underbody on bottom-wing, (middle-right) (4) upperbody on underbody, (bottom) (5) bolt through rear bodies.\n",
      "\n",
      "## <span id=\"page-49-0\"></span>**4.1 End Effector Pose Adjustment**\n",
      "\n",
      "The procedure from section [3.2](#page-33-0) is tested withing simulation and real-world. Gazebo is used for the virtual environment, whereby only the CRAM plan is needed for execution. In the real world the whole software stack (see section [2\\)](#page-14-0) is needed. All tasks for the real-world robot are similar to general Peg-in-Hole tasks, except the actuated and target shapes are switched. Almost every step of assembling the plane puts a hollow object onto a socket, like putting the wheel of a car on an axis.\n",
      "\n",
      "First the procedure of end effector adjustment is evaluated in Gazebo (see section [3.2.1\\)](#page-33-1). Then the heuristic's quality is determined by testing it with real-world contact forces per assembly scenario.\n",
      "\n",
      "### <span id=\"page-49-1\"></span>**4.1.1 Simulation in Gazebo**\n",
      "\n",
      "To verify the procedure of end effector adjustment in section [3.2](#page-33-0) the approach is tested in the Gazebo simulator. In this simulation a hollow object is held in the gripper and put onto a target box, as shown in the top images in figure [4.3.](#page-50-0) The goal of this evaluation is to see, if the procedure itself is successful before application in the real world, therefore the *H*<sup>0</sup> hypothesis is, that the procedure generally fails to put the actuated object onto the target. This scenario is similar to most of the assembly steps, but especially the first one: putting the chassis onto the horizontal\n",
      "\n",
      "<span id=\"page-50-0\"></span>![](_page_50_Figure_1.jpeg)\n",
      "\n",
      "FIGURE 4.3: (Top) Gripper holding a hollow object and target box used in a reversed peg-in-hole task in the Gazebo simulator. (Topleft) actuated object held above the target, (top-middle) successful assembly, (top-right) actuated object in collision, resulting in a wrench to reason upon. (Bottom) Target box's offsets in X and Y over 100 tests. The point's size relate to the amount of pushing the actuated downwards, before successfuly placing it onto the target.\n",
      "\n",
      "holder. A cylindrical shape resembles the arm's wrist, while the hollow box below simulates the gripper holding an object.\n",
      "\n",
      "To retrieve contact forces a wrench sensor is simulated between gripper and wrist. Since it is a joint, there is no collision shape to show. This setup is already described in chapter [3.2.1,](#page-33-1) where figure [3.3](#page-33-2) depicts wrenches during contact. The target object is standing on the ground. A Gazebo plugin fetches the simulated wrench data, filters it with a moving average over the past second, and publishes the filtered data on a ROS topic. Compared to the real-world wrench sensor, this data here doesn't need to be zeroed, since it is always stable and doesn't accumulate drift. Also the fl-gate isn't necessary, since the simulated sensor never dies. The gazebo\\_ros interface allows to move gripper and target through service calls. Except for these conveniences, the procedure from section [3.2](#page-33-0) is applied as is.\n",
      "\n",
      "For each test the target is placed at a random location on the X-Y plane. To create random positions, a vector of arbitrary magnitude between 0.3 and 1.2 meters is aligned with the X-Y plane, then rotated at an angle between 0 to 2PI around the world's Z axis. The resulting vector gives the target's offset from the origin. Agnostic of this position, the actuated object is held at X=0, Y=0 and raised above ground (top left image in figure [4.3\\)](#page-50-0).\n",
      "\n",
      "After the above setup the gripper moves down, which brings the actuated object into contact with the target, if not aligned correctly (top-right image in figure [4.3\\)](#page-50-0). After one second, when the wrench data reaches a stable curve, contact forces are\n",
      "\n",
      "|         | Time total | Time avg               | #Push min | #Push max                  | #Push avg |\n",
      "|---------|------------|------------------------|-----------|----------------------------|-----------|\n",
      "| Results | 5236 sec   | 53 sec                 | 8         | 68                         | 33.714    |\n",
      "|         | #Push med  | Class. after #Push avg |           | Class. after #Push max med |           |\n",
      "|         | 33         | 4.278                  |           | 5                          |           |\n",
      "\n",
      "<span id=\"page-51-1\"></span>TABLE 4.1: Results for end effector adjustment tests in Gazebo. Avg.: average, med.: median, #Push: amount of down-pushes, Class.: classification.\n",
      "\n",
      "<span id=\"page-51-2\"></span>![](_page_51_Picture_3.jpeg)\n",
      "\n",
      "FIGURE 4.4: Starting with a high offset in positive X the gripper gradually adjusts to the correct pose.\n",
      "\n",
      "evaluated to classify the current offset with the heuristic described in section [3.2.2.](#page-34-0) Then the gripper retracts. At least four times the gripper pushes down and retracts again, until one offset can be clearly determined over all results. If this is the case, the gripper adjusts its position along X and Y at 0.1 meters along one axis. Eventually the actuated object lands on the target without contact (top-middle image in [4.3\\)](#page-50-0).\n",
      "\n",
      "This push, retract, evaluate, adjust cycle repeats until no contact forces are detected. In the end, when the rim is around the box, it succeeds, if not it fails. The difference is determined by measuring the distance between hollow box and bottom cube.\n",
      "\n",
      "**Results:** 98 out of 100 random scenarios were successful. The only two failures were in such cases where the target is placed at [-0.07, 1.19] and [1.16, -0.02], where the actuated object and target completely miss each other when pushing down. The bottom plot in figure [4.3](#page-50-0) shows the target's offset in X and Y, in relation to the amount of contacts needed until successful assembly. In table [4.1](#page-51-1) the resulting data is listed. All 98 successful runs together took 5236 seconds (1h 17m 16s), the two failed runs immediately terminated within a second. Considering a success rate of 98% the procedure for end effector adjustment achieves its goal and *H*<sup>0</sup> is disproven.\n",
      "\n",
      "#### <span id=\"page-51-0\"></span>**4.1.2 Real-World Application**\n",
      "\n",
      "Contrary to most Peg-in-Hole tasks, assembling the Battat airplane from the YCB dataset includes manipulation of multiple differently shaped objects, therefore each step of the assembly must be addressed separately. Grasping different shapes is done with a designated orientation of the gripper per airplane part. Also, each two parts require a specific orientation to be put together. Implicitly, this changes the wrench-sensor's orientation as well. Therefore the heuristic must be adjusted, depending on the gripper's orientation during assembly, because the expected torques\n",
      "\n",
      "<span id=\"page-52-0\"></span>![](_page_52_Figure_1.jpeg)\n",
      "\n",
      "FIGURE 4.5: Left: Confusion matrix over all observed data sequences. Right: Same, but without bottom-wing.\n",
      "\n",
      "during contact events change w.r.t. the wrench-sensor's orientation and contact points between the two parts to assemble. Nevertheless, it only classifies an offset in four directions by comparing the torque's strength on two axes.\n",
      "\n",
      "Five steps of the assembly and their contact forces are observed, namely they are (1) chassis on holder, (2) bottom-wing on chassis, (3) underbody on bottom-wing, (4) upper-body on underbody and (5) bolt through rear bodies. To test the heuristic for each of these scenarios, first the actuated object, held by the gripper, is moved into successful assembly with the target. From this goal position a horizontal offset of 5 to 20 millimeters is applied, which resembles the current inaccuracy of the robot, determined later in section [4.2.2.](#page-54-2) Each of the four directions are tested separately. The gripper is moved downwards until the actuated and target object are in contact.\n",
      "\n",
      "For each scenario and offset, wrench data of at least five contact events are recorded. Afterwards, the recorded data is classified by the heuristic in section [3.2.2.](#page-34-0) The results are illustrated as confusion matrices between the actual offset in real-world and the heuristic's classification during contact. See appendix [A](#page-68-0) for an evaluation of each scenario and offset. The results in this section show the general performance of the heuristic.\n",
      "\n",
      "\"The heuristic is at least 80% accurate when classifying the gripper's offset based on contact forces\" is the hypothesis to be answered here, so the *H*<sup>0</sup> hypothesis to be disproven is, that the heuristic is not accurate enough to confidently classify the gripper's offset.\n",
      "\n",
      "**Results:** As shown in figure [4.5,](#page-52-0) the heuristic's accuracy over all recorded forcedata is only 0.4861%. When leaving out the scenario of putting the bottom-wing onto the chassis, accuracy rises to 0.5328%. A summary of each scenario's accuracy is shown in table [4.2.](#page-53-1) This heuristic is clearly too simple to reliably classify the gripper's offset based on real-world contact forces, but at least it is better than guessing (0.25%). Also the amount of data is barely enough to calculate any significant improvement (e.g. from student's T-test). The *H*<sup>0</sup> hypothesis can't be disproven.\n",
      "\n",
      "While training a proper classifier on accuracy was not the intend of this thesis it is still interesting to see the difficulties of evaluating contact forces between\n",
      "\n",
      "<span id=\"page-53-1\"></span>\n",
      "\n",
      "| Scenario | (1)    | (2)    | (3)    | (4)    | (5)    | Total  |\n",
      "|----------|--------|--------|--------|--------|--------|--------|\n",
      "| Accuracy | 0.2609 | 0.2273 | 0.7500 | 0.3871 | 0.6591 | 0.4861 |\n",
      "\n",
      "TABLE 4.2: Accuracy rating of the heuristic per scenario.\n",
      "\n",
      "every-day shaped objects. As already mentioned by (Bouchard et al., [2015\\)](#page-82-2) friction in rigid-rigid contact is a big problem, because the axis of applied force changes due to the object's stress, which influences measured contact forces. Another factor is the arm's joint-impedance controller, simulating a spring-like behavior. When the gripper pushes against the target, the arm's joints are not rigid enough to hold the gripper in position, but instead give like springs, pressing the gripper in a direction of relaxing the joint's stress. Even with higher impedance per joint this issue occurs.\n",
      "\n",
      "**Chassis** The influence of friction and the arm's flexibility is most prominent in the first assembly step (top-left image in figure [4.2\\)](#page-49-2). Pushing the chassis down against the holder invokes tension in the arm, which is relaxed by pushing the chassis away form the robot, towards positive X. This causes strong torques around *T<sup>Y</sup>* which confuses the heuristic: offsets in +/-Y are classified as +X.\n",
      "\n",
      "**Bottom-wing** Assembling the bottom wing is a good example for why the gripper's position is important to produce informative wrench data in contact (middleleft image in figure [4.2\\)](#page-49-2). With a gripper oriented horizontally, the contact forces are very similar over all offsets. Unfortunately, grasping the bottom-wing at a different angle was not possible, because the wing's dimensions were bigger than the gripper's maximum opening.\n",
      "\n",
      "**Under- and Upperbody** A huge difficulty surely is the diversity and asymmetry of the shapes. For assembling the upper-body onto the underbody (left images in figure [4.2\\)](#page-49-2) an offset in right and left (+X, -X) can be easily determined, while this is much more difficult along the Y axis because there are no distinct contact points between the two object's shapes.\n",
      "\n",
      "**Bolt** Putting a bolt into the rear-hole was the most difficult task, mainly because of the upper-body's surface around the hole (bottom image in figure [4.2\\)](#page-49-2). This scenario has been tested with several different offsets, yielding a variety of torque forces, especially along -X direction. At small offsets the bolt landed near the hole, pressing against the inner rim. At bigger offsets and oscillation overshoots the bolt glides off the upper-body's side instead, yielding completely opposing torque forces. In +Y direction the bolt glides down the rear wing, while in -Y the bolt presses against the upper-body's middle region.\n",
      "\n",
      "#### <span id=\"page-53-0\"></span>**4.1.3 Summary - End Effector Adjustment**\n",
      "\n",
      "After investigating real-world contact forces and the heuristic's accuracy it is foreseeable how the end effector adjustment procedure from section [3.2](#page-33-0) would behave in the real world. Even randomizing pushing force (as described in section [3.2.3\\)](#page-35-0) to cause variety in the torques, and taking the most frequenting classification, doesn't help the classifier's accuracy when putting the chassis onto the horizontal holder. Nevertheless, the Gazebo simulator shows, that a better classifier would make an application in the real world promising. Force feedback has been successfully implemented in the cognitive plans of CRAM, contributing to its reactionary mechanism during intricate, force-sensitive assembly tasks.\n",
      "\n",
      "### <span id=\"page-54-0\"></span>**4.2 Object Pose Adjustment**\n",
      "\n",
      "To evaluate the procedure of object pose adjustment from section [3.3](#page-39-0) the gripper's accuracy is measured before and after touching objects in the real world. The improvement depends on estimating contact points through pseudo ray casting in simulation (see section [3.3.1\\)](#page-41-0), and detecting actual contact points in real-world (section [3.3.2\\)](#page-43-0). Only if both are accurate, precision can be improved. Both components are taken into account separately.\n",
      "\n",
      "#### <span id=\"page-54-1\"></span>**4.2.1 Simulation in Bullet-Physics - Pseudo Ray Casting**\n",
      "\n",
      "The estimated contact point between two rigid bodies is obtained in Bullet by pseudo ray casting, explained in section [3.3.1.](#page-41-0) Their accuracy is mostly dependent on Bullet's contact point calculation, which again is heavily influenced by each object's grade of detail in its corresponding mesh representation. As already shown in figure [3.6](#page-42-1) during explanation of the procedure, the estimated contact point do not lay exactly on an object's surface. This is because the collision mesh of each object is slightly bigger than its visual shape.\n",
      "\n",
      "However, the precision of Bullet's contact points can be investigated, based on the meshes used. Provided that all included meshes are decomposed into a composition of convex hull shapes (see pseudo ray casting in section [3.3.1\\)](#page-41-0), two objects are in contact when their actual shapes overlap, not their bounding boxes, which is the usual case if the mesh is not decomposed.\n",
      "\n",
      "To obtain how far Bullet's calculated contact point is away from the actual surface, the calculated point is retrieved and visualized first. Then a linear translation is applied to the point until it is visualized directly on the object's surface. This error gives the distance between calculated and estimated contact point.\n",
      "\n",
      "When pseudo ray casting the chassis against the holder on eight different sides, Bullet calculates the contact point 1.2 to 1.7mm off the holder's surface. Vice versa the contact point on the chassis' axis is approx. 1.5mm before its axis' surface. The bottom-wing's contact points are similarly distant (1.0 - 1.4mm) to its actual surface.\n",
      "\n",
      "Changing the step distance for pseudo ray casting has no impact on the retrieved results. Overall the contact point offset lies between 1.0 and 1.7mm, depending on the mesh and its surface. This offset is taken into consideration when adjusting the object's pose on touch, but since it varies, some error persists.\n",
      "\n",
      "#### <span id=\"page-54-2\"></span>**4.2.2 Real-World Application**\n",
      "\n",
      "The touch procedure from section [3.3.2](#page-43-0) is evaluated through real-world application. The tests include (1) touching the MDF board without an object held, from top, front and left, before picking up the chassis, then (2) touching the horizontal holder with a chassis held, from front and left, before placing the chassis onto it, then (3) touching the bottom-wing without any object held from left and front, and (4) the chassis on the holder from left and the front. Afterwards (5) the bottom-wing is picked up and placed onto the chassis.\n",
      "\n",
      "To investigate the increase in accuracy, first the real-world error must be determined. Therefore all object's positions in Bullet and real-world are synchronized\n",
      "\n",
      "<span id=\"page-55-0\"></span>![](_page_55_Picture_1.jpeg)\n",
      "\n",
      "FIGURE 4.6: Real-World inaccuracy without object pose adjustment. (Top left) Holder's position in the real world as reference, (top middle) calibration in Bullet (top right) (1) grasping chassis, (bottom left) (2) putting chassis onto pedestal (bottom middle) (3a) grasp bottomwing (bottom right) (3b) putting bottom-wing onto chassis.\n",
      "\n",
      "first. This is done by moving the gripper-tip in real-world to a distinct position of an object, to then cast the object in Bullet until the simulated gripper and object are aligned in the same way.\n",
      "\n",
      "To test the real-world error, the robot is moved away from the assembly board by teleoperation. Then the torso, as well as the arm joints are arranged (homed) into a different joint-configuration. Now the robot moves back to the assembly board. When the gripper is moved to one of the distinct poses, used to synchronize an object, the position differs. The real-world inaccuracy without object pose adjustment is depicted in figure [4.6](#page-55-0) and table [4.3.](#page-56-0)\n",
      "\n",
      "Three tests show how accuracy is improved by touching objects: (1) touch the board to pick the chassis, (2) touch the holder to put the chassis on it and (3) touch the bottom wing and chassis, before picking up the wing (3a) and putting it onto the chassis (3b). Each object's position has been adjusted before a tests begin, and the robot is retracted from the assembly board and its arms configured in a homing position, before each test is executed. To verify an object's position it is touched from two perpendicular sides. After an object's position is verified, the following action is analyzed regarding its accuracy, which means in test (1) for example, how far off the chassis is grasped from the desired position. The hypothesis to prove is, that object pose adjustment improves accuracy in the investigated assembly tasks, therefore the *H*<sup>0</sup> hypothesis to disprove is, that there is no improvement.\n",
      "\n",
      "**Results:** As depicted in figure [4.7](#page-56-1) grasping the chassis (1) was still done with a slight offset. With an incorrectly held chassis, touching the horizontal holder (2) inherits the error, which is malicious along the Y-axis but beneficial in X, because the\n",
      "\n",
      "<span id=\"page-56-1\"></span>![](_page_56_Picture_1.jpeg)\n",
      "\n",
      "FIGURE 4.7: Manipulation accuracy after Object Pose Adjustment. (Top-left) Grasping the chassis after touching the board, (top-middle) holding the chassis above the holder's pedestal, (top-right) dropping the chassis onto the pedestal. (Bottom-left) Offset when putting the bottom-wing onto the chassis from top perspective (bottom-middle) same from the side, (bottom-right) successful bottom-wing on chassis assembly.\n",
      "\n",
      "TABLE 4.3: Real-world X, Y offsets in millimeters with and without object pose adjustment.\n",
      "\n",
      "<span id=\"page-56-0\"></span>\n",
      "\n",
      "| Scenario             | (1)         | (2)        | (3a)       | (3b)         |\n",
      "|----------------------|-------------|------------|------------|--------------|\n",
      "| W/o pose adjustment  | 26-32, 8-12 | 15-20, 6-8 | 28-33, 1-4 | 22-27, 12-18 |\n",
      "| With pose adjustment | 0-3, 2-5    | 0-3, 1-4   | 2-4, 1-3   | 0-2, 2-7     |\n",
      "\n",
      "error in the chassis' translation to the gripper can be omitted, as shown in the top middle figure in [4.7,](#page-56-1) where the chassis is only slightly off to the right.\n",
      "\n",
      "After touching the bottom-wing and chassis (3) the former was picked up to put on the chassis. Still with a slight offset (bottom left and middle image in figure [4.7\\)](#page-56-1), adjusting the parameters for contact point calculation for the bottom-wing corrected its offset, to finally assemble the bottom wing onto the chassis (bottom right image).\n",
      "\n",
      "By keeping the base still after touching an object, differences in localization can be omitted and since the torso's joint only affects the gripper's vertical position (along robot's Z axis) any error in the prismatic torso-joint can be ignored. Unfortunately, inaccuracies in the arm's joint-states persist, which adds to the error in determining the estimated contact point in Bullet (see section [4.2.1\\)](#page-54-1). When pressing the gripper against an object, the arm's joints give, because the joint-impedance controller is designed to simulate a spring-like behavior. To what extent this error influences object pose adjustment, see section [4.3.1.](#page-57-1)\n",
      "\n",
      "Over several test in the real world the offset for contact-points, the gripper's\n",
      "\n",
      "<span id=\"page-57-2\"></span>![](_page_57_Figure_1.jpeg)\n",
      "\n",
      "FIGURE 4.8: Lifted assembly board to mitigate the object's error in its vertical position, based on the gripper's contact pose.\n",
      "\n",
      "fingertip size and mitigation of the arm's flexibility were adjusted to get a successful execution of all assembly steps separately. After all parameter-adjustment and several object's touched a small offset was still present, depending on the assembly step. Compared to the offsets without object pose adjustment, the improvement is a strong enough indicator for the procedure to be applicable in real world.\n",
      "\n",
      "## <span id=\"page-57-0\"></span>**4.3 Working with Robots in the real world**\n",
      "\n",
      "The transition of robot's activities from simulation to real-world carries several difficulties, which are elaborated in the following section.\n",
      "\n",
      "Boxy is a home-brew robot, designed and assembled in the Institute for Artificial Intelligence. Compared to other robots, all technical support is provided by the staff. In general the robot works fine, but some of the software and hardware components have potential for improvement, which is elaborated in this section. Some improvements were partially contributed to in the development of this thesis, by extending the robot's drivers and equipping Boxy's system with new frameworks like Giskard.\n",
      "\n",
      "#### <span id=\"page-57-1\"></span>**4.3.1 KUKA LWR-4+ Arms**\n",
      "\n",
      "In freshly manufactured LWR-4+ arms the differences between actual and simulated joint state, retrieved from each optical encoder, are only around 1-2% (Hirzinger et al., [2002\\)](#page-82-7) but this error can accumulate over all 7 of the arm's joints down to the gripper. Also the gripper's weight and inertia must be calibrated accurately to keep its pose consistent. To what extent this impacts the assembly tasks in general is hard to determine, but an observation was made (see figure [4.8\\)](#page-57-2) where the real-world gripper hangs lower than the simulated joint-states imply. Therefore the whole assembly board is lifted by 2cm in Bullet, to mitigate the gripper's vertical offset. Zeroing the arm's joints was done manually, which may have contributed to their inaccuracy. Measuring the distance between the arm's base in simulation and real-world yielded no significant difference, therefore it is assumed that the error originates from the arm's calibration and hardware's wear.\n",
      "\n",
      "Adjusting the joint's impedance is a matter of calibration, in which different setups have their pro's and con's. Higher impedance make the arm stiffer, which increases contact forces, while less impedance enables some joints to damp impacting stress through contact forces at the right rate. Stiffness can only be achieved to a certain degree, where the joints still damp incoming forces a bit, even with very high impedance (double the below mentioned numbers). Lower impedance on joints close to the gripper makes it possible to press with the gripper against an obstacle, while the force is distributed along the joints set to lower impedance, allowing to omit some of the force from the gripper in contact. This kind of spring-like elasticity and damping in arm joints does not influence their simulated joint states, since all joint positions are determined through optical encoders within them. Low impedance overall on the other hand, can still decrease accuracy, and increases the above mentioned vertical offset. The impedance used over all joints is [500, 500, 500, 400, 300, 200] N/m, which works stable enough for all use-cases, without damaging the plastic objects during collision.\n",
      "\n",
      "The arm's joint controller, below DLR's Links-And-Nodes interface, died unexpectedly a couple of times, leaving all joints at zero impedance.\n",
      "\n",
      "Because of how the arm is attached to the torso, its configuration space is very restricting when planning joint trajectories in Giskard. Even though each joint has a pretty wide limit range (+/-170Â°, +/-120Â°, +/-170Â°, +/-120Â°, +/-170Â°, +80Â°/-45Â°, +60Â°/-30Â°)[2](#page-58-2) Giskard often ran into difficulties finding a joint trajectory. Compared to Willow Garage's PR2 this issue was rare, because its arms partially use infinite rotational joints. The work of (Chaves-Arbaiza, GarcÃ­a-Vaglio, and Ruiz-Ugalde, [2018\\)](#page-82-13) could have helped finding a suitable configuration of mounting the arm to Boxy's torso. It simulates several configurations, evaluating each by its ability to grasp objects. Also (GarcÃ­a-Vaglio, [2020\\)](#page-82-14) can be considered for this, which rapidly determines robot capabilities based on GPU calculation.\n",
      "\n",
      "#### <span id=\"page-58-0\"></span>**4.3.2 Localization**\n",
      "\n",
      "Usually Boxy has two Hokuyo LIDAR in its base. Unfortunately, the front-left sensor doesn't work, such that Boxy only receives its laser data at 270 degree to its back-right side, which was thought to be enough for localization at millimeters of precision. Later, however, the offset was found to be around 2-3cm between realworld and simulation. This makes localization way harder, which hindered as well as inspired the thesis' contribution. Using only the hind laser sensor Boxy's localization was always a bit off, which can be seen for example in figure [3.5,](#page-40-0) where the MDF plate is moved off the table in simulation, while its edge should be aligned with the table instead.\n",
      "\n",
      "Over several months of testing in the real world, Boxy has been moved by hand in between session. The LIDAR must have taken a hit, whereby it was rotated ever so slightly, causing the base's localization to be off by only a few millimeters. Accumulating this error up to the gripper it had a huge impact of several centimeters on the gripper's pose, not to mention a rotational offset. Even now the functioning LIDAR's orientation is prone to be off, because its holder is susceptible to be bent.\n",
      "\n",
      "#### <span id=\"page-58-1\"></span>**4.3.3 Giskard**\n",
      "\n",
      "Controlling robots is hard. Giskard has been thoroughly explained in chapter [2.4.](#page-16-1) Every time that Giskard fails to generate a joint trajectory, it is due to the arm's configuration space, as explained in section [4.3.1.](#page-57-1) Therefore the arm had to be brought into a promising configuration first, before giving Giskard the task to plan a trajectory, which increases execution time of the assembly steps a lot. In figure [4.9](#page-59-1) these joint configurations are depicted.\n",
      "\n",
      "Boxy's torso controller was included into Giskard's trajectory planner during development on this thesis. After the base's joint controller was included as well, Giskard was able to plan trajectories for the gripper's goal pose, even if they were\n",
      "\n",
      "<span id=\"page-58-2\"></span><sup>2</sup>[https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-12464/21732\\\\_read-49777/](https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-12464/21732_read-49777/)\n",
      "\n",
      "<span id=\"page-59-1\"></span>![](_page_59_Picture_1.jpeg)\n",
      "\n",
      "FIGURE 4.9: Arm standard configurations to start planning motion from. (Left) before any cartesian goal for the gripper, (middle) for top grasping and touching objects (right) picking and placing the *bottomwing* and *top-wing*.\n",
      "\n",
      "out of Boxy's reach, utilizing not only the arm and torso, but also the base. To do this, Giskard is giving each involved controller joint velocities over time. In simulation these trajectories are executed without a problem, but for real-world application each controller has to be synchronized, which is currently not the case. Especially the recently included torso-controller executed its joint velocities way before the arm's controller, while the arm was taking approximately 1.5 times longer than Giskard predicted. This is a huge issue when planning a trajectory with collision avoidance. Therefore the arm is brought into a safe configuration every time a potential collision is expected or when the gripper's target orientation strongly differs from its current one.\n",
      "\n",
      "A controller's task is finished, when their affected joint velocities are below a certain threshold, at least this is how Giskard determines if a controller has terminated his process. Giving the arm-controller a joint-trajectory does not result in a completely continuous movement, but usually pauses somewhere in the middle of the trajectory before finishing the latter half. Since CRAM can only know if a movement is done by listening to Giskard, which signalizes termination before the arm is actually done moving, CRAM is currently not capable to move the gripper to several different poses when working with Boxy. Therefore the procedures in section [3](#page-28-0) were paused after Giskard's response until the movement was actually done.\n",
      "\n",
      "#### <span id=\"page-59-0\"></span>**4.3.4 KMS40 Force-Torque Sensor**\n",
      "\n",
      "Communication from and to the KMS40 sensor is done via Telnet, wrapped within a C++ ROS driver that propagates the force data into the ROS network. Through this wrapper it is possible to receive data and send commands to the sensor, e.g. putting the current force offsets to zero. Receiving data and publishing it into ROS' network is stable, but sending commands to the sensor is not.\n",
      "\n",
      "The sensor constantly sends its data as ASCII strings to the C++ driver. When the driver wants to send a command to the sensor it expects a specific response, verifying that the command was accepted by the sensor, but when the sensor gets hotter its performance decreases, which manifests in stuttering data transfer. If the driver sends commands to the stressed force sensor, it does not respond immediately, instead it takes a while to execute the given command while still sending force data, which is not the response expected by the driver, so it terminates.\n",
      "\n",
      "Since zeroing force data is of elementary importance for reasoning, a solution was found which can work solely on the received force data. By subtracting the currently sensed forces from upcoming data, a wrapper was used that takes care of zeroing the data and providing a corresponding service to do so, instead of communicating directly with the low-level C++ controller or sensor.\n",
      "\n",
      "The sensor was not explicitly tested for cross-talk between its six axes. Manually applied force to the gripper yielded results as expected. Through the above mentioned nodes, the emitted data was filtered to such a degree, that noise was not an issue.\n",
      "\n",
      "### <span id=\"page-60-0\"></span>**4.4 Evaluation Summary**\n",
      "\n",
      "Both approaches from chapter [3](#page-28-0) have been thoroughly tested in an assembly environment, manipulating objects by an autonomous robot. The first approach seems stable in simulation with a high rate of success (see table [4.1\\)](#page-51-1), verifying the procedure of Gripper Pose Adjustment on the real robot, however, the classifier is not accurate enough to apply this procedure in the real world (see table [4.2\\)](#page-53-1), which is visualized by several confusion matrices per assembly scenario in appendix [A.](#page-68-0) On the other hand, adjusting the airplane parts by tactile perception before manipulating them increased the robot's accuracy significantly (see table [4.3\\)](#page-56-0), which made it possible to assemble the chassis onto the holder and bottom-wing onto the chassis. High offsets are not feasibly accountable, which is explained in section [5.2.](#page-62-2)\n",
      "\n",
      "Considering controller issues, hardware inaccuracy and offsets in localization, the gain in precision is immense, mainly because issues in the localization were omitted by keeping the base still between touching and manipulating an object. Changing the LWR-4 arm's joint-states yielded stronger offsets of the gripper between two configurations than expected, which became high when the bottom-wing is picked from a completely different joint configuration than it was touched before. This resulted in high offsets when picking it up.\n",
      "\n",
      "In summary: with a better classifier the first approach could be stable enough to be combined with the second approach, however, this statement can't be verified without working examples. Touching objects can adjust an object's position well enough to pick and place them confidently, while adjusting the gripper's pose during rigid-rigid contact between the actuated and target object could potentially help mitigating remaining inaccuracy.\n",
      "\n",
      "## <span id=\"page-62-0\"></span>**Chapter 5**\n",
      "\n",
      "## **Conclusion**\n",
      "\n",
      "## <span id=\"page-62-1\"></span>**5.1 Summary**\n",
      "\n",
      "Industrial assembly tasks evolve from a static sequence of movement to autonomous performance. The two approaches presented have shown, that autonomous robots are capable of performing assembly tasks, even if the robot is not rigidly attached nearby an assembly line, but can move around freely. Using a cognitive environment is an important foundation to react dynamically, and gaming engine physics are helpful to improve the quality of a belief state. Force feedback is a great benefit in this domain, providing important information at run time to utilize cognitive procedures in object manipulation the right way.\n",
      "\n",
      "When looking at rigid-rigid contact forces, research has progressed far, but handling a variety of shapes reliably is still not quite possible, at least it is not as simply achieved as with the presented heuristic. The concept of adjusting a gripper pose offset based on contact forces, however, seems to be a promising endeavor in the right direction to improve an assembly-robot's capability to achieve its goal in uncertainty.\n",
      "\n",
      "To handle uncertain positions of assembly parts, touching each object helps to reduce this uncertainty, as shown in the second approach. This again was only possible because of a sophisticated belief state like a gamin engine, which is capable of calculating certain features like contact points between simulated objects in three dimensional space. By combining reasoning on the actual and simulated environment in contact events, the inaccuracy of an object's believed position can be decreased by orders of magnitude.\n",
      "\n",
      "## <span id=\"page-62-2\"></span>**5.2 Discussion**\n",
      "\n",
      "Finding a proper classifier is hard for Peg-In-Hole tasks already and doesn't get easier with a diversity of shapes. However, most every-day scenarios usually require precision in the order of millimeters not micrometers, as opposed to industrial endeavors. As mentioned in related work of section [1.3](#page-12-0) the work of (Stelter, Bartels, and Beetz, [2018\\)](#page-84-5) was considered to be integrated as classifier under supervised training. When the execution of assembly tasks transitioned from simulation to real-world, controller problems arose where immediate interruption of continuous movement was programmatically impossible, which made time-series related classification infeasible.\n",
      "\n",
      "The autonomous robot used in this thesis was approximately 2-3cm off from the desired goal, before applying the mechanism presented in this thesis. Reducing the error to a few millimeters was enough to successfully perform an assembly, because the airplane part's holes and pins generously allow errors of this scale. For both approaches a maximum offset of 3-4cm was assumed. Any inaccuracy higher than that is not proficiently solvable, which is mostly because of the object's size, for example touching the holder allows an offset range of about 3.5cm from the estimated contact point. Trying to touch, perform or assemble an object under higher offsets would probably miss the target or yield unreliable data.\n",
      "\n",
      "The subset of airplane parts to assemble is selected with purpose, because each assembly involving these parts is done by moving downwards. This choice was made primarily for the first approach, where contact forces are evaluated to classify the gripper's error from the actual goal position, but also because of the arm's enormous wrist. These assembly scenarios can be executed with the gripper pointing down, since the arm requires around 40cm of free space between the gripper's fingertip and the wrist. A horizontally oriented gripper close to the assembly board has been causing a lot of potential collisions in simulation. An exception to this is the bottom-wing assembly onto the chassis, where the wrist and gripper are still quite low, but off the assembly-board's range.\n",
      "\n",
      "Touching objects allows to adjust their believed position on the X and Y plane. During the beginning of development for this procedure the robot's localization was off around its Z rotation, causing massive offsets between real-world and simulation for the gripper. The issue was caused by a small rotational error in the hardware, where the LIDAR was shifted by only a fraction of a degree. When adjusting objects by touch from different sides, the adjustment was completely off because of this. After pushing the LIDAR back into correct position, the procedure became stable. The gist of it: rotational errors in localization are not accounted for in the second approach.\n",
      "\n",
      "### <span id=\"page-63-0\"></span>**5.3 Recommendations on Future Work**\n",
      "\n",
      "When using autonomous, humanoid robots for assembly tasks the presented approaches can be applied to increase precision. Working with wrench sensors in wrists also requires the right hardware and controller setup. Instead of a springsimulating joint-impedance controller for the arm a force controller might be more suitable, because contact forces can be better adjusted and friction counteracted. In any case, friction and slipping should always be considered when trying to classify contact forces in real world.\n",
      "\n",
      "Fine calibration of every joint is crucial to simulate the robot state appropriately, where small errors can easily influence precision of an end effector. Especially localization can ruin a robot's performance, which relies on correct LIDAR configuration, reliable localization algorithms and an accurate map of the environment. The presented approaches can deal with a lot of imperfection, since they actively aim to mitigate these errors. Rotational error, however, make tactile perception much harder.\n",
      "\n",
      "Cognitive frameworks, gaming engines and wrench sensors are a strong tool-set for autonomous robots performing assembly tasks in uncertain environments. Contemporary work draws their usage in a promising picture for industrial application.\n",
      "\n",
      "## **List of Figures**\n",
      "\n",
      "| 1.1        | Battat Airplane from the YCB dataset<br>2                                                                          |\n",
      "|------------|--------------------------------------------------------------------------------------------------------------------|\n",
      "| 2.1<br>2.2 | Humanoid robot called Boxy, used for manipulation tasks.<br>.<br>6<br>Wrench sensor axes and orientation<br>.<br>7 |\n",
      "| 2.3        | Giskard procedure<br>.<br>8                                                                                        |\n",
      "| 2.4        | RViz and Bullet visualization<br>.<br>9                                                                            |\n",
      "| 2.5        | Giskard motion planner visual prediction<br>.<br>10                                                                |\n",
      "| 3.1        | Battat Airplane and the assembly board<br>.<br>20                                                                  |\n",
      "| 3.2        | Boxy and the gripper<br>.<br>21                                                                                    |\n",
      "| 3.3        | Gazebo torque to offset<br>24                                                                                      |\n",
      "| 3.4        | End effector pose adjustment procedure<br>26                                                                       |\n",
      "| 3.5        | Object pose adjustment<br>31                                                                                       |\n",
      "| 3.6        | Contact points between Meshes in Bullet<br>.<br>33                                                                 |\n",
      "| 3.7        | Object Pose Adjustment process diagram<br>.<br>34                                                                  |\n",
      "| 4.1        | Assembly demo setup<br>.<br>39                                                                                     |\n",
      "| 4.2        | Wrench sensor orientation during assembly<br>40                                                                    |\n",
      "| 4.3        | Gazebo Test End Effector Adjustment<br>.<br>41                                                                     |\n",
      "| 4.4        | End effector pose adjustment<br>.<br>42                                                                            |\n",
      "| 4.5        | Confusion matrix for all scenarios<br>.<br>43                                                                      |\n",
      "| 4.6        | Real-World offsets without object pose adjustment<br>46                                                            |\n",
      "| 4.7        | Real-World offset after object pose adjustment<br>.<br>47                                                          |\n",
      "| 4.8        | LWR3 Accuracy<br>.<br>48                                                                                           |\n",
      "| 4.9        | Arm standard configurations<br>.<br>50                                                                             |\n",
      "| A.1        | Chassis on Holder Orientation<br>.<br>60                                                                           |\n",
      "| A.2        | Chassis on Holder X offset<br>61                                                                                   |\n",
      "| A.3        | Chassis on Holder Y offset<br>61                                                                                   |\n",
      "| A.4        | Bottom-wing on Chassis Orientation<br>62                                                                           |\n",
      "| A.5        | Bottom-wing on Chassis Y offset<br>.<br>63                                                                         |\n",
      "| A.6        | Bottom-wing on Chassis Z offset<br>.<br>63                                                                         |\n",
      "| A.7        | Underbody on Bottom-Wing and Rear-Wing Orientation<br>.<br>64                                                      |\n",
      "| A.8        | Underbody on Bottom-Wing and Rear-Wing X offset<br>65                                                              |\n",
      "| A.9        | Underbody on Bottom-Wing and Rear-Wing Y offset<br>65                                                              |\n",
      "|            | A.10 Upperbody on Underbody Rotation<br>.<br>66                                                                    |\n",
      "|            | A.11 Upperbody on Underbody X offsets 5mm<br>.<br>67                                                               |\n",
      "|            | A.12 Upperbody on Underbody X offsets 10mm<br>.<br>67                                                              |\n",
      "|            | A.13 Upperbody on Underbody Y offsets<br>.<br>68                                                                   |\n",
      "|            | A.14 Upperbody on Underbody Orientation<br>.<br>69                                                                 |\n",
      "|            | A.15 Upperbody on Underbody X offset<br>70                                                                         |\n",
      "|            | A.16 Upperbody on Underbody Y offsets<br>.<br>71                                                                   |\n",
      "\n",
      "## **List of Abbreviations**\n",
      "\n",
      "| CRAM | Cognitive Robot Abstract Machine    |\n",
      "|------|-------------------------------------|\n",
      "| IAI  | Institut of Artificial Intelligence |\n",
      "| IK   | Inverse Kinematic                   |\n",
      "| Lisp | List Processing Language            |\n",
      "| ROS  | Robot Operating System              |\n",
      "|      |                                     |\n",
      "\n",
      "## <span id=\"page-68-0\"></span>**Appendix A**\n",
      "\n",
      "## **Real-World Force Data**\n",
      "\n",
      "Following are the average force-torque responses for 5 tested assembly scenarios and a confusion matrix of classifying the data by the heuristic in section [3.2.2.](#page-34-0) In each scenario the gripper is put at an offset with respect to the gripper's orientation during the assembly. In some scenarios multiple offsets of different distance are taken, when the resulting contact forces are expected to be significantly different. In general the offset is between 5 and 10 millimeters. At least 5 data-sets are recorded for each offset. The data-set sequences are synchronized towards the point of contact, then the average is calculated. The resulting plot is show below each offset of a scenario.\n",
      "\n",
      "![](_page_69_Picture_1.jpeg)\n",
      "\n",
      "## <span id=\"page-69-1\"></span><span id=\"page-69-0\"></span>**A.1 Chassis on Horizontal Holder**\n",
      "\n",
      "![](_page_69_Figure_4.jpeg)\n",
      "\n",
      "![](_page_69_Figure_5.jpeg)\n",
      "\n",
      "<span id=\"page-70-0\"></span>![](_page_70_Figure_1.jpeg)\n",
      "\n",
      "FIGURE A.2: Chassis on horizontal holder forces with X offsets.\n",
      "\n",
      "<span id=\"page-70-1\"></span>![](_page_70_Figure_3.jpeg)\n",
      "\n",
      "FIGURE A.3: Chassis on horizontal holder forces with Y offsets.\n",
      "\n",
      "![](_page_71_Figure_1.jpeg)\n",
      "\n",
      "## <span id=\"page-71-1\"></span><span id=\"page-71-0\"></span>**A.2 Bottom-wing on Chassis**\n",
      "\n",
      "![](_page_71_Figure_3.jpeg)\n",
      "\n",
      "![](_page_71_Figure_4.jpeg)\n",
      "\n",
      "<span id=\"page-72-0\"></span>![](_page_72_Figure_1.jpeg)\n",
      "\n",
      "FIGURE A.5: Bottom-wing on chassis forces with Y offsets.\n",
      "\n",
      "<span id=\"page-72-1\"></span>![](_page_72_Figure_3.jpeg)\n",
      "\n",
      "FIGURE A.6: Bottom-wing on chassis forces with Z offsets.\n",
      "\n",
      "![](_page_73_Figure_1.jpeg)\n",
      "\n",
      "## <span id=\"page-73-1\"></span><span id=\"page-73-0\"></span>**A.3 Underbody on Bottom-wing**\n",
      "\n",
      "![](_page_73_Figure_3.jpeg)\n",
      "\n",
      "![](_page_73_Figure_4.jpeg)\n",
      "\n",
      "<span id=\"page-74-0\"></span>![](_page_74_Figure_1.jpeg)\n",
      "\n",
      "FIGURE A.8: Underbody on bottom-wing and rear-wing forces with X offsets\n",
      "\n",
      "<span id=\"page-74-1\"></span>![](_page_74_Figure_3.jpeg)\n",
      "\n",
      "FIGURE A.9: Underbody on bottom-wing and rear-wing forces with Y offsets.\n",
      "\n",
      "![](_page_75_Figure_1.jpeg)\n",
      "\n",
      "## <span id=\"page-75-1\"></span><span id=\"page-75-0\"></span>**A.4 Upper-body on Underbody**\n",
      "\n",
      "FIGURE A.10: Top: X (red), Y (green), Z (blue) orientation of the end effector while assembling the upper-body onto the underbody. Bottom: Heuristic's confusion matrix for this scenario.\n",
      "\n",
      "<span id=\"page-76-0\"></span>![](_page_76_Figure_1.jpeg)\n",
      "\n",
      "FIGURE A.11: Upper-body on underbody forces with 5mm X offsets.\n",
      "\n",
      "<span id=\"page-76-1\"></span>![](_page_76_Figure_3.jpeg)\n",
      "\n",
      "FIGURE A.12: Upper-body on underbody forces with 10mm X offsets.\n",
      "\n",
      "<span id=\"page-77-0\"></span>![](_page_77_Figure_1.jpeg)\n",
      "\n",
      "FIGURE A.13: Upper-body on underbody forces with Y offsets.\n",
      "\n",
      "![](_page_78_Figure_1.jpeg)\n",
      "\n",
      "## <span id=\"page-78-1\"></span><span id=\"page-78-0\"></span>**A.5 Bolt into rear hole**\n",
      "\n",
      "![](_page_78_Figure_4.jpeg)\n",
      "\n",
      "FIGURE A.14: Top: X (red), Y (green), Z (blue) orientation of the end effector while assembling the bolt into the rear hole. Bottom: Heuristic's confusion matrix for this scenario.\n",
      "\n",
      "<span id=\"page-79-0\"></span>![](_page_79_Figure_1.jpeg)\n",
      "\n",
      "FIGURE A.15: Bolt in rear hole with X offsets.\n",
      "\n",
      "<span id=\"page-80-0\"></span>![](_page_80_Figure_1.jpeg)\n",
      "\n",
      "FIGURE A.16: Bolt in rear hole with Y offsets.\n",
      "\n",
      "# <span id=\"page-82-0\"></span>**Bibliography**\n",
      "\n",
      "- <span id=\"page-82-6\"></span>Bohren, J. et al. (2011). \"Towards autonomous robotic butlers: Lessons learned with the PR2\". In: *2011 IEEE International Conference on Robotics and Automation*, pp. 5568â€“ 5575. DOI: [10.1109/ICRA.2011.5980058](https://doi.org/10.1109/ICRA.2011.5980058).\n",
      "- <span id=\"page-82-2\"></span>Bouchard, C. et al. (2015). \"6D frictional contact for rigid bodies\". In: *Proceedings of the 41st Graphics Interface Conference, Halifax, NS, Canada, June 3-5, 2015*. Ed. by Hao (Richard) Zhang and Tony Tang. ACM, pp. 105â€“114. URL: [http://dl.acm.](http://dl.acm.org/citation.cfm?id=2788910) [org/citation.cfm?id=2788910](http://dl.acm.org/citation.cfm?id=2788910).\n",
      "- <span id=\"page-82-12\"></span>Bruyninckx, Herman, Stefan Dutre, and Joris De Schutter (1995). \"Peg-on-hole: a model based solution to peg and hole alignment\". In: *Proceedings of 1995 IEEE International Conference on Robotics and Automation*. Vol. 2. IEEE, pp. 1919â€“1924.\n",
      "- <span id=\"page-82-8\"></span>Burger, Robert et al. (2010). \"The driver concept for the DLR lightweight robot III\". In: *2010 IEEE/RSJ International Conference on Intelligent Robots and Systems, October 18-22, 2010, Taipei, Taiwan*. IEEE, pp. 5453â€“5459. DOI: [10.1109/IROS.2010.](https://doi.org/10.1109/IROS.2010.5650299) [5650299](https://doi.org/10.1109/IROS.2010.5650299). URL: <https://doi.org/10.1109/IROS.2010.5650299>.\n",
      "- <span id=\"page-82-1\"></span>Calli, B. et al. (2015). \"The YCB object and Model set: Towards common benchmarks for manipulation research\". In: *2015 International Conference on Advanced Robotics (ICAR)*, pp. 510â€“517. DOI: [10.1109/ICAR.2015.7251504](https://doi.org/10.1109/ICAR.2015.7251504).\n",
      "- <span id=\"page-82-13\"></span>Chaves-Arbaiza, I., D. GarcÃ­a-Vaglio, and F. Ruiz-Ugalde (July 2018). \"Smart Placement of a Two-Arm Assembly for An Everyday Object Manipulation Humanoid Robot Based on Capability Maps\". In: *2018 IEEE International Work Conference on Bioinspired Intelligence (IWOBI)*, pp. 1â€“9. DOI: [10.1109/IWOBI.2018.8464192](https://doi.org/10.1109/IWOBI.2018.8464192).\n",
      "- <span id=\"page-82-4\"></span>Coumans, Erwin (2015). \"Bullet physics simulation\". In: *Special Interest Group on Computer Graphics and Interactive Techniques Conference, SIGGRAPH '15, Los Angeles, CA, USA, August 9-13, 2015, Courses*. ACM, 7:1. DOI: [10 . 1145 / 2776880 .](https://doi.org/10.1145/2776880.2792704) [2792704](https://doi.org/10.1145/2776880.2792704). URL: <https://doi.org/10.1145/2776880.2792704>.\n",
      "- <span id=\"page-82-5\"></span>Cousins, S. (2010). \"ROS on the PR2 [ROS Topics]\". In: *IEEE Robotics Automation Magazine* 17.3, pp. 23â€“25. DOI: [10.1109/MRA.2010.938502](https://doi.org/10.1109/MRA.2010.938502).\n",
      "- <span id=\"page-82-14\"></span>GarcÃ­a-Vaglio D. Ruiz-Ugalde, F. (Oct. 2020). *GPU based approach for fast generation of robot capability representations*.\n",
      "- <span id=\"page-82-3\"></span>Haidu, Andrei et al. (2018). \"KNOWROB-SIM â€” Game Engine-enabled Knowledge Processing for Cognition-enabled Robot Control\". In: *International Conference on Intelligent Robots and Systems (IROS)*. IEEE. Madrid, Spain.\n",
      "- <span id=\"page-82-11\"></span>Hiemstra, P and A Nederveen (2007). \"Monte carlo localization\". In: *Ad Hoc Networks* 6.5, pp. 718â€“733.\n",
      "- <span id=\"page-82-9\"></span>Hietanen, Antti et al. (2020). \"AR-based interaction for human-robot collaborative manufacturing\". In: *Robotics and Computer-Integrated Manufacturing* 63, p. 101891.\n",
      "- <span id=\"page-82-7\"></span>Hirzinger, Gerd et al. (2002). \"DLR's Torque-Controlled Light Weight Robot III - Are We Reaching the Technological Limits Now?\" In: *Proceedings of the 2002 IEEE International Conference on Robotics and Automation, ICRA 2002, May 11-15, 2002, Washington, DC, USA*. IEEE, pp. 1710â€“1716. DOI: [10.1109/ROBOT.2002.1014788](https://doi.org/10.1109/ROBOT.2002.1014788). URL: <https://doi.org/10.1109/ROBOT.2002.1014788>.\n",
      "- <span id=\"page-82-10\"></span>Inoue, Tadanobu et al. (2017). \"Deep reinforcement learning for high precision assembly tasks\". In: *2017 IEEE/RSJ International Conference on Intelligent Robots and*\n",
      "\n",
      "*Systems, IROS 2017, Vancouver, BC, Canada, September 24-28, 2017*. IEEE, pp. 819â€“ 825. DOI: [10.1109/IROS.2017.8202244](https://doi.org/10.1109/IROS.2017.8202244). URL: [https://doi.org/10.1109/IROS.](https://doi.org/10.1109/IROS.2017.8202244) [2017.8202244](https://doi.org/10.1109/IROS.2017.8202244).\n",
      "\n",
      "- <span id=\"page-83-11\"></span>Jiang, Tao et al. (2020). \"A measurement method for robot peg-in-hole pre-alignment based on combined two-level visual sensors\". In: *IEEE Transactions on Instrumentation and Measurement*.\n",
      "- <span id=\"page-83-7\"></span>Kazhoyan, G. and M. Beetz (2019). \"Executing Underspecified Actions in Real World Based on Online Projection\". In: *2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, pp. 5156â€“5163. DOI: [10.1109/IROS40897.2019.](https://doi.org/10.1109/IROS40897.2019.8967867) [8967867](https://doi.org/10.1109/IROS40897.2019.8967867).\n",
      "- <span id=\"page-83-6\"></span>Kazhoyan, Gayane and Michael Beetz (2017). \"Programming robotic agents with action descriptions\". In: *2017 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2017, Vancouver, BC, Canada, September 24-28, 2017*. IEEE, pp. 103â€“ 108. ISBN: 978-1-5386-2682-5. DOI: [10. 1109/ IROS.2017 .8202144](https://doi.org/10.1109/IROS.2017.8202144). URL: [https:](https://doi.org/10.1109/IROS.2017.8202144) [//doi.org/10.1109/IROS.2017.8202144](https://doi.org/10.1109/IROS.2017.8202144).\n",
      "- <span id=\"page-83-8\"></span>Khansari-Zadeh, Seyed Mohammad, Ellen Klingbeil, and Oussama Khatib (2016). \"Adaptive human-inspired compliant contact primitives to perform surface-surface contact under uncertainty\". In: *Int. J. Robotics Res.* 35.13, pp. 1651â€“1675. DOI: [10.](https://doi.org/10.1177/0278364916648389) [1177/0278364916648389](https://doi.org/10.1177/0278364916648389). URL: <https://doi.org/10.1177/0278364916648389>.\n",
      "- <span id=\"page-83-13\"></span>Kumar, Deepesh et al. (2020). \"Neuromorphic Approach to Tactile Edge Orientation Estimation using Spatiotemporal Similarity\". In: *Neurocomputing*.\n",
      "- <span id=\"page-83-9\"></span>Kyung-Lyong Han et al. (2009). \"Design and control of mobile robot with Mecanum wheel\". In: *2009 ICCAS-SICE*, pp. 2932â€“2937.\n",
      "- <span id=\"page-83-4\"></span>Lee, YeonSun et al. (Nov. 2018). \"Quantitative Comparison of Acupuncture Needle Force Generation According to Diameter\". In: *Journal of Acupuncture Research* 35, pp. 238â€“243. DOI: [10.13045/jar.2018.00283](https://doi.org/10.13045/jar.2018.00283).\n",
      "- <span id=\"page-83-12\"></span>Liu, Nailong et al. (2020). \"Learning peg-in-hole assembly using Cartesian DMPs with feedback mechanism\". In: *Assembly Automation*.\n",
      "- <span id=\"page-83-2\"></span>Luo, Shan et al. (2017). \"Robotic Tactile Perception of Object Properties: A Review\". In: *CoRR* abs/1711.03810. arXiv: [1711 . 03810](http://arxiv.org/abs/1711.03810). URL: [http : / / arxiv . org / abs /](http://arxiv.org/abs/1711.03810) [1711.03810](http://arxiv.org/abs/1711.03810).\n",
      "- <span id=\"page-83-3\"></span>Maldonado, Alexis, Ulrich Klank, and Michael Beetz (2010). \"Robotic grasping of unmodeled objects using time-of-flight range data and finger torque information\". In: *2010 IEEE/RSJ International Conference on Intelligent Robots and Systems*. IEEE, pp. 2586â€“2591.\n",
      "- <span id=\"page-83-0\"></span>MÃ¶senlechner, Lorenz (2016). \"The Cognitive Robot Abstract Machine: A Framework for Cognitive Robotics\". PhD thesis. Technical University Munich, Germany. URL: [http://nbn-resolving.de/urn:nbn:de:bvb:91-diss-20160520-](http://nbn-resolving.de/urn:nbn:de:bvb:91-diss-20160520-1239461-1-3) [1239461-1-3](http://nbn-resolving.de/urn:nbn:de:bvb:91-diss-20160520-1239461-1-3).\n",
      "- <span id=\"page-83-1\"></span>Muxfeldt, Arne and Daniel Kubus (2016). \"Hierarchical decomposition of industrial assembly tasks\". In: *21st IEEE International Conference on Emerging Technologies and Factory Automation, ETFA 2016, Berlin, Germany, September 6-9, 2016*. IEEE, pp. 1â€“8. DOI: [10.1109/ETFA.2016.7733742](https://doi.org/10.1109/ETFA.2016.7733742). URL: [https://doi.org/10.1109/](https://doi.org/10.1109/ETFA.2016.7733742) [ETFA.2016.7733742](https://doi.org/10.1109/ETFA.2016.7733742).\n",
      "- <span id=\"page-83-5\"></span>MÃ¶senlechner, L. and M. Beetz (2013). \"Fast temporal projection using accurate physicsbased geometric reasoning\". In: *2013 IEEE International Conference on Robotics and Automation*, pp. 1821â€“1827. DOI: [10.1109/ICRA.2013.6630817](https://doi.org/10.1109/ICRA.2013.6630817).\n",
      "- <span id=\"page-83-10\"></span>Newman, Wyatt S., Yonghong Zhao, and Yoh-Han Pao (2001). \"Interpretation of Force and Moment Signals for Compliant Peg-in-Hole Assembly\". In: *Proceedings of the 2001 IEEE International Conference on Robotics and Automation, ICRA*\n",
      "\n",
      "*2001, May 21-26, 2001, Seoul, Korea*. IEEE, pp. 571â€“576. DOI: [10 . 1109 / ROBOT .](https://doi.org/10.1109/ROBOT.2001.932611) [2001.932611](https://doi.org/10.1109/ROBOT.2001.932611). URL: <https://doi.org/10.1109/ROBOT.2001.932611>.\n",
      "\n",
      "- <span id=\"page-84-12\"></span>Park, Hyeonjun et al. (2020). \"Compliant Peg-in-Hole Assembly Using Partial Spiral Force Trajectory With Tilted Peg Posture\". In: *IEEE Robotics and Automation Letters* 5.3, pp. 4447â€“4454.\n",
      "- <span id=\"page-84-8\"></span>Quigley, Morgan et al. (2009). \"ROS: an open-source Robot Operating System\". In: *ICRA Workshop on Open Source Software*.\n",
      "- <span id=\"page-84-14\"></span>Roth, Scott D. (1982). \"Ray casting for modeling solids\". In: *Comput. Graph. Image Process.* 18.2, pp. 109â€“144. DOI: [10.1016/0146- 664X\\(82\\)90169- 1](https://doi.org/10.1016/0146-664X(82)90169-1). URL: [https:](https://doi.org/10.1016/0146-664X(82)90169-1) [//doi.org/10.1016/0146-664X\\(82\\)90169-1](https://doi.org/10.1016/0146-664X(82)90169-1).\n",
      "- <span id=\"page-84-7\"></span>Ruiz-Ugalde, F., G. Cheng, and M. Beetz (2011). \"Fast adaptation for effect-aware pushing\". In: *2011 11th IEEE-RAS International Conference on Humanoid Robots*, pp. 614â€“621. DOI: [10.1109/Humanoids.2011.6100863](https://doi.org/10.1109/Humanoids.2011.6100863).\n",
      "- <span id=\"page-84-3\"></span>Schoettler, Gerrit et al. (2019). *Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards*. arXiv: [1906.05841 \\[cs.RO\\]](http://arxiv.org/abs/1906.05841).\n",
      "- <span id=\"page-84-4\"></span>Schoettler, Gerrit et al. (2020). \"Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks\". In: *arXiv preprint arXiv:2004.14404*.\n",
      "- <span id=\"page-84-11\"></span>Sharma, K., V. Shirwalkar, and P. K. Pal (2013). \"Intelligent and environment-independent Peg-In-Hole search strategies\". In: *2013 International Conference on Control, Automation, Robotics and Embedded Systems (CARE)*, pp. 1â€“6. DOI: [10 . 1109 / CARE .](https://doi.org/10.1109/CARE.2013.6733716) [2013.6733716](https://doi.org/10.1109/CARE.2013.6733716).\n",
      "- <span id=\"page-84-5\"></span>Stelter, Simon, Georg Bartels, and Michael Beetz (2018). \"Multidimensional Time-Series Shapelets Reliably Detect and Classify Contact Events in Force Measurements of Wiping Actions\". In: *IEEE Robotics Autom. Lett.* 3.1, pp. 320â€“327. DOI: [10 . 1109 / LRA . 2017 . 2716423](https://doi.org/10.1109/LRA.2017.2716423). URL: [https : / / doi . org / 10 . 1109 / LRA . 2017 .](https://doi.org/10.1109/LRA.2017.2716423) [2716423](https://doi.org/10.1109/LRA.2017.2716423).\n",
      "- <span id=\"page-84-0\"></span>Sun, Yi et al. (2020). \"Learn How to Assist Humans Through Human Teaching and Robot Learning in Human-Robot Collaborative Assembly\". In: *IEEE Transactions on Systems, Man, and Cybernetics: Systems*.\n",
      "- <span id=\"page-84-6\"></span>Taddeucci, Davide et al. (1997). \"An approach to integrated tactile perception\". In: *Proceedings of the 1997 IEEE International Conference on Robotics and Automation, Albuquerque, New Mexico, USA, April 20-25, 1997*. IEEE, pp. 3100â€“3105. DOI: [10.](https://doi.org/10.1109/ROBOT.1997.606759) [1109 / ROBOT . 1997 . 606759](https://doi.org/10.1109/ROBOT.1997.606759). URL: [https : / / doi . org / 10 . 1109 / ROBOT . 1997 .](https://doi.org/10.1109/ROBOT.1997.606759) [606759](https://doi.org/10.1109/ROBOT.1997.606759).\n",
      "- <span id=\"page-84-10\"></span>Takahashi, Tomoichi and Hiroyuki Ogata (1992). \"Robotic assembly operation based on task-level teaching in virtual reality\". In: *Proceedings 1992 IEEE International Conference on Robotics and Automation*. IEEE Computer Society, pp. 1083â€“1084.\n",
      "- <span id=\"page-84-13\"></span>Tsujimura, T. and T. Yabuta (1989). \"Object detection by tactile sensing method employing force/torque information\". In: *IEEE Transactions on Robotics and Automation* 5.4, pp. 444â€“450. DOI: [10.1109/70.88059](https://doi.org/10.1109/70.88059).\n",
      "- <span id=\"page-84-2\"></span>Wong, Percy Charles (1975). \"Peg-hole assembly; an investigation into tactile methods\". PhD thesis. University of Canterbury. Mechanical Engineering.\n",
      "- <span id=\"page-84-9\"></span>Yamamoto, Takashi et al. (2018). \"Human support robot (HSR)\". In: *ACM SIGGRAPH 2018 emerging technologies*, pp. 1â€“2.\n",
      "- <span id=\"page-84-1\"></span>Yang, L. et al. (2020). \"Rigid-Soft Interactive Learning for Robust Grasping\". In: *IEEE Robotics and Automation Letters* 5.2, pp. 1720â€“1727. DOI: [10 . 1109 / LRA . 2020 .](https://doi.org/10.1109/LRA.2020.2969932) [2969932](https://doi.org/10.1109/LRA.2020.2969932).\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T13:44:26.824279Z",
     "start_time": "2025-04-22T13:44:26.815535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "data1 = {\"text\": text_content}\n",
    "datasets1 = Dataset.from_dict(data1)\n",
    "data2 = {\"text\": docs_content}\n",
    "datasets2 = Dataset.from_dict(data2)\n",
    "datasets = concatenate_datasets([datasets1, datasets2])\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ],
   "id": "4b73d48380ae4525",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T13:44:27.533749Z",
     "start_time": "2025-04-22T13:44:27.527384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    return { \"text\" : [example + EOS_TOKEN for example in examples[\"text\"]] }\n",
    "datasets = datasets.map(formatting_prompts_func, batched = True,)"
   ],
   "id": "ad91c1bbf88ddbb7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 4775.86 examples/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T13:44:29.037471Z",
     "start_time": "2025-04-22T13:44:29.034520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for row in datasets[3:4][\"text\"]:\n",
    "    print(\"=========================\")\n",
    "    print(row)"
   ],
   "id": "c14b5c0c6766fffb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "UniversitÃ¤t Bremen Faculty 3, Mathematics and Computer Science\n",
      "\n",
      "# **Masterthesis**\n",
      "\n",
      "**Teaching robots manipulation skills with human controlled robots bodies - Spend a day in Virtual Reality inside a robots body** VR-basierte Methoden zur Vermittlung von ManipulationsfÃ¤higkeiten in der Robotik - Verbringe den Tag in einem RoboterkÃ¶rper in einer virtuellen RealitÃ¤t.\n",
      "\n",
      "> Alina Hawkin 2926383 Supervisor: Prof. Michael Beetz PhD Second Supervisor: Dr. RenÃ© Weller Advisor: Gayane Kazhoyan Advisor: Andrei Haidu\n",
      "\n",
      "> > December 7, 2020\n",
      "\n",
      "# **Declaration of Authorship**\n",
      "\n",
      "I, Alina Hawkin, declare that this thesis, titled \"Teaching robots manipulation skills with human controlled robots bodies -\n",
      "\n",
      "Spend a day in Virtual Reality inside a robots body\" and the work presented in it are my own and has been generated by me as the result of my own original research. I confirm that:\n",
      "\n",
      "- This work was done wholly or mainly while in candidature for a research degree at the University of Bremen.\n",
      "- Where any part of this thesis has previously been submitted for a degree or any other qualification at this University or any other institution, this has been clearly stated.\n",
      "- Where I have consulted the published work of others, this is always clearly attributed.\n",
      "- Where I have quoted from the work of others, the source is always given. With the exception of such quotations, this thesis is entirely my own work.\n",
      "- I have acknowledged all main sources of help.\n",
      "- Where the thesis is based on work done by myself jointly with others, I have made clear exactly what was done by others and what I have contributed myself.\n",
      "\n",
      "Alina Hawkin 07.12.2020, Bremen\n",
      "\n",
      "Date\n",
      "\n",
      "## **Abstract**\n",
      "\n",
      "Teaching robots how to perform everyday household activities while at the same time, experiencing how it is like to perform these tasks as a robot. This thesis proposes the autogeneration of a skeletal mesh of the PR2 robot and its transfer into a Virtual Reality environment, where a human user can perform everyday household tasks from inside the virtual robots body. In order make the virtual robot move as realistic as possible, an inverse kinematics solver is implemented and used to calculate positions of the arms for the virtual robot. Since the same inverse kinematics solver is used on the virtual robot as on the real one, it can be assured that if the virtual robot was able to perform a certain action and a kinematic solution was found, the same movement will be possible with the real robot in the real world. Furthermore, several adjustments are implemented in order to map the human body to the robots one, including the limits a robots body would introduce. The Goal of this is, that with the help of these limits, a human user will be able to generate much more precise and better usable data for the robot to learn from.\n",
      "\n",
      "## **Abstrakt**\n",
      "\n",
      "Einen Roboter lehren, alltÃ¤gliche Haushalts TÃ¤tigkeiten zu verrichten, wÃ¤hrend man selbst erfÃ¤hrt, wie es ist solche Aufgaben als Roboter zu bearbeiten. Diese Thesis schlÃ¤gt die automatische Generierung des Animations-Skelets eines Roboters vor, und die Ãœbertragung dessen in eine Virtuale RealitÃ¤t, in der ein Mensch alltÃ¤gliche Haushalt TÃ¤tigkeiten als Roboter verrichten kann. Um die Bewegung des virtuellen Roboters so realistisch wie mÃ¶glich zu gestalten, wurde der gleiche inverse Kinematik-Solver fÃ¼r den virtuellen Roboter implementiert, wie ihn auch der reale Roboter verwendet. Dies garantiert dem menschlichen Benutzer, dass wenn der virtuelle Roboter eine bestimmte Bewegung mit den Armen ausfÃ¼hren kann, dann wird der reale Roboter es genauso tun kÃ¶nnen. Es werden auÃŸerdem einige Anpassungen implementiert, die den Menschlichen KÃ¶rper dem des Roboters von den Limitierungen her, annÃ¤hern. Das Ziel dieser Limitierungen ist es, dass ein Mensch viel prÃ¤zisere und damit bessere Daten generieren kann, um den Roboter anhand dieser daten dann besser lehren zu kÃ¶nnen.\n",
      "\n",
      "# **Contents**\n",
      "\n",
      "| 1<br>Introduction<br>8<br>Motivation . |                              |                                   |                                                                      |          |  |\n",
      "|----------------------------------------|------------------------------|-----------------------------------|----------------------------------------------------------------------|----------|--|\n",
      "|                                        | 1.1                          |                                   |                                                                      | 8        |  |\n",
      "|                                        | 1.2<br>1.3                   | Hypothesis .                      | Contribution .                                                       | 8<br>9   |  |\n",
      "|                                        | 1.4                          |                                   | Structure of this Thesis .                                           | 9        |  |\n",
      "| 2                                      |                              | Related Work                      |                                                                      | 10       |  |\n",
      "| 3                                      |                              | Foundations                       |                                                                      | 15       |  |\n",
      "|                                        | 3.1                          |                                   | ROS - Robot Operating System                                         | 15       |  |\n",
      "|                                        |                              | 3.1.1                             | URDF                                                                 | 15       |  |\n",
      "|                                        | 3.2                          | Blender                           |                                                                      | 15       |  |\n",
      "|                                        |                              | 3.2.1                             | Skeletal Meshes                                                      | 16       |  |\n",
      "|                                        |                              | 3.2.2                             | phobos-Plugin .                                                      | 16       |  |\n",
      "|                                        | 3.3                          |                                   | Unreal Engine .                                                      | 16       |  |\n",
      "|                                        | 3.4                          |                                   | RobCog - Robot Commonsense Games                                     | 17       |  |\n",
      "|                                        | 3.5                          |                                   | KDL and Eigen                                                        | 18       |  |\n",
      "|                                        | 3.6                          |                                   | USemLog - Logging Data in Virtual Reality                            | 18       |  |\n",
      "|                                        | 3.7                          |                                   | CRAM - Cognitive Robot Abstract Machine                              | 18       |  |\n",
      "|                                        | 3.8                          | KnowRob                           |                                                                      | 19       |  |\n",
      "| 4                                      |                              | Approach and Implementation<br>20 |                                                                      |          |  |\n",
      "|                                        | 4.1<br>Architecture Overview |                                   |                                                                      | 20       |  |\n",
      "|                                        | 4.2                          |                                   | Implementation                                                       | 20       |  |\n",
      "|                                        |                              | 4.2.1                             | Blender and Phobos: generating a skeletal mesh from a URDF file      | 20       |  |\n",
      "|                                        |                              |                                   | 4.2.1.1<br>preparation of the URDF-File .                            | 20       |  |\n",
      "|                                        |                              |                                   | 4.2.1.2<br>Skeletal Mesh generation with phobos                      | 21       |  |\n",
      "|                                        |                              |                                   | 4.2.1.3<br>Export of the FBX Model from Blender to Unreal Engine     | 23       |  |\n",
      "|                                        |                              | 4.2.2                             | Unreal Engine: Preparing the Robots model for VR .                   | 25       |  |\n",
      "|                                        |                              |                                   | 4.2.2.1<br>Import of the FBX Model                                   | 25       |  |\n",
      "|                                        |                              |                                   | 4.2.2.2<br>Base movement with and without VR                         | 26       |  |\n",
      "|                                        |                              | 4.2.3                             | Skeletal Mesh Animation using Inverse Kinematics<br>4.2.3.1<br>CCDIK | 29<br>31 |  |\n",
      "|                                        |                              |                                   | 4.2.3.2<br>CCDIK + Apply Limits                                      | 32       |  |\n",
      "|                                        |                              |                                   | 4.2.3.3<br>FABRIK .                                                  | 33       |  |\n",
      "|                                        |                              |                                   | 4.2.3.4<br>Other IK Nodes .                                          | 33       |  |\n",
      "|                                        |                              |                                   | 4.2.3.5<br>Split CCDIK and ApplyLimits to Parts of the Arm           | 33       |  |\n",
      "|                                        |                              |                                   | 4.2.3.6<br>CCDIK with Physics .                                      | 35       |  |\n",
      "|                                        |                              |                                   | 4.2.3.7<br>KDL .                                                     | 37       |  |\n",
      "|                                        |                              | 4.2.4                             | Unreal Engine: Animation of the PR2 robot                            | 41       |  |\n",
      "|                                        |                              |                                   | 4.2.4.1<br>Opening and Closing Grippers                              | 41       |  |\n",
      "|                                        |                              |                                   | 4.2.4.2<br>Grasping Objects .                                        | 42       |  |\n",
      "|                                        |                              |                                   | 4.2.4.3<br>PR2 Head motion                                           | 42       |  |\n",
      "|                                        |                              | 4.2.5                             | Robot to Human Body Adaptation                                       | 43       |  |\n",
      "|                                        |                              |                                   | 4.2.5.1<br>Gripper and Arm Range Extension while Compensating        |          |  |\n",
      "|                                        |                              |                                   | for Torso Bending                                                    | 43       |  |\n",
      "|                                        |                              |                                   | 4.2.5.2<br>Mirror .                                                  | 44       |  |\n",
      "\n",
      "|   |     |                | 4.2.5.3                 | Body height adjustment                                               |  |  |  |  | 45 |\n",
      "|---|-----|----------------|-------------------------|----------------------------------------------------------------------|--|--|--|--|----|\n",
      "|   |     | 4.2.6          |                         | Adapting USemLog to collect VR data                                  |  |  |  |  | 45 |\n",
      "| 5 |     |                | Experimental evaluation |                                                                      |  |  |  |  | 47 |\n",
      "|   | 5.1 |                |                         | Evaluation of the VR-PR2-model and behavior .                        |  |  |  |  | 47 |\n",
      "|   |     | 5.1.1          |                         | Differences between the systems based on general differences between |  |  |  |  |    |\n",
      "|   |     |                |                         | the human and robots bodies                                          |  |  |  |  | 47 |\n",
      "|   |     |                | 5.1.1.1                 | General look and feel .                                              |  |  |  |  | 47 |\n",
      "|   |     |                | 5.1.1.2                 | Real world navigation mapping to VR                                  |  |  |  |  | 48 |\n",
      "|   |     |                | 5.1.1.3                 | Robot to human height adjustment .                                   |  |  |  |  | 49 |\n",
      "|   |     |                | 5.1.1.4                 | Human torso bending                                                  |  |  |  |  | 49 |\n",
      "|   |     |                | 5.1.1.5                 | Human feet and the robot-base position                               |  |  |  |  | 50 |\n",
      "|   |     |                | 5.1.1.6                 | Human arms and robot arms                                            |  |  |  |  | 50 |\n",
      "|   | 5.2 |                |                         | Result of the Evaluation of the VR-PR2-model and behavior            |  |  |  |  | 51 |\n",
      "|   | 5.3 |                |                         | PR2 KDL IK Evaluation .                                              |  |  |  |  | 52 |\n",
      "|   | 5.4 |                |                         | PR2-body-based VR data evaluation                                    |  |  |  |  | 53 |\n",
      "| 6 |     | Conclusion     |                         |                                                                      |  |  |  |  | 54 |\n",
      "|   | 6.1 | Summary        |                         |                                                                      |  |  |  |  | 54 |\n",
      "|   | 6.2 | Discussion     |                         |                                                                      |  |  |  |  | 54 |\n",
      "|   | 6.3 | Future Work .  |                         |                                                                      |  |  |  |  | 55 |\n",
      "| 7 |     | Appendix       |                         |                                                                      |  |  |  |  | 57 |\n",
      "|   | 7.1 | Bibliography . |                         |                                                                      |  |  |  |  | 57 |\n",
      "|   | 7.2 | Acronyms       |                         |                                                                      |  |  |  |  | 62 |\n",
      "\n",
      "## **Acknowledgement**\n",
      "\n",
      "I would like to thank Prof. Michael Beetz for introducing me to this research area, which has now accompanied me through two theses, and which I hopefully can continue to work on in the Future. I would also like to thank Dr. Rene Weller for giving me advice when I needed some and calling my attention to aspects of this topic I would have probably otherwise disregarded. I would like to thank the Institute of Artificial Intelligence Bremen for helping me to find a way to finish this thesis, even with regards to the current world situation. I would like to thank my advisors, Gayane Kazhoyan for always helping me find a solution, no matter what the problem might be and Andrei Haidu for answering all of my silly questions. I would also like to thank Sabine Veit for her support throughout all the years - it's been a while. And I would like to thank Patrick Mania and Dominic Kastens who are always there to help me debug the most ridiculous pc issues. Last but not least, I'd like to thank my better half, for keeping me sane during dark transform calculation times and my parents, who always did their best to support me. I couldn't have made it without you guys. Thank you!\n",
      "\n",
      "## <span id=\"page-7-0\"></span>**1 Introduction**\n",
      "\n",
      "## <span id=\"page-7-1\"></span>**1.1 Motivation**\n",
      "\n",
      "Teaching by demonstration is a very intuitive and practical approach, to pass on a skill from one to another and has been also generally applied to robotics in various ways. Within my bachelors thesis[\\[10\\]](#page-56-2) I looked into how Virtual Reality data can be used to teach a robot simple pick and place tasks, with a breakfast setup scenario in mind. The setup used there was based on the RobCoG[1](#page-7-3) project which consisted of two human hands which could be controlled via motion controllers by the human user and a replica of the kitchen environment of the PR2 robot.[2](#page-7-4) . Since there was no body visualized within the Virtual Reality environment, as a human user performing pick and place tasks, one always had to keep in mind and think about how a robot would perform this exact same task. For example, as a human it is very natural to bend the torso forward when reaching for an object. A robot can only rarely do that. The PR2 robot, on whom this research so far has been tested, has no option of bending the torso. He might however be able to compensate for this with the length of his arms. But now we have the reverse problem, that the robots arms are a lot longer than the humans are[\\[20\\]](#page-57-0), and a solution must be found on how this could be accounted for. Another issue was that only the positions of the two hand held controllers could be tracked within the Virtual Reality environment, and the position of the camera. There was no way to track where the human feet were placed. The solution in order to obtain a navigation pose for the robot back then was to remove the height component of the camera position and project it onto the floor, to estimate where the user might have stood. Another disparity were the human hands within VR. The robot has grippers with only two fingers instead of the humans five. This drastically changes the way the robot can grasp and interact with things compared to a human.\n",
      "\n",
      "These and some other issues can be addressed and potentially solved by introducing a robots body into the Virtual Reality system, within which the human user can perform everyday household activities. With such a robots body, the human user would gain better understanding of the robot's capabilities and therefore be able to generate data which is a lot more suitable for the robot. Also, all the positions of the links and joints of the robots body can now be tracked and recorded. The navigation pose for the robots base does not have to be generated based on the camera anymore but can be obtained from the robots skeletal mesh body.\n",
      "\n",
      "## <span id=\"page-7-2\"></span>**1.2 Hypothesis**\n",
      "\n",
      "How can a PR2 robot model be generated efficiently for an Virtual Reality environment within the Unreal Engine? How can the robots body limitations be implemented to the model and transferred to the human user? Will the human user within such a virtual robots body be able to generate more suitable data for the robot? The assumption is that scripts could be implemented and used to auto generate a robot model suitable for the Unreal Engine based on the Unified Robot Description Language. It is assumed that an already existing inverse kinematics solver within the Unreal Engine can be used to calculate the robot bodies movement with respect to the robots capabilities and limits.\n",
      "\n",
      "<span id=\"page-7-3\"></span><sup>1</sup>RobCog: <http://robcog.org/>\n",
      "\n",
      "<span id=\"page-7-4\"></span><sup>2</sup>RobCoG: <http://robcog.org/> (last accessed: 06.12.2020)\n",
      "\n",
      "Since the human user will be made more aware of the robots capabilities, the assumption is made that the so generated data will perform better at teaching robots to perform everyday activities than the previous setup[\\[11\\]](#page-56-3) without any virtual body.\n",
      "\n",
      "## <span id=\"page-8-0\"></span>**1.3 Contribution**\n",
      "\n",
      "The contribution of this thesis will be a skeletal mesh model of the PR2 robot, auto-generated from an URDF-file, fully human controllable within a Virtual Reality environment. The movement of the robots arms will use an inverse kinematics solution, so that the robots movement can be kept as realistic as possible. Some solutions will be explored and experimented with, which concern the mapping of the PR2s body to the human body. The goal there is to prevent the human user from performing movements which the robot will not be able to replicate. As a continuation of previous work[\\[10\\]](#page-56-2)[\\[11\\]](#page-56-3), some evaluation of this newly presented approach should be performed, so that it is comparable to the previous work.\n",
      "\n",
      "## <span id=\"page-8-1\"></span>**1.4 Structure of this Thesis**\n",
      "\n",
      "This thesis will introduce the approach of autogenerating a skeletal mesh for a robot and introducing it into a Virtual Reality environment in order to collect data on how to perform everyday household activities as a robot. To make this as realistic as possible, the same inverse kinematics solution will be used on the virtual robot, as is used on the real one. Furthermore, some limits will be introduced in order to limit the human user to the movement capabilities of the robot.\n",
      "\n",
      "This thesis is structured as follows:\n",
      "\n",
      "**Ch.2 Related Work:** will present similar work done in this field and will also discuss how this approach is different from the already existing ones.\n",
      "\n",
      "**Ch.3 Foundations:** within this chapter all tools and programs will be briefly introduced which play a role and are used within this thesis.\n",
      "\n",
      "**Ch.4 Approach and Implementation:** in this chapter all the implementation work will be described.\n",
      "\n",
      "**Ch.5 Experimental Evaluation:** will contain the evaluation and an overview over the performance of the resulting product.\n",
      "\n",
      "**Ch.6 Conclusion:** after giving a brief summary of the work done within this thesis, some of the problems which have been encountered will be discussed, as well as how this research can be continued.\n",
      "\n",
      "## <span id=\"page-9-0\"></span>**2 Related Work**\n",
      "\n",
      "Having robots which can help us in every day life by performing household activities is probably one of the oldest goals the scientific field of robotics has. It seems to be a very useful goal for many and has been the subject of countless research projects already. The lack of robots performing every day activities in our homes however, proves that it is a rather difficult task to achieve. Many home environments while fulfilling the same functions, look very different. For example, a kitchen will almost guaranteed have a stove, a dishwasher, a fridge and a dinner table, but the positions of these items, how they look like and how they are operated can vary greatly. A fridge can look like any other cupboard in the kitchen, the stove can be electric or gas operated, which would require different kind of handling, the dinner table might be sometimes located in a different room at all. Not to mention that different people store their kitchen items in the most various locations. People will also have different ways of setting up a breakfast table. With all this variations, it is very difficult to develop a robot, which would be able to perform in all of these different environments reliably and which will find all the necessary items on the first try for e.g. a breakfast. A solution for this problem, also a rather old and fundamental idea in its core, is to simply show a robot where the items are, and how one would like the breakfast table to be setup. After all, this is how we, as humans, teach each other also. So why not teach the robots the same way? The difficulty here lies also in how we would teach the robot. Many options have been already looked into by different researchers and will be described further in this chapter. One could teach a robot a new movement or action via kinesthetic learning, teleoperation, simple visual observation, or since the recent development of technology allows it, via Virtual Reality. This very last approach allows the robot to gain absolute data about its environment, since in a Virtual World, every cupboard's position, color, contents and much more can be easily recorded and passed on to the robot as data to learn from. It is also possible to change the environment fairly easily, allowing the collection of data across many different environments without much work. There are several projects which have looked into this approach and many questions and aspects associated with it, since while \"learning from VR\" sounds like a simple idea, it can be done in very many different ways.\n",
      "\n",
      "The closest related work to this thesis is its predecessor, *\"Towards robots executing observed manipulation activities of humans\"* [3](#page-9-1) and the resulting paper *\"Learning Motion Parameterizations of Mobile Pick and Place Actions from Observing Humans in Virtual Environments*[4](#page-9-2) . In this previous work, it was looked into if data acquired from Virtual Reality of every day household activities like setting up a breakfast table, can be used to teach a robot to perform the same task. In the Bachelors-thesis, the necessary chain of frameworks was developed, from collecting the data in VR, to loading it into a Knowledge base[\\[7\\]](#page-56-4) and using it within the CRAM[\\[14\\]](#page-57-1) planning tool to execute the household task within simulation[\\[13\\]](#page-56-5). In the following work[\\[11\\]](#page-56-3) this approach was developed further by generalizing the collected data, making it independent of the exact environment it was collected from. More data was collected within different virtual kitchen environments to be able to further generalize. It was also extensively tested, with two different robots\n",
      "\n",
      "<span id=\"page-9-1\"></span><sup>3</sup> [\\[10\\]](#page-56-2): Alina Hawkin. \"Towards robots executing observed manipulation activities of humans\". Bachelor Thesis. Institute of Artificial Intelligence, University of Bremen. (Visited on 04/23/2018)\n",
      "\n",
      "<span id=\"page-9-2\"></span><sup>4</sup> [\\[11\\]](#page-56-3): Gayane Kazhoyan et al. \"Learning Motion Parameterizations of Mobile Pick and Place Actions from Observing Humans in Virtual Environments\". In: *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*. 2020\n",
      "\n",
      "within the bullet world simulation[\\[13\\]](#page-56-5) and in the real world on the PR2 robot. The success rate of the robot being able to perform the simple breakfast table setup based on this data was very high, even though there were not that many positions to reference from. For example, with the previous approach, only the hands of the human user can be tracked and the headset. There are no feet positions recorded of where the human user stood while performing a task, so that a navigation position had to be estimated based on the location of the head mounted display. The grasping of objects with the human hands is very different than what the robot can do with his two fingers, leading to impossible or rather hard grasping configurations. There are no visual ques for the human user as to how big the robots base actually is, which leads to the human user standing very close to furniture. If the robot were to use that pose just as is, he would collide with the furniture, which is why some offsets needed to be applied to the gathered poses.\n",
      "\n",
      "In this thesis, keeping the previous work in mind, some of the issues described above should be resolved. By generating a robot body for the Virtual Reality environment, it will be possible to fully track that body, including the positions of the base and all links and joints. Better data could be generated simply by the human user being more aware of the robots limits.\n",
      "\n",
      "The *VirtualHome*[\\[15\\]](#page-57-2)[5](#page-10-0) -project created a data-set, which contains descriptions of everyday household activities in natural language and in the form of so called *programs* which are symbolic representations of these activities, defining every step in sequence, which is needed to perform the said activity. This data-set was aimed to be used by robots, in order to be able to perform these everyday activities and various household environments. The *VirtualHome* itself, and the name giver to this project, is a 3D simulator within the Unity[6](#page-10-1) game engine used to simulate these household activities. An virtual agent within this simulation was successfully used to perform these activities based on the previously generated programs. Furthermore, by placing multiple cameras within the virtual environment, it was possible to generate a video data-set of the performance of the everyday activities, which again can be used for further learning by robots. Based on this data the *VirtualHome* project team was able to show that an agent within the simulation was able to perform a household task given only a natural language description of it and a model, allowing the agent to learn from the provided programs and videos. Based on this work, these *programs* were developed further in the authors following work[7](#page-10-2) now also using *Activity Sketches*, which describe the central essence of an activity, which then can be used with given environmental constrains to generate a program, specifically designed to perform the given activity within the limits and capabilities the given environment provides.\n",
      "\n",
      "This project is similar to this thesis' idea in the sense that both projects work towards creating a data-set a robot can learn from, however the techniques applied and the desired results are different. While the *VirtualHome*[\\[15\\]](#page-57-2) uses natural language descriptions obtained from crowd sourcing as the basis if their task descriptions and their *program*\n",
      "\n",
      "<span id=\"page-10-0\"></span><sup>5</sup> [\\[15\\]](#page-57-2): Xavier Puig et al. *VirtualHome: Simulating Household Activities via Programs*. 2018. arXiv: [1806.07011 \\[cs.CV\\]](https://arxiv.org/abs/1806.07011)\n",
      "\n",
      "<span id=\"page-10-1\"></span><sup>6</sup>Unity game engine: <https://unity.com/>\n",
      "\n",
      "<span id=\"page-10-2\"></span><sup>7</sup> [\\[12\\]](#page-56-6): Yuan-Hong Liao et al. \"Synthesizing Environment-Aware Activities via Activity Sketches\". In: *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*. 2019\n",
      "\n",
      "*generation*, this thesis aims more towards generating a data-set based on the general positions of objects within the household environment and the generalization that can be performed based on these. The task description or the so called *programs*, are assumed to already be there in the form of *CRAM-plans* (which will be described in the [Foundations](#page-14-0) section). Also the focus of this thesis for the data generation aspect is to replicate the robots body in Virtual Reality so that the in the future obtained positions are more suitable for the robot, since the human and robot body both differ largely in size and capability. The use of VR also compared to the video data set of the *VirtualHome*-project would allow for absolute knowledge about the environment, without having to deal with the problem of occlusion of the task by objects or the actor himself.\n",
      "\n",
      "The *ROS Reality*[8](#page-11-0)[9](#page-11-1) framework provides a connection between the ROS[10](#page-11-2) framework and any Virtual Reality hardware supported by Unity[11](#page-11-3). It was developed and used to teleoperate a robot with the help of VR, based on kinetetic teaching. This means that instead of being inside the robots body, like presented in this thesis, the human user is outside the robots body, but is able to guide the robot by grasping the joints of the VR Robot and moving them manually into a goal position, like one would do with kenestatic learning. It was further made possible to set an end-effector target goal with the VR controller, send it via ROS to the robots own inverse kinematics solver, and visualize the result to the human operator based on if a solution for the target pose was found or not. The *ROS Reality* project also included a URDF parser, which auto-generates the robot within the Unity game engine by assembling simple game objects and connecting them with joints, as described in the URDF.\n",
      "\n",
      "In this thesis a model of the PR2 robot will also be generated for the use within the Unreal Engine (which will be introduced in the upcoming [Foundations](#page-14-0) chapter), but instead of just being assembled with game objects, it will be a skeletal mesh, which allows the use of the Unreal Engine build in kinematics and animation tools. The approach of kinestetic teaching provides the benefit that the robots links and joints can be posed very precisely. However, this way of moving the robot is a lot slower than just controlling the robots manipulators directly. Also, this thesis' approach differs from the *ROS Reality* one also in the fact that teleoperaton is not the end goal. While it probably would be possible to integrate it also, it might be done so in future work but not in this one. Also while sending one goal pose to the robots inverse kinematics solver and waiting for the robot to reach that pose is an intuitive way to teleoperate the robot, the way it currently seems to be implemented is that only one goal is being sent at a time, and then the human operator needs to wait for the computed result and the visualization. It is not performed continuously in real time, in the sense that the robot does not track the movement of the VR motion controller continuously using inverse kinematics, which is the goal of this thesis.\n",
      "\n",
      "A very close field to Virtual Reality is of course Augmented Reality, which has been used\n",
      "\n",
      "<span id=\"page-11-0\"></span><sup>8</sup> [\\[19\\]](#page-57-3): D. Whitney et al. \"ROS Reality: A Virtual Reality Framework Using Consumer-Grade Hardware for ROS-Enabled Robots\". In: *2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*. 2018, pp. 1â€“9. doi: [10.1109/IROS.2018.8593513](https://doi.org/10.1109/IROS.2018.8593513)\n",
      "\n",
      "<span id=\"page-11-1\"></span><sup>9</sup> [\\[16\\]](#page-57-4): Eric Rosen et al. \"Testing Robot Teleoperation using a Virtual Reality Interface with ROS Reality\". In: Mar. 2018\n",
      "\n",
      "<span id=\"page-11-2\"></span><sup>10</sup>[\\[5\\]](#page-56-7): Open Source Robotics Foundation. *Bullet world demonstration*. url: [http://cram-system.org/](http://cram-system.org/tutorials/intermediate/bullet_world) [tutorials/intermediate/bullet\\\\_world](http://cram-system.org/tutorials/intermediate/bullet_world) (visited on 04/09/2018)\n",
      "\n",
      "<span id=\"page-11-3\"></span><sup>11</sup>Unity game engine: <https://unity.com/>\n",
      "\n",
      "in conjunction with Virtual Reality[\\[4\\]](#page-56-8)[12](#page-12-0) to enhance the embodiment element teleoperation of a robot. While oftentimes in teleoperation the output of the 3D camera of a robot is used as the main source of observation of what the robot is currently doing by simply being mapped onto the headset of the human teleoperator, often times this visual representation is not enough and can lead to performance errors being caused by the end effector moving outside the field of view of the camera, or the low resolution of the 3D camera. The paper[\\[4\\]](#page-56-8) describes how by the use of AR the immersion aspect can be further increased by providing additional information to the teleoperator, in form of a 3D model representing the robots position even if it moves out of the field of view of the camera, or by providing other visual ques like visualizing the amount by which the gripper is closed with bars or by visualizing paths for the end effector to the target the robot should manipulate.\n",
      "\n",
      "While the above mentioned work is once again targeting the problem of teleoperation, some of the findings are very interesting and could be applied or developed within the future work of this thesis. For example, the paper[\\[4\\]](#page-56-8) states that the users of this system reported an increased sense of embodiment and that the learning curve of teleoperating the robot was significantly reduced. It remains to be seen to what degree these kinds of visualizations can be applied to this project.\n",
      "\n",
      "Visualizing parts of the robot within Virtual Reality in order to obtain better pick and place data has also been a point of research in Zihe Xu's masters thesis.[13](#page-12-1). Instead of adding the full robot body to the Virtual Reality, only some elements were added. The overall goal was to make the human user aware of the limitations of the robot. The field of view of the human via the VR headset was adapted to the same range and size as the robots. A robot usually perceives one image, analyses it and then performs the task based on that data a trigger based visualization was implemented also. A square robot base is attached to the humans position within Virtual Reality so that the human user is made aware of the size of the robots base and therefore can avoid collisions between the robot and the furniture. A visual que in the form of an arrow is also added to notify the human user before a collision can occur. The potential collision object is also highlighted visually. In order to compensate for the disparity in length of human vs. robots arms, Zihe measured the arms of the human user and added an offset, so that the outstretched length of the human arms would match the ones of the robot. Another problem between the two bodies is that the human user can bend forward in order to gain more arm reachability, while the robot cannot. In order to limit this, an approach with an additional tracker mounted on the chest of the user was tried, also providing a visual que if a bending motion was performed. However the sensor proved to be rather unreliable and often times got occluded by the humans arms, which influenced the obtained data negatively. The grippers used here were not the PR2s grippers, even though the limitations applied to the perception and interaction between the human user and the virtual reality environment were largely PR2 inspired, other, more simple parallel grippers were used instead.\n",
      "\n",
      "Zihes work was very largely focused on the perception side. Both, in the ways of how limitations to the field of view were applied as well as how visual ques can be implemented to make the human user more aware of the robots limits. This thesis however will focus\n",
      "\n",
      "<span id=\"page-12-0\"></span><sup>12</sup>[\\[4\\]](#page-56-8): F. Brizzi et al. \"Effects of Augmented Reality on the Performance of Teleoperated Industrial Assembly Tasks in a Robotic Embodiment\". In: *IEEE Transactions on Human-Machine Systems* 48.2 (2018), pp. 197â€“206. doi: [10.1109/THMS.2017.2782490](https://doi.org/10.1109/THMS.2017.2782490)\n",
      "\n",
      "<span id=\"page-12-1\"></span><sup>13</sup>[\\[20\\]](#page-57-0): Zihe Xu. \"Designing Human-controlled Robots in VR for Learning Everyday Manipulation Tasks\". Master Thesis. Institute of Artificial Intelligence, University of Bremen. (Visited on 12/10/2019)\n",
      "\n",
      "more on replicating the PR2s body completely within the Virtual Reality environment. Both theses have in common that the role model for the applied limitations is the PR2 robot and the body differences between robot and human will be addressed in both also, but different ways, presenting different solutions to the same initial problem.\n",
      "\n",
      "## <span id=\"page-14-0\"></span>**3 Foundations**\n",
      "\n",
      "The following section will introduce all the tools, frameworks, plugins which have been used within this thesis. Some of the tools and plugins have been adapted or changed in the process. All changes will be further described in the **Implementation**[4](#page-19-0) section. All software which has been adapted, is open-source. The order of mention within this chapter corresponds to order of use and implementation.\n",
      "\n",
      "## <span id=\"page-14-1\"></span>**3.1 ROS - Robot Operating System**\n",
      "\n",
      "The Robot Operating System[14](#page-14-4)(ROS) is a framework which provides many tools and libraries necessary to develop robot software. It supports the most commonly used programming languages, allowing to implement the necessary algorithms with the for them most efficient language. This is made possible by a standardized communication protocol which needs to implemented within the nodes which need to communicate via ROS. The entire system is similar in concept to building blocks, where researchers all over the world can share their solutions to robotic problems. ROS helped the preexisting problem, in which every robotics research lab or institute had to develop and implement essentially the same algorithms over and over again, adapting them to their robot, since there was no network or system which would be able to share these commonly used algorithms in a generic way. With the modular design that ROS provides, one can easily download many tools and various implementations, adapt a few parameters if at all, and run them without much hassle. This modular design helped bring robotics research forward across the globe.\n",
      "\n",
      "#### <span id=\"page-14-2\"></span>**3.1.1 URDF**\n",
      "\n",
      "The Unified Robot Description Format(URDF)[15](#page-14-5) is a widespread commonly used XML based format, containing robot descriptions. It is also used by ROS as a standard within the ROS community. It essentially describes all the robots links and joints in relation to one another, specifying their respective limits, forces and sizes. Based on this format, the Virtual Reality PR2 model will be generated.\n",
      "\n",
      "## <span id=\"page-14-3\"></span>**3.2 Blender**\n",
      "\n",
      "Blender[16](#page-14-6) is an open-source 3D-modeling software, which provides many tools centered around 3D computer graphics. Beside 3D-model creation, it can be used for animation, rigging, skinning, visual effects generation, texturing, particle, fluid and smoke simulation, sculpting, rendering, even video editing and many more. It is a very versatile tool which can always be extended by many available plugins and python scripts. It includes a python console which allows for element inspection or direct, on-the-go scripting. It is the most commonly used open-source 3D editing tool. In this thesis, Blender was used to generate the PR2s Virtual Reality model.\n",
      "\n",
      "<span id=\"page-14-4\"></span><sup>14</sup>ROS: Robot Operating System <https://www.ros.org/> (last accessed: 06.12.2020)\n",
      "\n",
      "<span id=\"page-14-5\"></span><sup>15</sup>URDF: <http://wiki.ros.org/urdf> (last accessed: 06.12.2020)\n",
      "\n",
      "<span id=\"page-14-6\"></span><sup>16</sup>Blender: <https://www.blender.org/> (last accessed: 06.12.2020)\n",
      "\n",
      "#### <span id=\"page-15-0\"></span>**3.2.1 Skeletal Meshes**\n",
      "\n",
      "Skeletal Meshes are 3D animated objects which contain a skeleton, which allows the mesh fitted around it to be moved and animated. The intuitive idea of a human skeleton within a human body describes the core principle quite accurately. An animation skeleton consists of bones. Each bone within blender has a head and a tail, which are described by 3D-coordinates. Bones can be connected to one another, but they don't have to be. These connections, if they exist, represent essentially the joints of the skeleton, while the bone body itself can be seen as the link. Bones within skeletons are usually surrounded by one or multiple meshes. The movement of the bones, moves the meshes around them or deforms them, depending on the influence radius given to the bone. By programming bone-movement, be it via key-frames and interpolation or forward/inverse kinematics, the entire mesh can be animated and brought to life.\n",
      "\n",
      "Constraints can be applied to automate the movement of a skeletal mesh or restrict it. For example, certain motion-limits between bones can be set up, or inverse-kinematics chains can be created for pieces of the model, which allow for a more automatic and environment responsive animations.\n",
      "\n",
      "#### <span id=\"page-15-1\"></span>**3.2.2 phobos-Plugin**\n",
      "\n",
      "Phobos[17](#page-15-3) is a plugin for Blender, which has been developed by the DFKI (Deutsches Forschungszentrum fÃ¼r KÃ¼nstliche Intelligenz, Bremen)[18](#page-15-4), to assist in the creation of 3D-robot-models for simulators like Gazebo[19](#page-15-5) or Mars[20](#page-15-6). It can read in a *urdf* or a *sdf* file, and generate a model out of it.\n",
      "\n",
      "## <span id=\"page-15-2\"></span>**3.3 Unreal Engine**\n",
      "\n",
      "The Unreal Engine[21](#page-15-7) is a games development engine created by Epic Games, which contains very many versatile tools needed to create all sorts of digital games. It allows for physics based and particle based animation, basic skeletal control animation, 3D world map creation and the development of game logic. It is one of the most popular game engines at the moment and is fairly well known for its stunning graphics. Game and animation logic can be programmed with the help of Blueprints, which is a visual way of programming which consists of connecting the correct nodes to one another following an execution path. Alternatively programming can be done with C++. The Blueprint coding style is more beginner friendly, but is of course more limited than the C++ approach, which is more powerful but which also contains many engine-specific language caveats and generally benefits from experience with C++ itself. Both languages can be used for either purpose, although it is strongly encouraged to use Blueprints for animation and it is made nearly impossible to not do so. Unreal Engines functionality can be further\n",
      "\n",
      "<span id=\"page-15-3\"></span><sup>17</sup>[\\[17\\]](#page-57-5): Kai von Szadkowski and Simon Reichel. \"Phobos: A tool for creating complex robot models\". In: *Journal of Open Source Software* 5.45 (2020), p. 1326. doi: [10 . 21105 / joss . 01326](https://doi.org/10.21105/joss.01326). url: <https://doi.org/10.21105/joss.01326>\n",
      "\n",
      "<span id=\"page-15-5\"></span><span id=\"page-15-4\"></span><sup>18</sup>DFKI Bremen: <https://robotik.dfki-bremen.de/en/startpage.html> (last accessed: 06.12.2020) <sup>19</sup>Gazebo Simulator: <http://gazebosim.org/>\n",
      "\n",
      "<span id=\"page-15-6\"></span><sup>20</sup>MARS Simulator: <https://robotik.dfki-bremen.de/de/forschung/softwaretools/mars.html> (last accessed: 06.12.2020)\n",
      "\n",
      "<span id=\"page-15-7\"></span><sup>21</sup>Unreal Engine: <https://www.unrealengine.com/> (last access: 06.12.2020)\n",
      "\n",
      "increased by the use of Plug-ins, which can introduce new features to the game itself or the development process.\n",
      "\n",
      "In this thesis the Unreal Engine will be used to setup and control the Virtual Reality environment in which the virtual robot is supposed to be manipulated and controlled by the human.\n",
      "\n",
      "## <span id=\"page-16-0\"></span>**3.4 RobCog - Robot Commonsense Games**\n",
      "\n",
      "The Robot Commonsense Games (RobCoG)[22](#page-16-1) [23](#page-16-2) project is largely the baseline for this thesis in Unreal Engine. Its objective is to collect commonsense knowledge for robots, based on everyday activities human users can perform within a Virtual Reality environment. The information about the performed tasks, e.g. positions of objects interacted with, how they were interacted with etc. can be logged symbolically. Within the RobCoG environment, the virtual world with which a human user interacts is the same kitchen environment which can be found in the laboratory of the Institute of Artificial intelligence Bremen.[24](#page-16-3) The Head Mounted Display or the HTC Vive[25](#page-16-4) Headset represents the head of the human user. It has no visual representation in VR at all and is mapped to the camera which defines the field of view for the human user. The user interacts with this world via two HTC Vive VR motion controllers, which in the Virtual Reality are represented as two human hands. So if one were to look in a mirror within VR, one would only see a pair of floating hands. The hands are not attached to any sort of body and are directly mapped to the motion controllers. There are no options or features currently build in to offset the hands from the motion controller position, at least to my knowledge, since that is not desired in this case nor necessary. The hands can be open or closed. They have physics bodies setup within the Unreal Engine for them, meaning that if they collide with another body in VR, they will not go through that body, but rather be pushed away by it. This means also that it is possible to interact with the environment without needing to grasp an item. For example, by hooking the open hand behind a handle, one would still be able to pull open a drawer. Attached to the motion controllers are also two red arrows, one for each hand. These represent the location of the motion controllers independently of the physics. For example, if one pushes with the hand in VR against a wall, the hand will not go through the wall, but the motion controller in the real world will, since there is no way of preventing this from happening. To visualize this for the user, the red arrow will always follow the motion controller, even if it means going through the wall. The mesh of the hand will be left behind, until the user is back in the open space where the mesh can attach itself back to the motion controller location.\n",
      "\n",
      "For grasping physics meant also that while closing the hand by pulling the trigger of the motion controller, the fingers would collide with the object the user is grasping and therefore not pass through the object visually, providing a more realistic grasping experience. However, the fingers always perform only one grasping motion. So while it mostly looks realistic, for many objects it also looks a bit off. For example, since\n",
      "\n",
      "<span id=\"page-16-1\"></span><sup>22</sup>RobCoG: <http://robcog.org/> (last accessed: 06.12.2020)\n",
      "\n",
      "<span id=\"page-16-2\"></span><sup>23</sup>[\\[8\\]](#page-56-9): Andrei Haidu. *Robot Commonsense Games*. url: <http://www.robcog.org/games.html> (visited on 04/08/2018)\n",
      "\n",
      "<span id=\"page-16-3\"></span><sup>24</sup>IAI Bremen: <https://ai.uni-bremen.de/> (last accessed: 06.12.2020)\n",
      "\n",
      "<span id=\"page-16-4\"></span><sup>25</sup>HTC Vive: <https://www.vive.com/eu/> (last accessed: 06.12.2020)\n",
      "\n",
      "the grasping motion consists of the fingers just coming together to the middle of the palm, one cannot perform a very controlled parallel pincer-grasp. This is mostly relevant for grasping the spoon from a flat surface. Also the grasping works by attaching an object to the hands palm-area, since a sphere is defined there with which an object can overlap and then be attached to. When grasping the spoon, since this point is relatively close to the palm of the hand, one has to really push the hand against the surface on which the spoon is located, fingers outward, so that one can grasp with the palm. This is very unrealistic but then again, spoons are very small and thin objects compared to a bowl and cup. They are really currently hard to grasp in VR with this approach.\n",
      "\n",
      "## <span id=\"page-17-0\"></span>**3.5 KDL and Eigen**\n",
      "\n",
      "KDL[26](#page-17-3) is a third party library for C++ or Python, developed by the Open Robot Control Software (Orocos)[27](#page-17-4), providing forward and inverse kinematic solutions. It reduces the motion modeling problem to essentially a geometric one. It is widely used within robotics and is also implemented within CRAM[28](#page-17-5)\n",
      "\n",
      "## <span id=\"page-17-1\"></span>**3.6 USemLog - Logging Data in Virtual Reality**\n",
      "\n",
      "The USemLog[29](#page-17-6)[30](#page-17-7) plugin for Unreal Engine allows to record all actions performed within the Virtual Reality environment. Each object can be tracked, with its position, size and other properties as well as any interaction an object might have with another. Hence, if an object e.g. a bowl is standing on a surface e.g. a table, it can be recorded that they both share a *contact* event. Then once the bowl is picked up by a user, it no longer is sharing this contact event but is rather being *acted on* by a user. All this sub-symbolic and symbolic information can be recorded in .owl and .json files, which then can be used by KnowRob (explained below), to infer this information and to base a robot's decision making onto these previous experiences.\n",
      "\n",
      "## <span id=\"page-17-2\"></span>**3.7 CRAM - Cognitive Robot Abstract Machine**\n",
      "\n",
      "CRAM[31](#page-17-8) is an extensive framework for developing robotics applications. It contains reasoning mechanisms, a lightweight simulator(bullet world[\\[13\\]](#page-56-5)) and many more tools, which support the development of cognition-enabled control programs for robots. It defines high level symbolic plans for manipulation activities which can be applied to many robots. It supports already various robots and can be used for any robot which supports ROS. In previous work, the *Cram-Knowrob-VR*[32](#page-17-9) package was developed, which provides\n",
      "\n",
      "<span id=\"page-17-3\"></span><sup>26</sup>KDL: <https://orocos.org/kdl.html> (last accessed: 06.12.2020)\n",
      "\n",
      "<span id=\"page-17-4\"></span><sup>27</sup>Orocos: <https://docs.orocos.org/> (last accessed: 06.12.2020)\n",
      "\n",
      "<span id=\"page-17-5\"></span><sup>28</sup>CRAM KDL implementation: [https://github.com/cram2/cram/blob/master/cram\\\\_pr2/cram\\\\_](https://github.com/cram2/cram/blob/master/cram_pr2/cram_pr2_low_level/src/kinematics-trajectory.lisp) [pr2\\\\_low\\\\_level/src/kinematics-trajectory.lisp](https://github.com/cram2/cram/blob/master/cram_pr2/cram_pr2_low_level/src/kinematics-trajectory.lisp)\n",
      "\n",
      "<span id=\"page-17-6\"></span><sup>29</sup>USemLog: <https://github.com/robcog-iai/USemLog> (last accessed: 06.12.2020)\n",
      "\n",
      "<span id=\"page-17-7\"></span><sup>30</sup>[\\[9\\]](#page-56-10): Andrei Haidu and Michael Beetz. *Automated Models of Human Everyday Activity based on Game and Virtual Reality Technology*. (Visited on 04/01/2018)\n",
      "\n",
      "<span id=\"page-17-8\"></span><sup>31</sup>[\\[14\\]](#page-57-1): Lorenz MÃ¶senlechner. \"The Cognitive Robot Abstract Machine\". Dissertation. MÃ¼nchen: Technische UniversitÃ¤t MÃ¼nchen, 2016, <http://cram-system.org/>\n",
      "\n",
      "<span id=\"page-17-9\"></span><sup>32</sup>CRAM KVR package: [https://github.com/cram2/cram/tree/boxy-melodic/cram\\\\_knowrob/](https://github.com/cram2/cram/tree/boxy-melodic/cram_knowrob/cram_knowrob_vr) [cram\\\\_knowrob\\\\_vr](https://github.com/cram2/cram/tree/boxy-melodic/cram_knowrob/cram_knowrob_vr)\n",
      "\n",
      "KnowRob queries and generalization mechanisms for processing data collected in Virtual Reality. It also contains robot plans and evaluation tools for this type of data. In this thesis it will be used to evaluate the newly gained PR2-body-based data.\n",
      "\n",
      "## <span id=\"page-18-0\"></span>**3.8 KnowRob**\n",
      "\n",
      "KnowRob[33](#page-18-1)[\\[18\\]](#page-57-6)[\\[3\\]](#page-56-11) is a knowledge base for robots which aims to assist robots in performing everyday household activities by providing them with common knowledge. KnowRob works mainly based with prolog-queries to provide answers to a robots questions. It can contain knowledge about the current environment, e.g. where is a spoon typically located in a kitchen? What can be done with a cooking pot? How should a cooking pot be handled, once it has been on the stove? All these questions and more is common knowledge for humans but not for robots. Even more important is the generation of this knowledge. It can be provided via experiences generated by robots while performing every day tasks. Or knowledge can also be provided by learning from humans. In a Virtual Reality environment, the full state of the world is known and can be logged in its entirety, giving even more importance to this kind of data. Even though a new version of KnowRob has been released[\\[3\\]](#page-56-11), in this thesis, still an old version will be used, in the hopes of avoiding any issues later down the line while testing and evaluating the newly acquired data.\n",
      "\n",
      "<span id=\"page-18-1\"></span><sup>33</sup>KnowRob: <http://www.knowrob.org/>\n",
      "\n",
      "# <span id=\"page-19-0\"></span>**4 Approach and Implementation**\n",
      "\n",
      "## <span id=\"page-19-1\"></span>**4.1 Architecture Overview**\n",
      "\n",
      "The following section will describe the implementation work performed within this thesis. It is structured based on the main tools used. First it will be described how Blender and phobos were used to generate a skeletal mesh of the PR2 robot. After this, the Unreal Engine will be introduced and how some basic functions of the PR2 robot were implemented. Then the various implementations of inverse kinematics nodes of Unreal Engine will be discussed, since they have been tested and experimented with in quite some depth. Also the resulting inverse kinematics solution will be presented. Then it only remains to describe which other features got implemented within Unreal Engine to help and get the virtual robot to move as realistic as possible and how the differences between the human and robot bodies were addressed. The last subsection will then discuss how the data can be recorded within Virtual Reality and what needed to be done to achive that.\n",
      "\n",
      "## <span id=\"page-19-2\"></span>**4.2 Implementation**\n",
      "\n",
      "### <span id=\"page-19-3\"></span>**4.2.1 Blender and Phobos: generating a skeletal mesh from a URDF file**\n",
      "\n",
      "The following section will provide an overview of the implementation, changes and settings to the Phobos-Plugin and Blender, which were necessary in order to generate a rigged skeletal mesh model of the PR2-Robot based on the URDF file. It will also include details about the import of the URDF file, as well as the resulting FBX export and the settings necessary for the Unreal Engine import.\n",
      "\n",
      "### <span id=\"page-19-4\"></span>**4.2.1.1 preparation of the URDF-File**\n",
      "\n",
      "The URDF-Format has clear rules on how the file should be set up in order to describe the robot properly, but there does not exist a standard which would enforce the use of the same XML-Tags throughout the file. This results into different Tags being used for the same things within one file.\n",
      "\n",
      "The Phobos plugin however, does not support all of these different tags, which leads to errors during the import process of the URDF-file. In order to prevent these errors, the problematic tags have to be removed or replaced with parse-able ones. One example of such parsing errors if the references ot the meshes within the URDF files. These references use the ROS notation, meaning they refer to the file path based on the package. However, phobos and Blender do not have a native ROS integration, so either one needs to install a plugin to reference all these paths accordingly, or one can search-replace the *package://* prefix with the according adress. The easiest way to achieve this was to use the search-replace function of any editor and to try to import the file into Blender via Phobos a few times, to see which tags could not be parsed.\n",
      "\n",
      "The following table provides an overview of the necessary changes:\n",
      "\n",
      "The *<gazebo>* references needed to me commented out completely, since otherwise phobos would throw errors on import.\n",
      "\n",
      "| original tag                      | phobos required change           |\n",
      "|-----------------------------------|----------------------------------|\n",
      "| package://                        | /home/blender/Desktop/pr2_common |\n",
      "| <parent>                          | <parent link =                   |\n",
      "| </parent>                         | />                               |\n",
      "| <child>                           | <child link =                    |\n",
      "| </child>                          | />                               |\n",
      "| <axis> <xyz> 0 1 0 </xyz> </axis> | <axis xyz=\"0 1 0\"/>              |\n",
      "\n",
      "<span id=\"page-20-2\"></span>Table 1: *Summary of the URDF adaptation which needed to be performed in order for phobos to parse the URDF correctly.*\n",
      "\n",
      "After all affected tags were replaced or removed, it was possible to successfully import the file into Blender.\n",
      "\n",
      "The original PR2 description file which was used can be found here: [https://github.](https://github.com/code-iai/iai_pr2/blob/master/iai_pr2_description/robots/pr2_calibrated_with_ft2.xml) [com/code-iai/iai\\\\_pr2/blob/master/iai\\\\_pr2\\\\_description/robots/pr2\\\\_calibrated](https://github.com/code-iai/iai_pr2/blob/master/iai_pr2_description/robots/pr2_calibrated_with_ft2.xml)\\_ [with\\\\_ft2.xml](https://github.com/code-iai/iai_pr2/blob/master/iai_pr2_description/robots/pr2_calibrated_with_ft2.xml). The resulting URDF file, after performing all the adaptations can be found here: [https://github.com/hawkina/phobos/blob/rigging/models/iai\\\\_pr2\\\\_](https://github.com/hawkina/phobos/blob/rigging/models/iai_pr2_inorder.urdf) [inorder.urdf](https://github.com/hawkina/phobos/blob/rigging/models/iai_pr2_inorder.urdf)\n",
      "\n",
      "#### <span id=\"page-20-0\"></span>**4.2.1.2 Skeletal Mesh generation with phobos**\n",
      "\n",
      "After importing the URDF file into Blender via phobos, the result looks like the PR2 robot but it is unfortunately build wrong. It is not one coherent skeletal mesh. Instead, every link is its own object, containing a mesh, an armature and a bone. A coherent and usable skeletal mesh however, would have only one armature, containing all the bones with the same parent-child relationships as described in the URDF file. Since this is not the case from the get go, adaptation was needed.\n",
      "\n",
      "The fastest way to do so seemed to start building the necessary adaptation on top of the phobos plugin, since it already imports the URDF and converts it into a python collection, one might as well use this collection as a starting point. Before a skeleton can be created, it must be understood how skeletal bones work within Blender:\n",
      "\n",
      "![](_page_20_Figure_8.jpeg)\n",
      "\n",
      "<span id=\"page-20-1\"></span>Figure 1: *Schematic drawing of a Blender bone.*\n",
      "\n",
      "Every bone has a head which represents its point of origin, as well as a bone tail, which represents its end. Bones can be connected to one another but they don't have to. Bones are organized in a hierarchy, via parent-child relationships. A bones child is always connected by its head to the parents tail. Ever bone must have a parent, but not every bone has a child. A childless bone is regarded as a leaf bone. The resulting structure of bones is called an armature or outside of Blender, a skeleton. The armature object within Blender, contains a set of bones which describe the origin or reference position of each bone. The armature has a second set of bones, identical to the first one, which are the pose-able bones. These are used for animation and to define constraints. The armature object also contains all the meshes associated with the bones. In other scenarios, usually the entire body consists of one mesh and the bones therefore define the areas in which the body or in this case the mesh can be deformed, depending on the radius of influence and the given weight of the bone. The connections between the bones represent the joints, just like within a human skeleton. Since the PR2 robot consists of many meshes, and each mesh is associated to a bone, the influence are of the bones can be disregarded, since a bone will always influence just the associated mesh.\n",
      "\n",
      "Within the scripts of phobos, the main function that was adapted mostly is createLink within the links.py file. As the name suggests, it is responsible for generating links, which in this case are the bones we wish to create. When this function is called for the first time, it creates an empty scene object, called pr2\\_empty, since it should only be the reference pose. Then the armature object gets created as well as the root bone, which in the case of the PR2 is the base\\_footprint. Since it is the very first bone and since two positions are needed for each bone and the root bone does not have a parent, it gets a fixed length of 0*.*001, by getting a pose for the tail of *x* = 0*, y* = 0*, z* = 0*.*001. If no length would be given and the bones head ends up exactly where the tail is, the bone would disappear again since Blender does not allow bones of length 0. The entire collection is being iterated over, creating a bone for every child based on the position of the parent. Every bone that would have had the length 0, gets a very tiny length assigned to it. The reason is that the PR2 has a few bones in the same place. For example, the *shoulder\\_lift\\_link*, *upper\\_arm\\_roll\\_link* and the *upper\\_arm\\_link* all have an origin of 0 and would all result in being non-existant if that rule would not be applied. But since most of the PR2s joints are revolute and move only along one axis, the idea to map them each to one bone seems close. Since the poses are always given relative to the parent, the transforms need to be calculated accordingly.\n",
      "\n",
      "After all the bones are generated, the meshes are loaded and mapped to the bones. An armature modifier is applied to every mesh, as well as a vertex group needs to be created, for the bone to be properly mapped to the skeleton. A rather PR2 specific rule, is that the meshes for the fingers initially appear at the wrong place. So for example, for the right gripper both fingers will appear where the right finger is supposed to be, one of them will have a little offset. This is due to the fact that it is the same mesh is being used for both of them, so to fix this it needs to be rotated.\n",
      "\n",
      "## <span id=\"page-22-0\"></span>**4.2.1.3 Export of the FBX Model from Blender to Unreal Engine**\n",
      "\n",
      "Exporting and FBX from Blender to Unreal Engine is a project of its own. Both systems treat FBX files slightly different and finding the correct settings takes some time. The settings which worked in the end are the following:\n",
      "\n",
      "| FBX 7.4 binary<br>Version:<br>Main<br>Geometri Armature Animation<br>Selected Objects<br>< Scale:<br>1.00<br>All Local<br>Apply Scal<br>Forward:<br>-Z Forward<br>Up:<br>YUp<br>Empty Camer Lamp Armat   Mesh   Other<br>!EXPERIMENTAL! Apply Transform | Version:<br>FBX 7.4 binary<br>Main<br>Geometri Armature Animation<br>Apply Modifiers<br>Use Modifiers Render Setting<br>Smoothing:<br>Normals Only<br>Loose Edges<br>Tangent Space | Version:<br>FBX 7.4 binary<br>Main<br>Geometri Armature Animation<br>Only Deform Bones<br>Add Leaf Bones<br>Primary B<br>Y Axis<br>Secondar<br>X Axis<br>Armature<br>Null | Version:<br>FBX 7.4 binary<br>Geometri Armature Animation<br>Main<br>Baked Animation<br>Key All Bones<br>><br>NLA Strips<br>All Actions<br>Force Start/End Keying<br>Sampling Rate:<br>1.00<br>Simplify:<br>1.00 ) |\n",
      "|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| Custom Properties<br>Path Mo<br>9<br>Copy<br>Ã· é«˜<br>Batch M Off                                                                                                                                                                                         |                                                                                                                                                                                    |                                                                                                                                                                           |                                                                                                                                                                                                                    |\n",
      "\n",
      "<span id=\"page-22-1\"></span>Figure 2: *Blender FBX export settings.*\n",
      "\n",
      "Also, since both programs treat the files rather differently there were a few other issues. In the beginning, the export failed because all the meshes had the same names as the bones. They were both referred to as e.g. *base\\_link* etc. Within Blender that was not an issue, since there is a differentiation between the meshes and the bones within the armature. They are different objects of different classes. However Unreal sees meshes as bones and adds them as such to the Skeletal tree. There is a setting within the FBX import of Unreal Engine which supposedly avoids this but it this case it did nothing. Therefore, for Unreal, there were multiple objects with the same name, which of course could not be imported properly.\n",
      "\n",
      "After iterating over all the meshes and prefixing them simply with the term *mesh*, this problem was solved. However the skeleton still did not behave correctly. It seemed like the bones which are supposed to control a specific mesh have an offset to one another. Basically the wrong bone is controlling the wrong mesh. After some research, it was found that the Unreal Engine has a very different understanding of bones compared to Blender. In Blender, every bone has a head and tail, therefore two positions which define it. In Unreal Engine, only one position describes the bone. The connection seen between to bones, which looks very similar to the Blender bone body, is simply a visualization of where the parent of the current bone is. The mismatch and weird movement of the bones occurred, because Unreal Engine places its bone on the blenders tail of a bone. The tail was first defined as the position of the next child, and the head being the important origin of a bone, but for Unreal this all needed to be shifted. This results in the skeleton within Blender to look a bit disconnected, but works fine within Unreal Engine.\n",
      "\n",
      "![](_page_23_Figure_1.jpeg)\n",
      "\n",
      "<span id=\"page-23-0\"></span>Figure 3: *PR2 skeleton after having all the necessary settings and modifications applied for the Unreal Engine export.*\n",
      "\n",
      "Another important setting is the *path mode: copy* setting. This copies all textures of the meshes into a folder which will be exported with the FBX. If this is not done, the textures will end up missing in the resulting object within the Unreal Engine.\n",
      "\n",
      "As far as bone limits and constraints go: they can be applied to the bones fairly easily within Blender. Even IK chains can be setup. This can all be done with the script also, based on the URDF. However, either Blender does not export these constraints properly or they are generally not supported by FBX or Unreal Engine simply ignores them. This is a bit unclear but during my research I had found several forum posts suggesting that this is unfortunately, impossible. It might be related to Unreal Engine not having a definition of general skeletal mesh constraints, outside of the physics asset or animation blueprint. So unfortunately, the code to generate these constraints within Blender exists, but has been commented out again since it has no effect on the model within Unreal Engine.\n",
      "\n",
      "### <span id=\"page-24-0\"></span>**4.2.2 Unreal Engine: Preparing the Robots model for VR**\n",
      "\n",
      "The following section will describe in more detail what exactly was done to get the robot to move within the virtual reality. How the robots body was mapped to the humans, which difficulties arose and how they were solved. The section is split into different aspects of this process, from the import of the FBX model to inverse kinematics used to move the robot and other features implemented. In the beginning of the development and for many of the features, Blueprints were used. This is due to the fact that they are intuitive to use and allow for very fast prototyping of functionality. For getting to know how the Unreal Engine works in the beginning suing them was totally fine. For the more complicated things later on in the development, C++ was used since the limits of what the Blueprints can do were hit. It also seems like the support for C++ code for general in-game logic is a lot more present than for animation. It seems like Unreal almost forces the use of Blueprints for Animation purposes.\n",
      "\n",
      "#### <span id=\"page-24-1\"></span>**4.2.2.1 Import of the FBX Model**\n",
      "\n",
      "For the import of the FBX, no setting changes need to be done, except setting the tick at *skeletal mesh* in the Unreal Engine import window, and using the import all button. Even though the FBX skeleton might have looked a bit off in Blender, within Unreal Engine it produced the expected result:\n",
      "\n",
      "<span id=\"page-24-2\"></span>![](_page_24_Picture_5.jpeg)\n",
      "\n",
      "Figure 4: *The imported PR2 skeletal mesh within the Unreal Engine. The tiny spheres are essentially the bones, the long connections between them simply visualize their parent relation between one another.*\n",
      "\n",
      "It might take a few minutes for the textures to render and be fully displayed, but the above depiction should be the result. Auto-generated at import as individual files are the skeletal mesh, the skeleton and a physics asset which contains some collision capsules, but not for all the bones. The skeleton really only contains the bones and can be used to inspect how moving a particular bone would influence the skeleton, while the skeletal mesh component is rather focused on the 3D-mesh aspect of the skeleton. So while having similar names, these components do rather different things.\n",
      "\n",
      "#### <span id=\"page-25-0\"></span>**4.2.2.2 Base movement with and without VR**\n",
      "\n",
      "To be able to move the skeleton within the game, a BP\\_PR2\\_Character was created, which is a character blueprint representing the PR2 within the name. It is linked to the imported mesh of the PR2 and is pretty much responsible for how the character interacts with the Virtual Reality environment. It takes care of essentially everything except the skeletal animation, which is being computed within the *PR2\\_Anim\\_BP*, which is an animation Blueprint associated with the character.\n",
      "\n",
      "One of the very first things that got implemented was the characters movement based on button-press on the keyboard. Even though later on the movement got taken over by VR entirely, for the beginning phase of the project it was very useful to be able to move the PR2 with the keyboard. Within the projects input settings, the w,a,s,d,q,e-keys were mapped to input axis of the Engine. E.g. whenever the button w would get pressed, a value of 0*,* 5 gets generated and sent to the according input axis event. Whenever this happens, the forward vector is being obtained based on the position of the PR2 mesh, or later on, the VRCamera object. This vector as well as the value of the axis input is then being passed on to the Add Movement Input node, to generate the according movement. For general testing with the standard view port of the engine this is completely fine. While when using VR and the VR Previw view port, using this movement can cause motion sickness.\n",
      "\n",
      "![](_page_25_Figure_5.jpeg)\n",
      "\n",
      "<span id=\"page-25-1\"></span>Figure 5: *part of the BP\\_PR2\\_Character Blueprint, showing a part of the implementation of key-press based navigation/movement.*\n",
      "\n",
      "In order to include the HTC Vive headset and motion controllers into the movement capabilities, a few more objects needed to be created and attached to the BP\\_PR2\\_Character. First, a scene component was added. Scene components are similar to the empty objects in Blender. They are essentially objects which do not have any mesh or other visual component, they are just a point or a coordinate system, that can be placed in space in order to reference a transform in the virtual space. They can also be used to set up parent-child dependencies between the objects. One of these components is called VR\\_Origin and represents the origin point of the VR setup, as the name suggests. It has a position of *x* = 0*, y* = 0*, z* = 0 relative to the PR2 mesh, which is its parent. It has three children, two motion controllers, MC\\_Right and MC\\_Left and the VRCamera, which is the camera component that gets possessed by the player, or rather the HTC Vive headset and generates the field of view that the human user perceives.\n",
      "\n",
      "The movement mapping of the headset to the PR2 character might be set up a bit unconventional. It just developed into what it currently is based on testing and based on how some solutions were found and developed over time. If set up completely from scratch, it might result in a completely different solution. But this is currently what is used:\n",
      "\n",
      "![](_page_26_Figure_3.jpeg)\n",
      "\n",
      "<span id=\"page-26-0\"></span>Figure 6: *Navigation based on the position of the Head Mounted Display.*\n",
      "\n",
      "The position of the Head Mounted Display (HMD) is being obtained in device space. The *X, Y* components are being extracted, since the *Z* component which represents the height of the headset does not matter for navigation, it is being ignored and replaced with âˆ’90. This value is important, because the BP\\_PR2\\_Character has a capsule as its root component, and the skeletal mesh if not given an offset, would be located in the middle of that capsule. So in order for the PR2 to not float in the middle of the capsule, he has this *Zoffset*, which of course needs to be maintained throughout navigational position changes.\n",
      "\n",
      "To correctly rotate the PR2, was a whole other issue. The rotational *Z* component can be easily obtained, but if it is applied directly to the mesh or the root component, it would cause interesting side effects. Either the robot would rotate but not around the *Z* axis of itself, but some other point in space, which results in the robot driving around in a circle like a car instead of rotating in place. This would also cause the VRCamera not to follow but to just stand in place, leaving the PR2s body and watch him rotate, until after 360â—¦ he would be at the original position again. Or, the mesh would rotate correctly around its own axis but the motion controllers would fly away on rotation, since they would rotate around the same origin point the PR2 initially wanted to rotate around\n",
      "\n",
      "also. So the solution to this problem was to save the *Z* rotational value in a variable, pass it to the PR2\\_Anim\\_BP where it is used within the Animation Graph to rotate the PR2s *pr2\\_empty* bone, which is the origin of the mesh.\n",
      "\n",
      "As previously mentioned, this solution probably came to be due to an error in the initial setup and how objects are inheriting from one another, but it works.\n",
      "\n",
      "#### <span id=\"page-28-0\"></span>**4.2.3 Skeletal Mesh Animation using Inverse Kinematics**\n",
      "\n",
      "In order for the robots body to be able move its arms, the original idea was to use the inverse kinematics nodes Unreal Engine provides. It would be possible to define an inverse kinematics chain, describing the arm of the robot, e.g. from the *shoulder\\_pan\\_link* to the *gripper\\_palm\\_link*, setting the gripper\\_palm\\_link as the end-effector, and mapping its pose to a VR-motion-controller. The position of the base of the robot would be mapped to the VR-headset, ignoring the z component of the pose since the robot should be positioned on the floor and not mid-air, as is also described in the previous section.\n",
      "\n",
      "All the Unreal Engine skeletal control nodes can be found within the animation blueprint, which is associated to a skeletal mesh, which in this case is the skeletal mesh of the PR2 robot. This means that the inverse kinematics is animation based only.\n",
      "\n",
      "Unfortunately, this did prove to be a lot more labor intensive and harder than it sounds. In games development, the focus of the animation is to look good. In our case, the focus was not only to make the animation look a certain way, but it had to behave a certain way. It needs to mimic the robots capabilities as closely as possible, and this involves moving the arms in a way the real robot would be able to do too. This means that each joint has to be restricted in the same way as it would be on the real robot, or at least as close to the real robot as possible. For the PR2 robot this means that the joints have to be limited on a per-axis basis, since each joint has a certain range of motion on one axis only, since most of the joints are revolute, and others are continuous. Without these restrictions, one would be able to move the arms of the robot within VR in ways which would look broken, dislocated and which would be impossible for the real robot to achieve. Since the goal of this thesis is to try and collect better data for the robot to use as a baseline to perform a task, and to limit the human user to the capabilities the robot provides, these limits are crucial. A little difference between the real robot and the animated version within VR can be expected, but the closer the Unreal-Robot is to the real one, the better and more useful the collected data will be.\n",
      "\n",
      "Before we dive into detail of all the different approaches which were tried, here is a tabular overview of them, which nodes they involved and what their main result or issue was, hence why they failed to become the end solution. It mainly boils down to being able to set bone or joint constraints on a per axis basis, to replicate the PR2s movement. It will also mention a few IK nodes which will not be explained further into detail in the upcoming section, where the sentence within the table is enough to explain why they could not be used.\n",
      "\n",
      "| Used<br>node<br>combina<br>tion                                              | Result                                                                                                                                                                                                                                      |  |  |  |\n",
      "|------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|--|\n",
      "| CCDIK                                                                        | IK with only symmetric rotational limits per joint. Can<br>not apply limit per-axis.                                                                                                                                                        |  |  |  |\n",
      "| CCDIK + ApplyLimits                                                          | Limits are applied after the IK calculation, shifting the<br>end-effector away from the goal.                                                                                                                                               |  |  |  |\n",
      "| FABRIK                                                                       | No option to set limits at all.                                                                                                                                                                                                             |  |  |  |\n",
      "| FABRIK + ApplyLimits                                                         | Limits are applied after the IK calculation, shifting the<br>end-effector away from the goal.                                                                                                                                               |  |  |  |\n",
      "| Two Bone IK                                                                  | PR2's arm has more than two bones in a chain.                                                                                                                                                                                               |  |  |  |\n",
      "| Leg IK                                                                       | Always takes the floor into consideration, leading to very<br>weird configurations. Allows limits, but it's not enough.                                                                                                                     |  |  |  |\n",
      "| CCDIK + ApplyLimits +<br>CCDIK + ApplyLimits                                 | Recalculate IK after moving it into limits. This can be<br>repeated several times. Improves the result but does not<br>reach goal.                                                                                                          |  |  |  |\n",
      "| FABRIK + ApplyLimits<br>+ CCDIK + ApplyLimits                                | Similar to previous, although slightly better, closer to<br>the goal.                                                                                                                                                                       |  |  |  |\n",
      "| CCDIK + ApplyLimits +<br>elbow goal                                          | Split the IK chain in half, at the elbow. Calculate upper<br>arm and lower arm IK separately. Generates nice elbow<br>behavior, but the limits do not act correctly.                                                                        |  |  |  |\n",
      "| CCDIK + Physics Asset                                                        | Generate a physics asset for the PR2, including setting<br>up all joint constraints within physics.<br>Looks good in<br>physics asset simulation, behaves strangely/breaks in the<br>game world on play. Also CCDIK ignores physics limits. |  |  |  |\n",
      "| FABRIK + Physics Asset                                                       | Similar to the above. Also ignores physics constraints.                                                                                                                                                                                     |  |  |  |\n",
      "| CCDIK<br>or<br>FABRIK<br>+<br>Physics Asset + Anima<br>tion/Physics Blending | Does not reach goal.<br>Similar problem to ApplyLimits<br>that CCDIK and FABRIK bot ignore the physics asset<br>constraints                                                                                                                 |  |  |  |\n",
      "| Virtual Bones + any IK                                                       | Summarize upper arm and lower arm bones into one<br>virtual bone each, have only the elbow joint defined. IK<br>ignores virtual bones and does not see them as a chain,<br>even if they are chained.                                        |  |  |  |\n",
      "| KDL based PR2 IK Node                                                        | Takes limits into account, does not always find a valid<br>configuration but works.                                                                                                                                                         |  |  |  |\n",
      "\n",
      "<span id=\"page-29-0\"></span>Table 2: *Overview of all the IK approaches tried within this thesis.*\n",
      "\n",
      "## <span id=\"page-30-0\"></span>**4.2.3.1 CCDIK**\n",
      "\n",
      "The core idea of using an inverse kinematics node is also largely based on the CCDIK (Cyclic Coordinate Descent Inverse Kinematics) node, which is a newly added experimental feature in Unreal Engine, and which on the first glance provides a way to set limits.\n",
      "\n",
      "| 4 Warning                                    |                                |  |  |  |  |\n",
      "|----------------------------------------------|--------------------------------|--|--|--|--|\n",
      "| Uses experimental class: AnimGraphNode_CCDIK |                                |  |  |  |  |\n",
      "| A Effector                                   |                                |  |  |  |  |\n",
      "| D Effector Location<br>)                     | Y 0,0<br>2 2 0,0<br>\"<br>Ã— 0,0 |  |  |  |  |\n",
      "| Effector Location Space                      | Component Space â–¼              |  |  |  |  |\n",
      "| Effector Target                              | None                           |  |  |  |  |\n",
      "| A Solver                                     |                                |  |  |  |  |\n",
      "| Tip Bone                                     | 5<br>r_gripper_palm_li â–¼       |  |  |  |  |\n",
      "| Root Bone                                    | 5<br>r_shoulder_pan_li â–¼       |  |  |  |  |\n",
      "| Precision                                    | 1,0                            |  |  |  |  |\n",
      "| Max Iterations                               | 10<br>\"                        |  |  |  |  |\n",
      "| Start from Tail                              | >                              |  |  |  |  |\n",
      "| Enable Rotation Limit                        | ><br>a                         |  |  |  |  |\n",
      "| Rotation Limit Per Joints                    | 7 Array elements               |  |  |  |  |\n",
      "| 0                                            | 30,0<br>\"<br>P                 |  |  |  |  |\n",
      "| 1                                            | 30,0<br>\"<br>5                 |  |  |  |  |\n",
      "| 2                                            | \"<br>9<br>30,0                 |  |  |  |  |\n",
      "| 3                                            | 30.0<br>\"<br>9                 |  |  |  |  |\n",
      "| 4                                            | 9<br>30,0                      |  |  |  |  |\n",
      "| 5                                            | 0<br>30,0                      |  |  |  |  |\n",
      "| 6                                            | 30,0<br>P                      |  |  |  |  |\n",
      "\n",
      "<span id=\"page-30-1\"></span>Figure 7: *Settings options for the CCDIK Animation Graph Node*\n",
      "\n",
      "However, after some research and trial and error, the limits set within the node are all meant to be symmetrical, and not on a per-axis basis. Meaning that this limit just limits the motion across all axes, and not a single one, like would be needed. This leads to impossible configurations, which look broken and wrong, but the goal position can be reached.\n",
      "\n",
      "<span id=\"page-30-2\"></span>![](_page_30_Picture_6.jpeg)\n",
      "\n",
      "Figure 8: *Some of the disjointed states of the PR2 arm, while using the CCDIK node*\n",
      "\n",
      "## <span id=\"page-31-0\"></span>**4.2.3.2 CCDIK + Apply Limits**\n",
      "\n",
      "In order to fix this and try to apply some realistic PR2 limits, the idea was to use the ApplyLimits node, which allows to set max and min limits between -180 and +180 degrees, on an per-axis basis. Placing this node after the CCDIK node however, results in the application of these limits after the CCDIK calculations, which moved the arm away from it's goal. Placing this node before the kinematics node has no effect, since the kinematics calculations will not take the limits into account at all. Another closely related idea to this approach was to chain multiple **CCDIK** and **ApplyLimits** nodes together, in the hope that for the second ik node, the input position will be one which was moved closer to the goal already by the previous ik node and had the limits applied to it, so maybe it could move it closer to the goal again and the limits can be applied again to, in a way, try and force it to get closer and closer to the goal. While this slightly improved the result, it didn't solve the issue that with these limits, the goal position could not be reached.\n",
      "\n",
      "![](_page_31_Picture_3.jpeg)\n",
      "\n",
      "Figure 9: *By rotating the shoulder\\_pan\\_link away from the goal (green circle) and allowing the upper\\_arm\\_link to rotate at all and bend the elbow, the goal position could be reached. However, CCDIK and the ApplyLimits node do not work well together and cannot reach the target goal.*\n",
      "\n",
      "<span id=\"page-31-2\"></span>Also some of the limit application yielded weird results. For example, the *upper\\_arm\\_link* is able to rotate beyond 180 degrees in one direction. This is impossible to be set with the settings. Adding an offset did not seem to shift it and it seemed to actually block the movement if it reached 180 degrees and not allow it to rotate further, even if it should be possible with the given offset or setting the limits to -180 and +180 degrees. There also seems to be a known bug with this node, that the min and max values for X and Z axis seem to be swapped.[34](#page-31-1) This meant that for the *shoul-*\n",
      "\n",
      "<span id=\"page-31-1\"></span><sup>34</sup>ApplyLimits-Node bug report: <https://issues.unrealengine.com/issue/UE-66293> (last accessed: 30.11.2020)\n",
      "\n",
      "*der\\_pan\\_link*, which movement should be only around the Z axis, it needed to be set in the settings as around the X axis. This just led to overall confusion when setting the limits.\n",
      "\n",
      "#### <span id=\"page-32-0\"></span>**4.2.3.3 FABRIK**\n",
      "\n",
      "Another inverse kinematics node that could be used is the FABRIK node, which is an implementation of the FABRIK algorithm[35](#page-32-3) which in the original version of the algorithm does not support joint limits, but in an extension[36](#page-32-4) developed a few years later, does. However the Unreal Engine node of this algorithm does not have the ability to set any limits at all, but the idea was to see if it maybe interacts better with the ApplyLimits node. Unfortunately, it did not. It reached the goal in a more straight-forward fashion than CCDIK did, but it distorted the arm even more away from the goal once the limits were applied. Applying limits seemed to work the same way with FABRIK as it did with CCDIK, meaning that they got ignored completely. After some research it was found that generally, the FABRIK algorithm itself supports axis-based limits, but that this feature of the algorithm just simply haven't been implemented yet.[37](#page-32-5)\n",
      "\n",
      "#### <span id=\"page-32-1\"></span>**4.2.3.4 Other IK Nodes**\n",
      "\n",
      "Unreal Engine has a few more inverse kinematics nodes, for example Two-Bone-IK, Leg-IK and Spline-IK, but most of them just consider very small IK-chains of just two links and a joint (hence the name Two-Bone-IK), or have very specific uses. For example, the Leg-IK node allows to specify limits, but always considers and aims towards the floor, which defeats the purpose of using it for an arm. Trying to use the Two-Bone-IK while creating Virtual Bones which essentially allow to divide the arm into two parts, the upper and lower part with the elbow as a joint, didn't work that well either.\n",
      "\n",
      "#### <span id=\"page-32-2\"></span>**4.2.3.5 Split CCDIK and ApplyLimits to Parts of the Arm**\n",
      "\n",
      "So if one very long chain is a problem and the joint-limits cannot be set properly, it was tried to split up the chain into two parts for the IK-solvers. This means, that instead of defining the arm in one node, it will be defined with two nodes. The first going from *shoulder\\_pan\\_link* to *elbow\\_flex\\_link*, and the other would go from *elbow\\_flex\\_link* to the *gripper\\_palm\\_link*. This approach also means that two goal positions would be needed: one for the end effector, *gripper\\_palm\\_link* and one for the elbow *elbow\\_flex\\_link*. In order to know where to place the *elbow\\_flex\\_link*, a pose was calculated based on the motion controllers current position. Basically, the forward vector of the motion controller is obtained so that it is known where the controller is pointing to, and based on this the\n",
      "\n",
      "<span id=\"page-32-3\"></span><sup>35</sup>[\\[2\\]](#page-56-12): Andreas Aristidou and Joan Lasenby. \"FABRIK: A fast, iterative solver for the Inverse Kinematics problem\". In: *Graph. Models* 73.5 (Sept. 2011), pp. 243â€“260. issn: 1524-0703. doi: [10.1016/j.gmod.](https://doi.org/10.1016/j.gmod.2011.05.003) [2011.05.003](https://doi.org/10.1016/j.gmod.2011.05.003). url: <http://dx.doi.org/10.1016/j.gmod.2011.05.003>\n",
      "\n",
      "<span id=\"page-32-4\"></span><sup>36</sup>[\\[1\\]](#page-56-13): Andreas Aristidou, Yiorgos Chrysanthou, and Joan Lasenby. \"Extending FABRIK with Model Constraints\". In: *Comput. Animat. Virtual Worlds* 27.1 (Jan. 2016), pp. 35â€“57. issn: 1546-4261. doi: [10.1002/cav.1630](https://doi.org/10.1002/cav.1630). url: <https://doi.org/10.1002/cav.1630>\n",
      "\n",
      "<span id=\"page-32-5\"></span><sup>37</sup>Angle Contraints for FABRIK Node:\n",
      "\n",
      "[https://forums.unrealengine.com/development-discussion/blueprint-visual-scripting/](https://forums.unrealengine.com/development-discussion/blueprint-visual-scripting/40344-fabrik-node-doesnt-respect-angle-constraints) [40344-fabrik-node-doesnt-respect-angle-constraints](https://forums.unrealengine.com/development-discussion/blueprint-visual-scripting/40344-fabrik-node-doesnt-respect-angle-constraints) (last accessed: 30.11.2020)\n",
      "\n",
      "motion controller pose is offset backwards by an amount which was estimated with trial and error.\n",
      "\n",
      "![](_page_33_Picture_2.jpeg)\n",
      "\n",
      "Figure 10: *It can be seen in the left image, that this approach provides a better elbow movement, and reaches the goal in some configurations. However as can be seen in the right image, just a little rotation can shift the entire result by a very large margin.*\n",
      "\n",
      "<span id=\"page-33-0\"></span>This would allow for a nice behavior of the elbow, since now it was actually trying to bend outwards away from the body while the palm link would try to go inwards. The ApplyLimits node could me monitored here a lot easier also, since it was responsible for just a few bones instead of the entire chain, which allowed for more easier debugging. Unfortunately, this does not solve the limit problems entirely either but allowed for a more natural looking animation, at least in regards to the elbows behavior. For some other bones, as for example the *upper\\_arm\\_link*, which had issues with the ApplyLimits node as described above, a manual offset can be added using the Transform-Modify-Bone node, which allows to completely freely modify bones.\n",
      "\n",
      "![](_page_33_Figure_5.jpeg)\n",
      "\n",
      "<span id=\"page-33-1\"></span>Figure 11: *The nodes setup necessary to achieve this motion. It does not need to be viewed in detail it should just visualize how many different nodes are needed to get this sort of interaction, and that even then it is not yet perfect.*\n",
      "\n",
      "This approach, while the most promising, was very setup-time intensive, contained a lot of calculations to try and calculate parts of it manually, was rather not reliable and really hard to maintain. If no other approach would have been found, as it will be described below, this might have been the end solution. It would have not been perfect, but it would be acceptable, even if some movements would cause the arms to look crazy and disjointed.\n",
      "\n",
      "### <span id=\"page-34-0\"></span>**4.2.3.6 CCDIK with Physics**\n",
      "\n",
      "While looking for a way to constrain the Unreal Engine IK somehow, it was discovered that joint limits for a skeleton can be set within the physics asset of the skeletal mesh. Each skeleton imported into Unreal Engine, by default, will generate a physics asset which is used to compute collisions, forces and other physics-based events between objects. In order to make collision detection more efficient, primitive shapes like spheres, cubes or capsules are used. Usually a capsule would be created around a bone and the mesh. Then constraints can be set between the shapes. There are no limits as to how many constraints can be set for an individual shape or bone. The auto-generated asset only contained three capsules, one for the torso and one for each arm. This of course, would not be enough. So a capsule for each bone in the arm was created with the according constrains.\n",
      "\n",
      "![](_page_34_Picture_3.jpeg)\n",
      "\n",
      "Figure 12: *On the left: the inactive physics asset with all the currently setup capsules defining the constraints. On the right: the physics asset during simulation. The right arm is being positioned by a mouse click.*\n",
      "\n",
      "<span id=\"page-34-1\"></span>Each constraint can limit a joint for translation or rotation alongside an axis. The rotational constraints however are not referred to as XYZ, but rather *Swing 1 Motion, Swing 2 Motion* and *Twist Motion*. *Swing 1* and *Swing 2* each allow to constrain the angle of motion around the XY and XZ pane, which are supposed to be the symmetric angles of a cone, and *Twist* is limiting the symmetric angle of roll along the X-axis. While these limits are still not quite what is needed, since they are symmetric and the PR2's range of motion of the individual joints is not, they were looking quite promising within the physics-asset simulation. The PR2 could essentially be used as a rag-doll there. However, unfortunately the physics did not interact well with the rest of the environment and with the animation blending. Animation blending in this case means that it is possible to set a value alpha, between 0 and 1, which would describe the strength of the two inputs against each other. Meaning, that if the physics blending is set to 1, everything within the Animation Blueprint will be ignored and only physics will be used to move the robot and vice versa. Setting it to 0,5 allowed for a good mix between the two, but often times it seemed like the IK within the Animation Blueprint, was not strong enough to move the arms of the Robot to where they were supposed to be. Motors could be added within the Physics Asset to drive the animation, but the individual values to drive these motors cannot be mapped to the IK result, since the IK result is an Animation Pose and does\n",
      "\n",
      "not provide access to the individually computed values for the individual joints. This also caused occasionally very weird effects at run-time. With some settings, the robot would float in the middle of the room or fly away out of it, hectically move the joints and fall apart entirely, fall through the floor or have one arm fly off. This also didn't tackle the essential problem of not being able to apply the limits to the IK solver, so that they could be respected during computation of the new pose. It was only another way to apply limits after the computation, and as seen before with the ApplyLimits node, this will just shift the end effector out of its goal position. Maybe there is a way to get it to run with the help of the physics asset, and I just lack the experience to do so, but it seemed to simply introduce more problems than providing a good solution.\n",
      "\n",
      "### <span id=\"page-36-0\"></span>**4.2.3.7 KDL**\n",
      "\n",
      "After all these attempts which yielded rather unsatisfying results, it was clear that Unreal Engine at the time of writing this, does not have a good and free inverse kinematics solver which would respect joint limits on an axis basis. There were mentions of other IK plugins found, but they were not open source and not available for free, unfortunately. The best way to get the virtual PR2 robot to move as similar as possible to the real world would be to use the same inverse kinematics solver within the Animation Blueprint as is used on the real robot. There are several options for the PR2 out there, all with their own benefits and downsides, but the best solution seemed to be to use the Orocos-KDL(Kinematics and Dynamics Library)[38](#page-36-1) and the Newton-Raphson[39](#page-36-2) position IK-Chain solver with joint limits. This solution had four developmental stages:\n",
      "\n",
      "- 1. Create a custom Node for the Animation Graph.\n",
      "- 2. Import the KDL library into this node.\n",
      "- 3. Create a kinematic chain based on the given inputs, including joint limits.\n",
      "- 4. Calculate the correct transforms.\n",
      "\n",
      "### **Creating a new custom Node for the Animation Graph**\n",
      "\n",
      "In order to be able to use the KDL library for the animation in Unreal Engine, an animation graph node needs to be created, which will be the interface between Unreal Engine and the KDL third party library. After some research, another custom animation graph node was found[40](#page-36-3), which was used as a basis. This template also wraps the node already into a plugin structure, which is very useful, since this will not be needed to be implemented later on, in case this should become a public plugin.\n",
      "\n",
      "Generally, an animation graph node contains two components. The code for the node itself, in this case AnimNode\\_PR2IK.h and AnimNode\\_PR2IK.cpp, and the code which describes how this node appears and behaves like within the Unreal Engine Editor and the Animation Graph: AnimGraphNode\\_PR2IK.h and AnimGraphNode\\_PR2IK.h. The later two don't really do that much so they won't be explained further, but the AnimNode\\_PR2IK.cpp will be explained in more detail below. At this point it simply needs to implement certain functions it is inheriting from and that is that.\n",
      "\n",
      "<span id=\"page-36-1\"></span><sup>38</sup>KDL: <https://orocos.org/kdl.html>\n",
      "\n",
      "<span id=\"page-36-2\"></span><sup>39</sup>[\\[6\\]](#page-56-14): A. Goldenberg, B. Benhabib, and R. Fenton. \"A complete generalized solution to the inverse kinematics of robots\". In: *IEEE Journal on Robotics and Automation* 1.1 (1985), pp. 14â€“20. doi: [10.1109/JRA.1985.1086995](https://doi.org/10.1109/JRA.1985.1086995)\n",
      "\n",
      "<span id=\"page-36-3\"></span><sup>40</sup>Custom Animation Graph Node: [https://github.com/dawnarc/ue4\\\\_custom\\\\_anim\\\\_graph\\\\_node](https://github.com/dawnarc/ue4_custom_anim_graph_node)\n",
      "\n",
      "![](_page_37_Figure_1.jpeg)\n",
      "\n",
      "<span id=\"page-37-4\"></span>Figure 13: *On the left: the PR2 IK node within the animation graph. On the right: exemplary settings for the PR IK node*\n",
      "\n",
      "#### **Importing the KDL library into the Custom Animation Node**\n",
      "\n",
      "This has proven to be more tricky. Not only is KDL needed, but the Eigen[41](#page-37-0) library, since KDL has a dependency on it. The first approach was to follow a tutorial[42](#page-37-1). However that did not work initially, since the libraries imported there, were already build while Eigen and KDL were not. The idea that since Unreal Engine uses Microsoft Visual Studio[43](#page-37-2) to build everything, and since Visual Studio with a plugin can build CMake files (as explained in this tutorial[44](#page-37-3)) it might also be able to build Eigen and KDL, failed unfortunately. Another approach was manually build these libraries by hand and import them into the Plugin. This is not the most elegant solution, and it remains to be seen if there is a more elegant one, but it worked. Some other tweaks were however also needed. For example, the Eigen library contains exclusively header files. However, they do not have a .h extension, so Unreal could not find them. The solution was to add the extension there manually. This unfortunately also needed to be done with all the includes where KDL references Eigen or Eigen references itself. They also needed to be specified with the correct extension. After this was done, it was finally possible to use KDL within the animation graph node.\n",
      "\n",
      "#### **Creating a Kinematic Chain based on the Input received**\n",
      "\n",
      "The PR2\\_IK node within the animation graph needs to get the two end-points of an IK chain to be defined. These can be selected in the settings of the node, in the Tip Bone and Root Bone. For the right arm the Tip Bone is set to the r\\_gripper\\_palm\\_link and the Root Bone to textttr\\_shoulder\\_pan\\_link. In order to set the limits, one can expand the list under these settings, select any bone and set which axis should be limited, if the joint is fixed or revolute and the min and max limits can be set in radians. Another feature is the default bone angle field. It allows to define the starting point for the joint. This is used to create PR2s initial position, where both arms are bend at the elbow and facing down slightly. If the arms remain outstretched, maintaining the default position given by the URDF, the IK will have a harder time finding a valid configuration for nearby\n",
      "\n",
      "<span id=\"page-37-0\"></span><sup>41</sup>Eigen library: <http://eigen.tuxfamily.org/> (last accessed: 06.12.2020)\n",
      "\n",
      "<span id=\"page-37-1\"></span><sup>42</sup>Importing a third party library into Unreal Engine tutorial: [http://www.valentinkraft.de/](http://www.valentinkraft.de/including-the-point-cloud-library-into-unreal-tutorial/) [including-the-point-cloud-library-into-unreal-tutorial/](http://www.valentinkraft.de/including-the-point-cloud-library-into-unreal-tutorial/) (last accessed: 06.12.2020)\n",
      "\n",
      "<span id=\"page-37-2\"></span><sup>43</sup>Microsoft Visual Studio: <https://visualstudio.microsoft.com/de/>\n",
      "\n",
      "<span id=\"page-37-3\"></span><sup>44</sup>CMake with Microsoft Visual Studio: [https://docs.microsoft.com/en-us/cpp/build/](https://docs.microsoft.com/en-us/cpp/build/cmake-projects-in-visual-studio?view=msvc-160) [cmake-projects-in-visual-studio?view=msvc-160](https://docs.microsoft.com/en-us/cpp/build/cmake-projects-in-visual-studio?view=msvc-160)\n",
      "\n",
      "poses. A default pose can also be defined via animation blending, but this might lead to some bad side effects, in which the poses are permanently offset for the IK solver. This is what seemed to happen during debugging, but could be investigated further in future work.\n",
      "\n",
      "The main development is within the EvaluateSkeletalControl\\_AnyThread(...) function. First, all members of the IK chain are found by iterating from the Tip Bone to the Root Bone and adding every found parent which does not match the Root Bone to an array of pairs, where each pair contains the bone name and the Unreal Engines internal bone index. Every bone within a skeleton has a unique autogenerated index received upon import. Since the obtained array contains the bone references in tip to root order, and the IK solver requires them in the root to tip order, the array is being flipped to correct this. Then another array gets initialized, which contains all the default values for the bones. If the IK has already moved the bones previously, then this step is skipped since the array will contain the previously calculated poses of the joints, aka. the previous state, which is used as a seed state for the solver.\n",
      "\n",
      "Before any poses can be fed into the IK solver however, the difference in units needs to be accounted for. In Unreal Engine, poses in world and component space are calculated in centimeters, and local space transforms are represented in meters. KDL requires poses to be in defined meters also. Therefore some conversions need to be made to match the different units to one another. Since there were some issues with scaling, the scale parameter of all transforms received from the Unreal Engine is by default set to 1*.*0*,* 1*.*0*,* 1*.*0, to ensure that it stays correct and does not accidentally get affected by transform multiplications.\n",
      "\n",
      "Before the IK chain can be created, the corresponding KDL::Joint and KDL::Frame objects need to be defined. Once again, the list of all bones is iterated upon and for each bone a KDL::Joint object is created, containing the axis of rotation specified earlier in the settings of the node. The KDL::Frame contains the location vector of the bone in local space. Once both of these objects are generated, they are both added to the IK-chain, which combines these two elements into a KDL::Segment. At the same time, two lists of joint limits are generated, one for the maximal and the other for the minimal joint angles specified in the node. The input to the KDL solver is not only the IK-chain but also an array of joint angles of the previous KDL computation or the default angle state given in the settings of the node.\n",
      "\n",
      "#### **Calculate the necessary Transforms**\n",
      "\n",
      "The parent bone of the root bone is obtained, which in this case is the torso\\_lift\\_link and the goal transform, which originally is provided to the node in world frame, is being recalculated to the torso\\_lift\\_link frame. It is important to note, that within ROS transforms in a product are usually applied from right to left and in Unreal Engine, they are applied from left to right. This initially caused a lot of errors.\n",
      "\n",
      "In order to apply this transform, the transform of torso\\_lift\\_link is obtained in component space. The values of the location component are divided by 100 so that they are converted from centimeters into meters. The scale is set to 1*.*0*,* 1*.*0*,* 1*.*0 for reasons previously mentioned, and an offset of about 90 degrees is added around the *Z* axis to the rotation component. This offset is there because the stretched-out arms of the PR2 when pointing forward, are not in their 0 position rotation wise. Instead, they have an offset of +90 degrees around the *Z* axis, which needs to be compensated by an additional transform\n",
      "\n",
      "multiplication of just this offset. The code snippet, performing these calculations is the following:\n",
      "\n",
      "| 1 | FTransform<br>RootTransformComp<br>=<br>Output . Pose .                             |  |  |  |\n",
      "|---|-------------------------------------------------------------------------------------|--|--|--|\n",
      "|   | GetComponentSpaceTransform ( Output . Pose . GetPose ( ) .                          |  |  |  |\n",
      "|   | GetParentBoneIndex ( RootIndex ) ) ;                                                |  |  |  |\n",
      "| 2 | RootTransformComp<br>=<br>FlipT ran sfo rm ( RootTransformComp ) ;                  |  |  |  |\n",
      "| 3 | RootTransformComp . S e tLo ca tio n ( RootTransformComp .                          |  |  |  |\n",
      "|   | Ge tLoca tion ( )<br>/<br>100 ) ;                                                   |  |  |  |\n",
      "| 4 | RootTransformComp . Se tScale3D ( FVector ( 1 . 0 ,<br>1 . 0 ,<br>1 . 0 ) ) ;       |  |  |  |\n",
      "| 5 | FTransform<br>adjustYaw ;                                                           |  |  |  |\n",
      "| 6 | adjustYaw . S e tRo ta tio n (FQuat ( FVector ( 0 ,<br>0 ,<br>1 ) ,<br>3.14159<br>/ |  |  |  |\n",
      "|   | 2 ) ) ;<br>//arm<br>o f f s e t<br>r o t a t i o n                                  |  |  |  |\n",
      "| 7 | FTransform<br>GoalInBoneSpace<br>=<br>adjustYawâˆ—                                    |  |  |  |\n",
      "|   | S cal e dEff e c to rGoalT ra n sfo rm<br>âˆ—<br>ComponentTransform .                 |  |  |  |\n",
      "|   | I n v e r s e ( )<br>âˆ—<br>RootTransformComp . I n v e r s e ( ) ;                   |  |  |  |\n",
      "\n",
      "The types are FTransform since this is the transform-type Unreal Engine works with. After this calculation, the scale is being fixed again, and the same operations are being performed for the calculation of the tip bone transform in torso\\_lift\\_link space.\n",
      "\n",
      "Since KDL uses its own types for vectors and quaternions (e.g. KDL::Vector), the goal pose needs to be converted into these types. After this, the solver is finally initialized and computes the joint angles needed for the IK-chain to reach its goal. If the goal cannot be reached and maximal amount of iterations is exceeded, an error message is printed and the arms simply remain in their last known position. This is how the arms essentially can get *stuck*. One can accidentally move them in a way that KDL cannot compute a solution to move them out of the configuration. After this, the solution computed by KDL needs to be converted into Unreal Engine's units, meaning the location component back into centimeters as well as the types need to be cast into Unreal Engine vector and transform types again. The result is then applied to the skeletal mesh.\n",
      "\n",
      "Most of the code needed to use KDL is essentially just parsing values from Unreal Engine data types and classes into Eigen or KDL data types, and then back again. For future work this could be generalized so that it wouldn't need to cloud up the code needed to run KDL and the necessary transform multiplications.\n",
      "\n",
      "A feature is also the potential detachment of the grippers. They are always located at the same pose as the goal which is sent to KDL, which is based on the motion controller location within the VR world. When the goal is reachable and KDL finds a solution for it, the arm will be attached to the gripper at the wrist and move with the movement of the gripper. If KDL cannot find a solution, the arm will detach from the gripper instead. This allows the human user to still see where the goal is, and also take notice that this potential configuration does not seem to have a solution. The user can then try to reattach the gripper by moving it close to the wrist again and try to move the arm out of the locked configuration.\n",
      "\n",
      "Since the node currently includes specific mentions of the PR2 links, namely for the feature mentioned above, it is being checked if a name matches to e.g. r\\_gripper\\_palm\\_link. Therefore the node is currently still called the PR2 IK node, since that call makes it PR2 specific. Once these references are removed or exposed to the node settings available within the blueprints, it can be renamed into a general KDL node.\n",
      "\n",
      "## <span id=\"page-40-0\"></span>**4.2.4 Unreal Engine: Animation of the PR2 robot**\n",
      "\n",
      "This section will describe in more detail which features were implemented within the animation blueprint in order to create a realistically moving PR2 model.\n",
      "\n",
      "### <span id=\"page-40-1\"></span>**4.2.4.1 Opening and Closing Grippers**\n",
      "\n",
      "The opening and closing of the gripper is mapped to the trigger of the motion controller. Since the values of the trigger input are between 0 and 1, they are scaled up to be in the range between 0 and 30. This range is based on the visible range of motion of the gripper. These calculations look like this:\n",
      "\n",
      "![](_page_40_Picture_6.jpeg)\n",
      "\n",
      "Figure 14: *Scaling of the motion controller trigger value in order to be in the range of 0 to 30*\n",
      "\n",
      "<span id=\"page-40-2\"></span>Based on this value, within the animation graph one finger of the gripper, the r\\_gripper\\_r\\_finger\\_link gets moved via the use of the Transform (Modify) Bone node, which takes the value calculated based on the trigger and transformed into a *Z* component of a rotator and adds it to the current rotation of the bone in bone space. The r\\_gripper\\_r\\_finger\\_tip\\_link copies the resulting rotational movement from the r\\_gripper\\_r\\_finger\\_link using the Apply a Percentage of Rotation node, but since it has to rotate into another direction around its *Z* axis, the multiplied of âˆ’1 is applied. This is repeated accordingly for the fingers on the left side of the gripper. The resulting chain of movement can be seen in the following figure:\n",
      "\n",
      "![](_page_40_Figure_9.jpeg)\n",
      "\n",
      "<span id=\"page-40-3\"></span>Figure 15: *Opening and closing of the gripper based on a sequence of copying the rotation of one bone.*\n",
      "\n",
      "### <span id=\"page-41-0\"></span>**4.2.4.2 Grasping Objects**\n",
      "\n",
      "Grasping objects is implemented based on ray tracing between the fingers of the gripper. A socket within the skeletal mesh is created, representing the origin point from which the ray tracing should take place. The socket is in the same position as the mesh\\_r\\_gripper\\_r\\_finger\\_tip\\_link, since it is defined as its child. The reason for the use of a sockets is, that it is easier to access via blueprints than a bone would be. The socket will however follow the pose of its parent bone. Within the BP\\_PR2\\_Character another scene component, RightHand\\_Grasping\\_Point is added, which follows the position of the socket and to which a *SemLog* object is parented. This object is important for generating grasping events, but this will be discussed later on in this chapter. Based on the location of the RightHand\\_Grasping\\_Point and if a pickup event occurs, which occurs once the trigger of the motion controller is pressed, the ray tracing event is triggered. A short ray is being projected between the fingers of the robot and if another object is in between them, and this object is a bowl, cup or spoon, it is attached to the gripper and its physics simulation is being disabled. If it would not get disabled, the object would very likely slip out of the gripper or move weirdly during the transporting event, since it is essentially attached to only one point it tends to wobble and rotate. Once the motion control trigger is released and if an object is currently held, the object is released and its physics simulation is enabled again.\n",
      "\n",
      "A physics handle in order to perform physics-based grasping was also implemented and tested, but for the current setup which is very animation heavy, the attachment implemented in the way above is just more robust and easier to work with from a user perspective.\n",
      "\n",
      "## <span id=\"page-41-1\"></span>**4.2.4.3 PR2 Head motion**\n",
      "\n",
      "In order for the PR2 model to feel more responsive to the user, the robots head movement was mapped to the HTC VIVE Headset. The rotation of the headset is being obtained and the *Z* and *X* components are extracted. The *Z* component is being put back into an empty rotator and passed on a Transform (Modify) Bone which applied this rotation to the head\\_pan\\_link. The same is being done with the *X* component, just that this time it is applied to the head\\_tilit\\_link.\n",
      "\n",
      "Depending on if the PR2s body-rotation is mapped directly to the headset or to buttons on the motion controllers, this implementation might be less visible. In the first scenario, only the up and down movement of the head will be seen by the user, since the rotation from side to side is also followed by the body. With the second navigation scenario, the head would be fully movable.\n",
      "\n",
      "#### <span id=\"page-42-0\"></span>**4.2.5 Robot to Human Body Adaptation**\n",
      "\n",
      "This section will cover the measures taken to try and make the usability of the virtual robot as natural and intuitive for the human user as possible. This will also cover how some differences between the two bodies were handled and addressed, and which solutions were found to try and eliminate these differences.\n",
      "\n",
      "### <span id=\"page-42-1\"></span>**4.2.5.1 Gripper and Arm Range Extension while Compensating for Torso Bending**\n",
      "\n",
      "As also already mentioned in her thesis by Zihe Xu[\\[20\\]](#page-57-0), the PR2's arms are longer than the average human ones. However, a human is able to compensate for this by bending the torso forward, while the robot cannot. In order to mimic the robots limitations as closely as possible, bending of the torso should be avoided since in a grasping scenario, it would also move the position of the VR camera closer to the surface from which the object is being picked up. Since the base position of the robot is being calculated based on the VR Headsets position, this can lead to the robots base colliding with the environment. In order to avoid that, bending the torso forward should either be prohibited, or not necessary. It was already attempted in a previous approach[\\[20\\]](#page-57-0) to prohibit the bending of the human torso by attaching another tracker onto the human users chest and providing visual ques accordingly. However, this only had limited success. While investigating this same issue and trying to solve it based on the headsets position alone, it was also found that just by looking down while wearing the headset, the Z component of the transform which describes the height position, varies so much, that it would be very likely nearly impossible to differentiate between the bending of the torso or just simply looking down.\n",
      "\n",
      "Therefore the attempt in this thesis is to render it unnecessary for the human to bend the torso in the first place. To achieve this, the human user is able to move the detached grippers forwards or backwards, basically reaching forward beyond the motion controllers position. This allows to deal with two issues at once: It allows to compensate for the difference in arms length between the human and the robot, since the human can now virtually extend the grippers beyond the length of the users arms, and it allows to avoid the bending forward issue, since it is now less effort to just press a button instead of having to bend forward to reach for something. Of course, this effect applies more over time, the longer the system is being used by the human. This feature also helps with the initial adjustment of the human user to the robots shape, since a comfortable position to maneuver the robots arms can be found within a few seconds after launching the project. From a personal perspective, it almost feels like stepping into a robot-suit.\n",
      "\n",
      "The position of the Grippers is computed based on the current position of the motion controller. The forward-vector is obtained, and scaled incrementally, depending on how long the up (face button 1) or down (face button 3) buttons on the track pad of the controller are pressed. The grippers are then offset into the forward or backward direction accordingly.\n",
      "\n",
      "![](_page_43_Picture_1.jpeg)\n",
      "\n",
      "Figure 16: *Gripper detachment feature. the green circle shows the position of the motion controller, while the orange circle shows the position of the gripper and how it can be extended.*\n",
      "\n",
      "<span id=\"page-43-1\"></span>![](_page_43_Figure_3.jpeg)\n",
      "\n",
      "<span id=\"page-43-2\"></span>Figure 17: *A part of the blueprint which is calculating and scaling the forward vector, in order to adjust the robot to humans arms length.*\n",
      "\n",
      "## <span id=\"page-43-0\"></span>**4.2.5.2 Mirror**\n",
      "\n",
      "In order for the human user to get a better understanding of how the robots body behaves based on the provided input, a very large mirror was setup within the VR environment. The mirror was simply created by creating a rectangle mirror body, which essentially is just a flat cube object, which got a very reflective material applied to it, and which is encapsulated within a planar reflection component. The mirror is overall very useful and can be also used for when an arm gets IK-stuck out of the field of view. It is a very simple but also effective solution.\n",
      "\n",
      "<span id=\"page-43-3\"></span>![](_page_43_Picture_7.jpeg)\n",
      "\n",
      "Figure 18: *A mirror object to help the human user to settle into the robots body.*\n",
      "\n",
      "### <span id=\"page-44-0\"></span>**4.2.5.3 Body height adjustment**\n",
      "\n",
      "Not every human user is the same height as the PR2, and since the VR Headsets height is based on the human users height, the PR2 needs to adapt accordingly. If the robot does not adapt, manipulating the VR world through the robot gets an unpleasant uncanny feeling, since the human user is either stuck in the middle of the torso and the shoulders and arms height of the robot do not match up with the humans at all, or the user could be floating above the robot, experiencing similar issues. In order to avoid that, depending on the height of the human, the torso of the PR2 is moved up or down.\n",
      "\n",
      "![](_page_44_Picture_3.jpeg)\n",
      "\n",
      "Figure 19: *Body height calculations.*\n",
      "\n",
      "<span id=\"page-44-3\"></span>Some of these values might seem rather large on the first glance, for example the Clamp (float) node is set to 70, which is a rather large value to add towards the height of the robot. The value is divided by 1*,* 5 in order to scale it down again. This is done because if the mapping is setup in a one to one way, the body ends up shaking, since the headsets height varies constantly to some degree also. After some trial and error, this seemed to be a good solution to the otherwise very jumpy torso problem.\n",
      "\n",
      "## <span id=\"page-44-1\"></span>**4.2.6 Adapting USemLog to collect VR data**\n",
      "\n",
      "In order to be able to log everything the human does within the Virtual Reality environment, a logger is needed. USemLog is a plugin for Unreal Engine providing just that. However, since this thesis uses the pipeline developed prior[\\[10\\]](#page-56-2)[\\[11\\]](#page-56-3), and since the new version of KnowRob, at the time of writing this is not yet complete, it was decided to stick with the old KnowRob and therefore also the old USemLog 0.2[45](#page-44-2) version. In order to setup the logging, different objects needed to be added to the VR environment, depending on what kind of event needs to be tracked. Also, each object need to be defined within its *Actor - Tag* property, as a dynamic or static object. Other properties can be of course given also. E.g. what class an object belongs to or which mesh should be loaded for its representation. For example, the Tag of the PR2 robot looks like this: SemLog;Runtime,Dynamic;Class,PR2;PathToSkeletalMesh,package://robcog/Content/Models/PR2\\_model.;Id,SoVP; The ID is always autogenerated, whenever the Generate New Ids button is pressed within the SemLog plugin. All dynamic objects can be moved, while static objects stay where they are. This differentiation is important. When this tag is provided, raw data, meaning the position of the object within the world is being recorded to a json file. Since the PR2\n",
      "\n",
      "<span id=\"page-44-2\"></span><sup>45</sup>USemLog 0.2: <https://github.com/robcog-iai/USemLog/releases/tag/v0.2>\n",
      "\n",
      "is a skeletal mesh object, the position of every bone is being recorded.\n",
      "\n",
      "In order to be able to generate contact events between objects, an SLContactManager object needs to be added to the surface, in a way encapsulating it. These objects were added for the kitchen island and the sink area, since these are the surfaces which hold the objects for the performed pick and place tasks.\n",
      "\n",
      "I Generating grasping events was a bit more tricky. This event generator is essentially bound to an implementation of the motion controllers within the UMCInteraction[46](#page-45-0). Since motion controllers were already setup and defined for this thesis, the code of the UMCInteraction plugin was altered and decoupled from the motion controllers. Fundamentally, a sphere is attached to the gripper of the PR2. Whenever something overlaps with this sphere, a grasping something event is generated. This solution was very hacky and is not necessarily stable, but it works and generates some data.\n",
      "\n",
      "In order to be able to generally log data, two more objects need to be added to the world. These objects are: a SLFurnitureStateManager which, as the name already suggests, logs the state of all the furniture items within the world, and a SLRuntimeManager, which generally contains settings as to how often raw data should be recorded and similar.\n",
      "\n",
      "Since there was not much documentation about this very old version of this plugin, it could be that the setup is wrong, since it was essentially reverse-engineered based on an old RobCoG project version. It does generate data, but does not seem to be as accurate about it as former data sets suggest.\n",
      "\n",
      "<span id=\"page-45-0\"></span><sup>46</sup>UMCInteraction plugin: <https://github.com/robcog-iai/UMCInteraction>\n",
      "\n",
      "## <span id=\"page-46-0\"></span>**5 Experimental evaluation**\n",
      "\n",
      "## <span id=\"page-46-1\"></span>**5.1 Evaluation of the VR-PR2-model and behavior**\n",
      "\n",
      "This section will discuss how the generated PR2 model behaves within the VR environment, based on the overall look and feel from a user perspective. It will be described how it feels to pilot the PR2 and to be *inside the robots body*, how responsive the system is, which features it contains and which flaws. It will be also discussed how performing a pick and place task as the PR2 robot compares to performing the same task with the previous method of just piloting human hands within the VR environment. This comparison will unfortunately be only subjective, since it is currently impossible to conduct a proper user survey because of the current pandemic. However due to my personal previous experience with the RobCoG, hands-only-VR system within my Bachelors thesis and the evaluation performed for a paper I will try my best to make a fair comparison between the two systems.\n",
      "\n",
      "#### <span id=\"page-46-2\"></span>**5.1.1 Differences between the systems based on general differences between the human and robots bodies**\n",
      "\n",
      "The following sections will discuss the general differences between the human and robots bodies, as well as how the two different systems, meaning the RobCoG approach with only human hands in VR and the new approach in this thesis of having the robots body in VR, compare to one another, based on look and feel for the human user, as well as how these differences of the bodies might play into the data acquisition process for teaching the real robot to perform everyday pick and place tasks. Some aspects which are described in the *general look and feel* section are results of the mapping between the human to robot body and the reasoning behind them will be explained in their respective sections.\n",
      "\n",
      "#### <span id=\"page-46-3\"></span>**5.1.1.1 General look and feel**\n",
      "\n",
      "The **RobCoG** system allows for a lot of freedom for the human user. Since one only has to essentially focus on the virtual human hands, and not have to take into a count any sort of body, one can really focus on grasping objects, from which angle and direction to grasp, and to try and make the grasps as realistic as possible, if one wishes to. Since the objects attach to the palm of the hand however, some grasps are harder to perform then others. For example, grasping the spoon is probably the hardest one. In order to grasp it, one has to press essentially the hand completely onto the surface on which the spoon is located, so that it can get in reach of the palm, in order to be grasped. This is not necessarily a realistic grasp, neither for the human nor the robot, but this is a rather extreme example. Navigating the virtual environment is rather easy, since, again, one does not have to worry about a body potentially colliding with things. It also allows to pass through furniture, as long as the human user holds the hands in a way that they won't collide with the environment. For example, in order to place an object on the opposite side of the kitchen island, one can walk through it, even though that feels weird for the human user since the human brain obviously sees an object there that one usually cannot just walk through, instead of having to walk around the kitchen island. This might sound crazy at first but it can be beneficial, since the cable of the VR headset has a limited length and this allows the user to reach positions which otherwise would be unreachable, limited by the size of the available real world environment and the mentioned cable length of the headset.\n",
      "\n",
      "Since the hands have physics objects defined on them, one can open drawers by hooking the hand between the handle and the drawer and just pulling to open the drawer that way. The collision occurring between the hand and the drawer handle will cause it to open as expected. One can also push the drawers closed by just pushing on them with the hand, without needing to grasp anything for it.\n",
      "\n",
      "Overall the system is fairly intuitive to use, it is a lot of fun. The grasping takes a while to get used to and some limitations of the system, like missing a physical body, can be abused to gain some benefits where Virtual Reality might have been limited by the real world.\n",
      "\n",
      "The **PR2 VR** system is comparatively quite different. The human user now has a new body that is very different than the real human body and which takes a while to get used to. In order to ease in this transition, a full body mirror was created in which the human user can observe how the robots body behaves based on the human movement. The robots body follows the headset completely, including the rotation. This makes the navigation of the robot within VR intuitive, but since one cannot rotate the head independently of the body, it can in some specific cases be a hindrance. For example, if one wants to look at the robots arm which might be just out the field of view, one cannot just rotate the head and look at it. With the current implementation the human user would have to walk back to the mirror, to see how exactly the arm is positioned. Since the grippers are attached to the position of the motion controllers rather than being attached to the rest of the arm, the intuitive first reaction is to attach them to the arms again, which can be done by simply moving both grippers to the wrist. Since, as will be explained further down in this chapter, the arms of the robot are a lot longer than the human arms, the human user can move the gripper further forward or backwards by pressing buttons on the motion controller. While this sounds like it might not be intuitive, it does feel intuitive. From my personal point of view, it feels like one is stepping into an exo-skeleton, which happens to be the PR2 robot. This feature also allows for the human user to stretch out the PR2s arm beyond the humans arms length but within the robots arm length, allowing for better robot base positions since now the user does not have to move that close to the surface the objects to manipulate are located on. After a while, once one gets used to this feature, it is noticeable that instead of stretching out the real humans arm in the real world, it is preferred to just stretch out the virtual arm instead, while keeping the real humans elbows rather close to the torso. After all, pressing a button requires less effort than moving an entire arm.\n",
      "\n",
      "Grasping objects is currently not physics based at all. This means that in order to grasp an object, it has to be located between the two fingers of the gripper. This also means that the grippers currently will always close completely in order to grasp an item, going visually through it in the process. While this is not very realistic, for now it is a decent solution that can be improved upon in the future.\n",
      "\n",
      "#### <span id=\"page-47-0\"></span>**5.1.1.2 Real world navigation mapping to VR**\n",
      "\n",
      "In RobCoG the position of the VR headset has no visual representation. It is just the position where the human user is within the VR world. The position of the motion controllers follow the headset and are always positioned relative to it.\n",
      "\n",
      "The VR PR2 also follows the position of the VR headset and also rotates with it. However there are essentially two ways of setting up the rotation. In this case the rotation of the headset was mapped directly to the body. Another option is to map it to two buttons instead, so that the head can be moved completely independently of the body. In earlier testing, the second approach was the original idea, to just rotate the body when needed on button press, so that it matches the rotation of the headset again. However for collecting data, it seemed rather tedious to always have to set the rotation manually, so the first approach was rather implemented. However this is open to preference of the user. Rotating the body of the PR2 including the camera on button press would however cause motion sickness, so this approach was not followed further.\n",
      "\n",
      "#### <span id=\"page-48-0\"></span>**5.1.1.3 Robot to human height adjustment**\n",
      "\n",
      "Within the RobCoG approach, the height of the human user compared to the robot was never an issue, since there was no body being visualized which might have been impacted by it. The motion controllers were directly mapped to the position of the human hands in the real world and that was completely sufficient.\n",
      "\n",
      "However, when accounting for a robots body which needs to be mapped to the human user one, height becomes an important concern. The robots torso need to move up and down depending on the height of the human, within a limit, so that the head-position as well as the arms and shoulders heights feel natural to the user. If the robots body is too low, the viewpoint of the human is above the robot, the shoulders are way too low and then the overall interacting experience suffers. Same applies for the robots torso being too high up, which would lead to the shoulders being above the head and the viewpoint being inside the torso of the robot. If any of these two cases occurs, it gives an uncanny feeling to the user since one can see that something is not quite right. Therefore, the height of the robots head is essentially mapped to the height og the VR headset, by moving the torso of the robot accordingly to match the height.\n",
      "\n",
      "This solution however has its own limits. One can argue that very small or very tall people might hit the limit of the capabilities of the real PR2, since the torso only has a limited movement space. In order to compensate for it, one could go above the PR2s limit. It might not look too off visually within the VR environment but would have to be accounted for later, when processing the data. For rather small people there might not be such an intuitive solution. Moving the robots torso down has a fixed limit. It can be forced down further but then it would also look ad feel wrong, since the shoulder-meshes would collide with the base and essentially need to pass through it. There the solution might lay in scaling the entire world down, or the PR2's body. But that would be a rather exceptional and rare case.\n",
      "\n",
      "#### <span id=\"page-48-1\"></span>**5.1.1.4 Human torso bending**\n",
      "\n",
      "The human user can bend the torso forward, in order to reach for an object and keep the feet in place. This prolongs in a way, the arms reachability capabilities. While the user can do so within the RobCoG game, which might lead to potential positions for the robot which collide with the environment, since the head and therefore the headset on which the base position is later based moves forward also, this is not possible to do with the PR2 VR model. Of course the user can perform this movement in the real world, there is no stopping that, but the result in VR would just be not the expected one. Since when bending forward, the head also moves forward, it would mean that the entire robot moves forward in VR also, potentially colliding with the environment. Also the grippers might detach from the wrist in this process and the human user learns quite quickly that this is an overall undesired effect, not gaining the expected benefit to the situation. Instead it is way more comfortable to stand upright at the same spot and just press a button to stretch out the arm further. After just a few minutes, this adaptation is intuitively made and the torso-bending is no longer a concern.\n",
      "\n",
      "#### <span id=\"page-49-0\"></span>**5.1.1.5 Human feet and the robot-base position**\n",
      "\n",
      "Humans have very small feet, compared to the PR2 robots base. Also our feet are located directly under our head, while the base of the PR2 robot extends forward by quite a margin. This generally means that the human can stand a lot closer to surfaces for pick and place activities as the robot would be able to.\n",
      "\n",
      "Within the RobCoG approach, there was nothing to account for this difference during data acquisition. The human is not able to extend the VR hands to be able to manipulate objects from more of a distance, and there are no visual ques at all for the position of the feet or the size of the robots base. For the so collected data, this would mean that the feet position is calculated based on the position of the VR headset, removing the z component of the pose in order to project the position onto the floor. For the robot to be able to use these poses to learn e.g. where the human was standing when he grasped an object, an offset of 0.2 meters was added to the cameras position, basically projecting the position away from the object, to make space for the robots base.\n",
      "\n",
      "With this new approach of the PR2 robots body being in VR, the human user can directly see the robots base and be therefore made aware of the limitations and potential collisions it entails. Also, not only is the camera position tracked, but since the PR2s body is a skeletal mesh, the position of each bone is recorded. Meaning that the position of the base can be used directly, and no offsets need to be applied to it.\n",
      "\n",
      "#### <span id=\"page-49-1\"></span>**5.1.1.6 Human arms and robot arms**\n",
      "\n",
      "As previously mentioned throughout the thesis, the arms of the PR2 robot are a lot longer than the human arms. One rather major difference is that the grippers of the PR2 are initially detached from the rest of the arm. The gripper locations are mapped to the motion controllers, while the movement of the arm is computed by KDL. In order to compensate for the length difference, the location of the grippers can be offset to the the motion controller on button press. The grippers can move further forward or backwards, however the human user prefers or needs. Some might prefer to have the motion controllers as close to the grippers as possible and to stretch out the human arms as much as they can, in order to achieve that, while other users might prefer to keep their arms rather close to their body, and use this feature more instead. Generally, this feature allows the human user to keep standing further away from a surface, keeping the distance far enough to not collide the base with the potential surface base, and use the length of the PR2s arms to an advantage. The detachment of the grippers also compensates for the realistic scenario of KDL not being able to find a solution for the current configuration. Instead of completely stopping the arms movement and potentially confusing the user, the gripper only detaches. The user is than made aware by this that the current position\n",
      "\n",
      "or grasp attempt, might not be suitable for the robot. The user can then reattach the gripper by moving it closer to the arm again and moving the arm to an overall better configuration for the robot. Sometimes the reattachment does not work completely, and as un-intuitive as it sounds, shaking the controller a bit can help. Sometimes however the arm can get stuck entirely, and the project has to be restarted entirely to unstuck it. In future work however, there could be a button implemented to just reset the arms position to a default position to help unstuck them.\n",
      "\n",
      "## <span id=\"page-50-0\"></span>**5.2 Result of the Evaluation of the VR-PR2-model and behavior**\n",
      "\n",
      "Based on the previous section and my subjective overall experience, the differences in both approaches are many. With the PR2 model as a body inside of VR, the human user has to keep a few more things in mind while performing pick and place tasks. One has to keep an eye on the base, although after a while as a user one gets an intuitive automatic estimation of where it is without having to look to check. Because the movement of the arms is calculated by KDL, which occasionally might not find a solution, one has to keep an eye on the gripper to arm connection, quite literally. While within RobCoG the focus lies completely on the hands, here the focus is also on the overall arm position and how it behaves. The resulting movement might be slower and more controlled since instead of just watching what object one is grasping, the entire arm is being watched. It is also pretty hard to keep both grippers attached to both arms at the same time. The resulting solution was one of two things: either one performs the pick and place setup one arm at a time, transporting all objects from the sink to the kitchen island one by one with first only the right arm, and then with the other arm to essentially generate a full data set. Or to keep the gripper to arm connection while performing one grasp, then basically ignoring the detachment while the other arm is grasping and ignoring it during the navigation portion between the kitchen island and sink area, and then attach the grippers again to the arms while placing down the objects. The later solution is an acceptable one, because for the real robot one can define stable carrying positions which can be used no matter which object has been grasped and needs to be transported. Usually these carrying poses would also want to move the arm out of the field of view, in order to not obstruct future perception tasks, which is something the human user might want to avoid in VR, since if the gripper gets detached while outside the field of view, one would need to go back to the mirror to see and reattach it.\n",
      "\n",
      "Collecting pick and place data with the PR2 body is overall very similar to doing so with RobCoG. The differences are mainly that while collecting data with RobCoG, one could either move completely freely and behave entirely like a human would, bending forward if needed be, or choose to try to generate data which might be the most useful to the robot later on. The later would require to always keep in mind not to bend the torso forward, try and imagine how the PR2s gripper would be able to grasp an object, try and stand in front of an area in a way that would seem like a position that would be plausible for the PR2 given a specific offset. All these things come with experience and observing either the real PR2, or collecting the data and running a lot of simulation on it in order to learn how to generate good and usable data. A lot of these things are now no longer concerns since one can directly already see if the PR2 would be able to stand in a specific location, if the arm can reach the object, if it fits into the gripper in order to grasp an object from the front, if the arm or the base would collide with something\n",
      "\n",
      "while performing a certain motion. Of course, there is room for improvement and a lot of feedback can be provided to the human user via vibration of the controllers or more visual ques, but this is another step into generating better data to teach robots with. This solution also gives the human user a better understanding of the PR2s movement capabilities. One could say, the human user learns about the robot by becoming the robot.\n",
      "\n",
      "Grasping objects as the PR2 is a lot more different then what it was expected to be like when collecting the data previously within the RobCoG setup. With RobCoG more top grasps were performed, because it seemed to be the easier way to grasp something for the robot. While inside the PR2s body, getting a good arm configuration to perform a top grasp was not as easy as it once seemed, so that in the resulting data many front grasps were performed, or things were grasped from the top but with an angle, instead of the straight down approach. This might be due to the arm tending to get stuck in a configuration while performing that grasp, or because it feels like an unusual movement to perform as a human. In RobCoG, since one had no visual representation of the robot, it might have appeared to be the only good way to grasp an object. Now, that one could directly see another solution working at least virtually, as a human one might intuitively resort to a more comfortable movement for grasping.\n",
      "\n",
      "Overall, this solution could serve in generating more reliable data for the robot in order to perform pick and place tasks and further help in understanding, as a human, how a robot interacts with the environment since one can experience it, in a way, first hand.\n",
      "\n",
      "## <span id=\"page-51-0\"></span>**5.3 PR2 KDL IK Evaluation**\n",
      "\n",
      "In order to be able to move the arms of the robots body as realistically as possible, an inverse kinematics solver node was implemented, which uses the KDL library with the Newton-Raphson solver, which is also used for the real PR2 robot. This node takes joint limits into account and is used to calculate the joint angles between the gripper\\_palm\\_link and the shoulder\\_pan\\_link of each arm. The limits have to be set by hand in radians, and in order to help avoid getting the arms stuck in the outstretched forward position, default values can be passed to the node in order to generate an initial pose. To further make the piloting of these arms a bit easier, and in order to avoid having the upper arm hit the table while performing a front grasp, the limits set for some links were stronger limited than the PR2 description and URDF file would specify. This mostly affected the shoulder\\_lift\\_links. Both continuous joints of the arms were fixed or strongly limited, since it was observed that KDL struggles very hard to find feasible configurations with continuous joints. These changes were more of quality of life improvements and can easily be adapted to the original PR2s limits without having to touch the nodes code. As already discussed previously in this chapter, the feature of detaching the grippers is also partially implemented within the KDL node, since if it cannot find a solution within the set amount of iterations, it will detach the gripper\\_palm\\_link from the rest of the arm. Currently this is hard-coded within the nodes code and would need to be adjusted for the future. Overall the PR2 IK node works well. It takes a bit of practice to get used to the robots arm positions, but it is very important for future work, that now while collecting data for the robot, the human user gets direct visual feedback of what would be reachable for the robot and what not. The user can also be sure that if e.g. the grasping action was performed without getting the gripper detached from the arm, the real robot will be able to replicate that movement.\n",
      "\n",
      "## <span id=\"page-52-0\"></span>**5.4 PR2-body-based VR data evaluation**\n",
      "\n",
      "A goal of this thesis was to collect data from the Virtual Reality while performing pick and place tasks within the PR2s virtual body. A dataset was created for this purpose, containing 15 episodes, of which each contains a full set of bringing the bowl, cup and spoon from the kitchen sink area to the kitchen island and back once with the right and then with the left hand, keeping the IK chain intact as much as possible while performing the task with the corresponding hand. This data has been imported to an old KnowRob and MongoDB version, using scripts[47](#page-52-1). Unfortunately, not many evaluation runs could be performed, since for an unknown reason, the chain of CRAM->KnowRob->CRAM- >BulletWorld kept on crashing, approximately every 5 minutes. Restarting it every time was just taking too much time, so that I had to abort this evaluation.\n",
      "\n",
      "From the 23 performed runs within the bullet world simulator, unfortunately only one was completely successful, meaning that all three objects were successfully picked and placed. 12 runs reported only one transporting failure, 8 reported 2 failures and 3 runs failed entirely. There were no search fails, meaning that the robot could find a pose to perceive the object from every time. There occurred 24 fetch and 13 delivery fails. This kind of evaluation is based on this paper[\\[11\\]](#page-56-3), since it is using the same tools.\n",
      "\n",
      "There are a few theories to what could have caused this. Either something with the recorded data is not quite right, which is probably very likely since the logging solution is very improvised. Maybe an offset between the Virtual Reality and the CRAM Bullet World changed, since the real world environment in which the data got recorded, also changed and was very limited. An update could have broken something on accident or maybe something else entirely caused this error.\n",
      "\n",
      "In order to solve this the dataset could be analyzed in more detail, or recreated within the same physical environment, as the previous one, just to avoid this being a real-worldto-vr-scaling issue. However, this should be investigated, so that this new approach of having the PR2 body within VR can be evaluated properly.\n",
      "\n",
      "<span id=\"page-52-1\"></span><sup>47</sup>scripts to import data to MongoDB and KnowRob automatically: [https://github.com/hawkina/vr\\\\_](https://github.com/hawkina/vr_neems_to_knowrob) [neems\\\\_to\\\\_knowrob](https://github.com/hawkina/vr_neems_to_knowrob)\n",
      "\n",
      "# <span id=\"page-53-0\"></span>**6 Conclusion**\n",
      "\n",
      "## <span id=\"page-53-1\"></span>**6.1 Summary**\n",
      "\n",
      "In this thesis a skeletal mesh model of the PR2 robot was largely auto generated via a few scripts for Blender and imported into the Unreal Engine and the RobCoG Virtual Reality environment. In order to be able to replicate the robots movement as close to the real robot as possible, an inverse kinematics node was implemented for the Unreal Engine's animation graph. It uses the Newton-Raphson IK solver provided by the orocos-KDL library, to compute joint angles for the arms of the robot, taking min and max limits on a per axis basis into account. This solution can guarantee to the human user that if the robot was able to perform a certain motion in the Virtual Reality environment, the real robot could perform this motion too, since now both robots use the same inverse kinematics solver. The opening and closing of the grippers was implemented essentially based on the trigger of the motion controller and the movement of one of the finger bones. Grasping of objects was implemented via ray-tracing between the fingers of the PR2 gripper. There were a few other features implemented, in order to map the robots body to the human and provide as an immersive experience as possible. For example, the robots head moves the same way the head mounted display of the VR user does. The height of the robot is being adjusted to the height of the human. A large mirror is provided so that the human user can get accustomed to piloting the robot. A way of logging the performance of manipulation activities within the Virtual Reality using an old version of the USemLog and UMCInteraction plugins was set up. A dataset of 15 episodes was created and attempted to be evaluated.\n",
      "\n",
      "## <span id=\"page-53-2\"></span>**6.2 Discussion**\n",
      "\n",
      "This thesis proves to have been a great journey. In the beginning, it was not expected that an own IK node would be implemented as a result of this. The idea was originally to use one of the IK nodes Unreal Engine already provides and the IK setup was expected to be much easier and straight forward than what it turned out to be. However, this node could prove to be useful for other researches as well and could be made into a publicly available plugin.\n",
      "\n",
      "Same goes for the adaptation done within the phobos plugin for Blender. It could become a sister-project of phobos. Overall, while some features got implemented, there seems to be always room for more. The way the PR2 gripper interacts with the objects it grasps at the moment could be improved with physics, so that the robot does not grasp through them. Where problems were not expected at all, problems occurred. One such case was the export of the generated skeletal mesh from Blender to Unreal Engine. It has proven to be very fragile and required many attempts in order to figure out the correct settings for the export and import, as well as how an object has to look like within Blender to be exportable within Unreal Engine. It was surprising how many differences the two programs had with one another. How different skeletal meshes could be handled and that there are several ways of defining bones was also rather surprising.\n",
      "\n",
      "Overall being inside the PR2s body and performing pick and place tasks as a robot seems in a way to be more intuitive than the previous approach since now instead of wondering if one is standing far enough away from the furniture and if the current position would generate a viable navigation pose, one can directly see it. It is also directly visible if\n",
      "\n",
      "the robot would be able to reach a certain pose or not with the arm, since if the goal is impossible KDL will not be able to find a solution for it.\n",
      "\n",
      "## <span id=\"page-54-0\"></span>**6.3 Future Work**\n",
      "\n",
      "There are many ways in which this work can be continued. For one the entire process of generating the skeletal mesh of the robot based on an URDF file could be finessed, in the sense that the file might not need to be filtered by hand to be able to be imported into blender to begin with. This could become a stand alone plugin for Blender and be decoupled from Phobos or to become a stand-alone extension or sister project of Phobos, called Deimos. The KDL-IK Node for the animation graph within Unreal Engine, which was developed for this thesis, could also become a publicly available plugin, which hopefully would help other users and researchers. Even when Unreal Engine is planning to introduce their own full-body IK system[48](#page-54-1) KDL might still be useful to other researchers who would like to be able to use the Newton-Raphson iterations algorithm for their own research.\n",
      "\n",
      "Overall using a robots model within VR could help new students who are starting to study robotics to understand the robots movement better and maybe even help the general public to gain interest in robots and reduce their fear of robots. The task of performing every day activities as a robot within Virtual Reality could even become a game, to help and collect more data for research.\n",
      "\n",
      "Another plugin for the Unreal Engine could be developed, which reads in a URDF file and configures the IK automatically, based on the limits provided in the URDF. If the robot will be equipped with a full physics asset, the plugin could automatically configure the constraints within the physics asset and generate the respective bodies. If a full body physics asset would be beneficial for the robots performance in VR should be discussed, but maybe physics could be generated and applied to the base and the grippers of the robot, so that collisions can be detected and the human user could obtain feedback based on these collisions.\n",
      "\n",
      "The entire approach could be tested with other robots, to see how it would be to pilot less human-like robots, e.g. robots with only one arm, or which are a lot larger or smaller than humans are, and which solutions might these robots need in order to be mapped to the human body, if at all.\n",
      "\n",
      "The data collection methods can be also improved upon. The currently implemented solution uses very old plugin versions, which by now have newer versions available, with new features and a different way to set them up. Both USemLog[49](#page-54-2) and KnowRob[50](#page-54-3) have been hugely reworked and updated in the last few months. They should also be updated within this project.\n",
      "\n",
      "Overall the evaluation of the here presented model should be evaluated in as much detail as its predecessor, which is described in the paper was, so that both approaches could be directly compared.\n",
      "\n",
      "<span id=\"page-54-1\"></span><sup>48</sup>Announcement of the introduction of full body IK with limits for a new upcoming Unreal Engine version: [https://docs.unrealengine.com/en-US/WhatsNew/Builds/ReleaseNotes/4\\\\_26/index.](https://docs.unrealengine.com/en-US/WhatsNew/Builds/ReleaseNotes/4_26/index.html) [html](https://docs.unrealengine.com/en-US/WhatsNew/Builds/ReleaseNotes/4_26/index.html) (last access: 05.12.2020)\n",
      "\n",
      "<span id=\"page-54-3\"></span><span id=\"page-54-2\"></span><sup>49</sup>USemLog GitHub repository <https://github.com/robcog-iai/USemLog> (last access: 05.12.2020) <sup>50</sup>KnowRob homepage: <http://www.knowrob.org/> (last access: 05.12.2020)\n",
      "\n",
      "#### 6 Conclusion\n",
      "\n",
      "A navigation capability based on button-presses could be developed also. It would be important to develop it in such a way that reduces or avoids motion sickness, which is a general VR problem. This would the user to navigate a larger Virtual Reality environment and be less limited by the available real-world space for VR.\n",
      "\n",
      "# <span id=\"page-56-0\"></span>**7 Appendix**\n",
      "\n",
      "## <span id=\"page-56-1\"></span>**7.1 Bibliography**\n",
      "\n",
      "## **References**\n",
      "\n",
      "- <span id=\"page-56-13\"></span>[1] Andreas Aristidou, Yiorgos Chrysanthou, and Joan Lasenby. \"Extending FABRIK with Model Constraints\". In: *Comput. Animat. Virtual Worlds* 27.1 (Jan. 2016), pp. 35â€“57. issn: 1546-4261. doi: [10.1002/cav.1630](https://doi.org/10.1002/cav.1630). url: [https://doi.org/10.](https://doi.org/10.1002/cav.1630) [1002/cav.1630](https://doi.org/10.1002/cav.1630).\n",
      "- <span id=\"page-56-12\"></span>[2] Andreas Aristidou and Joan Lasenby. \"FABRIK: A fast, iterative solver for the Inverse Kinematics problem\". In: *Graph. Models* 73.5 (Sept. 2011), pp. 243â€“260. issn: 1524-0703. doi: [10.1016/j.gmod.2011.05.003](https://doi.org/10.1016/j.gmod.2011.05.003). url: [http://dx.doi.org/](http://dx.doi.org/10.1016/j.gmod.2011.05.003) [10.1016/j.gmod.2011.05.003](http://dx.doi.org/10.1016/j.gmod.2011.05.003).\n",
      "- <span id=\"page-56-11\"></span>[3] Michael Beetz et al. \"Know Rob 2.0 â€” A 2nd Generation Knowledge Processing Framework for Cognition-Enabled Robotic Agents\". In: May 2018, pp. 512â€“519. doi: [10.1109/ICRA.2018.8460964](https://doi.org/10.1109/ICRA.2018.8460964).\n",
      "- <span id=\"page-56-8\"></span>[4] F. Brizzi et al. \"Effects of Augmented Reality on the Performance of Teleoperated Industrial Assembly Tasks in a Robotic Embodiment\". In: *IEEE Transactions on Human-Machine Systems* 48.2 (2018), pp. 197â€“206. doi: [10 . 1109 / THMS . 2017 .](https://doi.org/10.1109/THMS.2017.2782490) [2782490](https://doi.org/10.1109/THMS.2017.2782490).\n",
      "- <span id=\"page-56-7\"></span>[5] Open Source Robotics Foundation. *Bullet world demonstration*. url: [http://cram](http://cram-system.org/tutorials/intermediate/bullet_world)[system.org/tutorials/intermediate/bullet\\\\_world](http://cram-system.org/tutorials/intermediate/bullet_world) (visited on 04/09/2018).\n",
      "- <span id=\"page-56-14\"></span>[6] A. Goldenberg, B. Benhabib, and R. Fenton. \"A complete generalized solution to the inverse kinematics of robots\". In: *IEEE Journal on Robotics and Automation* 1.1 (1985), pp. 14â€“20. doi: [10.1109/JRA.1985.1086995](https://doi.org/10.1109/JRA.1985.1086995).\n",
      "- <span id=\"page-56-4\"></span>[7] A. Haidu and M. Beetz. \"Action recognition and interpretation from virtual demonstrations\". In: *2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*. 2016, pp. 2833â€“2838. doi: [10.1109/IROS.2016.7759439](https://doi.org/10.1109/IROS.2016.7759439).\n",
      "- <span id=\"page-56-9\"></span>[8] Andrei Haidu. *Robot Commonsense Games*. url: [http://www.robcog.org/games.](http://www.robcog.org/games.html) [html](http://www.robcog.org/games.html) (visited on 04/08/2018).\n",
      "- <span id=\"page-56-10\"></span>[9] Andrei Haidu and Michael Beetz. *Automated Models of Human Everyday Activity based on Game and Virtual Reality Technology*. (Visited on 04/01/2018).\n",
      "- <span id=\"page-56-2\"></span>[10] Alina Hawkin. \"Towards robots executing observed manipulation activities of humans\". Bachelor Thesis. Institute of Artificial Intelligence, University of Bremen. (Visited on 04/23/2018).\n",
      "- <span id=\"page-56-3\"></span>[11] Gayane Kazhoyan et al. \"Learning Motion Parameterizations of Mobile Pick and Place Actions from Observing Humans in Virtual Environments\". In: *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*. 2020.\n",
      "- <span id=\"page-56-6\"></span>[12] Yuan-Hong Liao et al. \"Synthesizing Environment-Aware Activities via Activity Sketches\". In: *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*. 2019.\n",
      "- <span id=\"page-56-5\"></span>[13] L. MÃ¶senlechner and M. Beetz. \"Parameterizing actions to have the appropriate effects\". In: *2011 IEEE/RSJ International Conference on Intelligent Robots and Systems*. 2011, pp. 4141â€“4147. doi: [10.1109/IROS.2011.6094883](https://doi.org/10.1109/IROS.2011.6094883).\n",
      "- <span id=\"page-57-1\"></span>[14] Lorenz MÃ¶senlechner. \"The Cognitive Robot Abstract Machine\". Dissertation. MÃ¼nchen: Technische UniversitÃ¤t MÃ¼nchen, 2016.\n",
      "- <span id=\"page-57-2\"></span>[15] Xavier Puig et al. *VirtualHome: Simulating Household Activities via Programs*. 2018. arXiv: [1806.07011 \\[cs.CV\\]](https://arxiv.org/abs/1806.07011).\n",
      "- <span id=\"page-57-4\"></span>[16] Eric Rosen et al. \"Testing Robot Teleoperation using a Virtual Reality Interface with ROS Reality\". In: Mar. 2018.\n",
      "- <span id=\"page-57-5\"></span>[17] Kai von Szadkowski and Simon Reichel. \"Phobos: A tool for creating complex robot models\". In: *Journal of Open Source Software* 5.45 (2020), p. 1326. doi: [10.21105/joss.01326](https://doi.org/10.21105/joss.01326). url: <https://doi.org/10.21105/joss.01326>.\n",
      "- <span id=\"page-57-6\"></span>[18] Moritz Tenorth and Daniel BeÃŸler. *KnowRob*. url: <http://knowrob.org> (visited on 04/15/2018).\n",
      "- <span id=\"page-57-3\"></span>[19] D. Whitney et al. \"ROS Reality: A Virtual Reality Framework Using Consumer-Grade Hardware for ROS-Enabled Robots\". In: *2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*. 2018, pp. 1â€“9. doi: [10.1109/](https://doi.org/10.1109/IROS.2018.8593513) [IROS.2018.8593513](https://doi.org/10.1109/IROS.2018.8593513).\n",
      "- <span id=\"page-57-0\"></span>[20] Zihe Xu. \"Designing Human-controlled Robots in VR for Learning Everyday Manipulation Tasks\". Master Thesis. Institute of Artificial Intelligence, University of Bremen. (Visited on 12/10/2019).\n",
      "\n",
      "# **List of Figures**\n",
      "\n",
      "| 1  | Schematic drawing of a Blender bone.                                                                                                | 21 |  |  |  |\n",
      "|----|-------------------------------------------------------------------------------------------------------------------------------------|----|--|--|--|\n",
      "| 2  | Blender FBX export settings.<br>23                                                                                                  |    |  |  |  |\n",
      "| 3  | PR2 skeleton after having all the necessary settings and modifications                                                              |    |  |  |  |\n",
      "|    | applied for the Unreal Engine export.                                                                                               | 24 |  |  |  |\n",
      "| 4  | The imported PR2 skeletal mesh within the Unreal Engine. The tiny spheres                                                           |    |  |  |  |\n",
      "|    | are essentially the bones, the long connections between them simply visualize                                                       |    |  |  |  |\n",
      "|    | their parent relation between one another.                                                                                          | 25 |  |  |  |\n",
      "| 5  | part of the BP_PR2_Character Blueprint, showing a part of the imple                                                                 |    |  |  |  |\n",
      "|    | mentation of key-press based navigation/movement.                                                                                   | 26 |  |  |  |\n",
      "| 6  | Navigation based on the position of the Head Mounted Display.                                                                       | 27 |  |  |  |\n",
      "| 7  | Settings options for the CCDIK Animation Graph Node                                                                                 | 31 |  |  |  |\n",
      "| 8  | Some of the disjointed states of the PR2 arm, while using the CCDIK node                                                            | 31 |  |  |  |\n",
      "| 9  | By rotating the shoulder_pan_link away from the goal (green circle) and                                                             |    |  |  |  |\n",
      "|    | allowing the upper_arm_link to rotate at all and bend the elbow, the goal                                                           |    |  |  |  |\n",
      "|    | position could be reached. However, CCDIK and the ApplyLimits node do                                                               |    |  |  |  |\n",
      "|    | not work well together and cannot reach the target goal.                                                                            | 32 |  |  |  |\n",
      "| 10 | It can be seen in the left image, that this approach provides a better elbow                                                        |    |  |  |  |\n",
      "|    | movement, and reaches the goal in some configurations. However as can be                                                            |    |  |  |  |\n",
      "|    | seen in the right image, just a little rotation can shift the entire result by a                                                    |    |  |  |  |\n",
      "|    | very large margin.                                                                                                                  | 34 |  |  |  |\n",
      "| 11 | The nodes setup necessary to achieve this motion. It does not need to be                                                            |    |  |  |  |\n",
      "|    | viewed in detail it should just visualize how many different nodes are needed                                                       |    |  |  |  |\n",
      "|    | to get this sort of interaction, and that even then it is not yet perfect.                                                          | 34 |  |  |  |\n",
      "| 12 | On the left: the inactive physics asset with all the currently setup capsules                                                       |    |  |  |  |\n",
      "|    | defining the constraints. On the right: the physics asset during simulation.                                                        |    |  |  |  |\n",
      "|    | The right arm is being positioned by a mouse click.                                                                                 | 35 |  |  |  |\n",
      "| 13 | On the left: the PR2 IK node within the animation graph. On the right:                                                              |    |  |  |  |\n",
      "|    | exemplary settings for the PR IK node                                                                                               | 38 |  |  |  |\n",
      "| 14 | Scaling of the motion controller trigger value in order to be in the range of                                                       |    |  |  |  |\n",
      "|    | 0 to 30                                                                                                                             | 41 |  |  |  |\n",
      "| 15 | Opening and closing of the gripper based on a sequence of copying the                                                               |    |  |  |  |\n",
      "|    | rotation of one bone.                                                                                                               | 41 |  |  |  |\n",
      "| 16 | Gripper detachment feature.<br>the green circle shows the position of the                                                           |    |  |  |  |\n",
      "|    | motion controller, while the orange circle shows the position of the gripper                                                        |    |  |  |  |\n",
      "|    | and how it can be extended.                                                                                                         | 44 |  |  |  |\n",
      "| 17 | A part of the blueprint which is calculating and scaling the forward vector,<br>in order to adjust the robot to humans arms length. | 44 |  |  |  |\n",
      "| 18 | A mirror object to help the human user to settle into the robots body.                                                              | 44 |  |  |  |\n",
      "| 19 | Body height calculations.                                                                                                           | 45 |  |  |  |\n",
      "|    |                                                                                                                                     |    |  |  |  |\n",
      "\n",
      "# **List of Tables**\n",
      "\n",
      "| 1 | Summary of the URDF adaptation which needed to be performed in order |    |\n",
      "|---|----------------------------------------------------------------------|----|\n",
      "|   | for phobos to parse the URDF correctly.                              | 21 |\n",
      "| 2 | Overview of all the IK approaches tried within this thesis.          | 30 |\n",
      "\n",
      "## **List of used software**\n",
      "\n",
      "| Name                         | Version<br>or<br>Branch | Repository or Download link                              |\n",
      "|------------------------------|-------------------------|----------------------------------------------------------|\n",
      "| Blender                      | 2.79                    | https://www.blender.org/download/releases/<br>2-79/      |\n",
      "| phobos                       | rigging                 | https://github.com/hawkina/phobos/tree/rigging           |\n",
      "| Unreal Engine                | 4.22.3                  | https://www.unrealengine.com                             |\n",
      "| RobCoG                       | master                  | https://github.com/hawkina/RobCoG                        |\n",
      "| CustomAnimNode               | main                    | https://github.com/hawkina/CustomAnimNode                |\n",
      "| VR<br>Neems<br>to<br>KnowRob | master                  | https://github.com/hawkina/vr_neems_to_knowrob           |\n",
      "| CRAM                         | boxy                    | https://github.com/cram2/cram/tree/boxy                  |\n",
      "| Eigen                        | Eigen3                  | http://eigen.tuxfamily.org/index.php?title=<br>Main_Page |\n",
      "| KDL                          |                         | https://orocos.org/kdl.html                              |\n",
      "| KnowRob                      | kinetic                 | https://github.com/knowrob/knowrob/tree/<br>kinetic      |\n",
      "| UMCInteraction               | master                  | https://github.com/hawkina/UMCInteraction                |\n",
      "| USemLog                      | master                  | https://github.com/hawkina/USemLog                       |\n",
      "\n",
      "## <span id=\"page-61-0\"></span>**7.2 Acronyms**\n",
      "\n",
      "**HMD** Head Mounted Display **CRAM** Cognitive Robot Abstract Machine **IK** Inverse Kinematics **owl** Web Ontology Language **RobCoG** Robot Commonsense Games **ROS** Robot Operating System **VR** Virtual Reality<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T14:07:59.559262Z",
     "start_time": "2025-04-22T14:07:56.270895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = datasets,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 10,\n",
    "\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 5e-6,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.00,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ],
   "id": "a5de395ad32a1df0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  4.70 examples/s]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T14:10:55.689021Z",
     "start_time": "2025-04-22T14:08:01.992940Z"
    }
   },
   "cell_type": "code",
   "source": "trainer_stats = trainer.train()",
   "id": "47a52677209cb856",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 13 | Num Epochs = 10 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 296,198,144/4,000,000,000 (7.40% trained)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 02:46, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.674100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.102200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.465200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.850900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.201700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.639700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.209600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.317300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.199800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.680500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.702500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.778300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.973700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.166100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malineni/envs/unsloth/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "text_streamer = TextIteratorStreamer(tokenizer)\n",
    "import textwrap\n",
    "max_print_width = 100\n",
    "\n",
    "prompts = [\n",
    "    \"what is CRAM action designator,\",\n",
    "    \"what is the comprehensive action designator for the task- cut the apple. Give it in json format, no further explanation is needed\",\n",
    "    \"what are the flanagan motion phases involved in the task- cut the apple. Give it in json format, no further explanation is needed\",\n",
    "    \"what are the motion constraints involved in the task- cut the apple. Give it in json format, no further explanation is needed\",\n",
    "    \"what are the framenet elements involved in the task- cut the apple. Give it in json format, no further explanation is needed\",\n",
    "    \"what are the object and tools involved in the task- cut the apple. Give it in json format, no further explanation is needed\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompts[0]\n",
    "]*1, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    inputs,\n",
    "    streamer = text_streamer,\n",
    "    max_new_tokens = 2048*2,\n",
    "    use_cache = True,\n",
    ")\n",
    "thread = Thread(target = model.generate, kwargs = generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "length = 0\n",
    "for j, new_text in enumerate(text_streamer):\n",
    "    if j == 0:\n",
    "        wrapped_text = textwrap.wrap(new_text, width = max_print_width)\n",
    "        length = len(wrapped_text[-1])\n",
    "        wrapped_text = \"\\n\".join(wrapped_text)\n",
    "        print(wrapped_text, end = \"\")\n",
    "    else:\n",
    "        length += len(new_text)\n",
    "        if length >= max_print_width:\n",
    "            length = 0\n",
    "            print()\n",
    "        print(new_text, end = \"\")\n",
    "    pass\n",
    "pass"
   ],
   "id": "9b4848d5d10b9627",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the model",
   "id": "d96a402bef45608b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:26:19.138799Z",
     "start_time": "2025-03-20T12:26:17.320187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"gemma_4b_trained\")\n",
    "tokenizer.save_pretrained(\"gemma_4b_trained\")"
   ],
   "id": "51d6a6dc8606bc6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemma_4b_trained/processor_config.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "459bba9c2fdf95c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "unsloth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
